{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "\n",
    "import HPO\n",
    "import pysgpp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D, Conv2D\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from numpy.random import seed\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "seed(2)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.random.set_seed(3)\n",
    "\n",
    "VERBOSE = 0\n",
    "CV = 2 #[(slice(None), slice(None))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter space definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameterspace = {\n",
    "    'epochs': [\"interval-int\", 1, 40],\n",
    "    'batch_size': [\"interval-int\", 30, 2050],\n",
    "    'learning_rate': [\"interval-log\", 0.000000001, 0.1],\n",
    "    'number_conv_layers': [\"interval-int\", 1, 20],\n",
    "    'number_fc_layers': [\"interval-int\", 1, 20],\n",
    "    'kernel_size': [\"interval-int\", 2, 5],\n",
    "    'pool_size': [\"interval-int\", 2, 5],\n",
    "    'neurons_per_fc_layer': [\"interval-int\", 1, 50],\n",
    "    'dropout_prob': [\"interval\", 0, 1]\n",
    "}\n",
    "\n",
    "hyperparameterspace_special = {}\n",
    "for key in hyperparameterspace.keys():\n",
    "    liste = []\n",
    "    for i in range(1, len(hyperparameterspace[key])):\n",
    "        liste.append(hyperparameterspace[key][i])\n",
    "    hyperparameterspace_special[key] = liste\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "\n",
    "def create_model(learning_rate=1e-4, number_conv_layers=2, number_fc_layers=0, kernel_size=3, pool_size=2, neurons_per_fc_layer=4, dropout_prob=0.5):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "\n",
    "    for _ in range(number_conv_layers):\n",
    "        model.add(layers.Conv2D(32, kernel_size=(kernel_size, kernel_size), activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(dropout_prob))\n",
    "\n",
    "    for _ in range(number_fc_layers):\n",
    "        model.add(layers.Dense(neurons_per_fc_layer, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"Current_tests/\"+time.strftime(\"%H_%M_%S\", time.localtime())\n",
    "\n",
    "SPARSE_PARAMS = [2, 0.85, \"gradient_descent\"]\n",
    "\n",
    "BUDGETS = [30, 50, 70, 90, 110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 16:32:16.269528: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-07 16:32:16.483480: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 169344000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 21s 48ms/step - loss: 0.3743 - accuracy: 0.8852 - val_loss: 0.0800 - val_accuracy: 0.9780\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.1109 - accuracy: 0.9660 - val_loss: 0.0550 - val_accuracy: 0.9863\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.0837 - accuracy: 0.9738 - val_loss: 0.0448 - val_accuracy: 0.9873\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.0707 - accuracy: 0.9775 - val_loss: 0.0413 - val_accuracy: 0.9878\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.0617 - accuracy: 0.9812 - val_loss: 0.0368 - val_accuracy: 0.9897\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 20s 48ms/step - loss: 0.0569 - accuracy: 0.9821 - val_loss: 0.0393 - val_accuracy: 0.9890\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 19s 45ms/step - loss: 0.0507 - accuracy: 0.9842 - val_loss: 0.0347 - val_accuracy: 0.9902\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 19s 46ms/step - loss: 0.0476 - accuracy: 0.9847 - val_loss: 0.0321 - val_accuracy: 0.9908\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 19s 45ms/step - loss: 0.0442 - accuracy: 0.9861 - val_loss: 0.0300 - val_accuracy: 0.9913\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.0413 - accuracy: 0.9866 - val_loss: 0.0327 - val_accuracy: 0.9910\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 0.0386 - accuracy: 0.9874 - val_loss: 0.0302 - val_accuracy: 0.9918\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 0.0379 - accuracy: 0.9878 - val_loss: 0.0306 - val_accuracy: 0.9917\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0363 - accuracy: 0.9879 - val_loss: 0.0303 - val_accuracy: 0.9918\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0346 - accuracy: 0.9890 - val_loss: 0.0295 - val_accuracy: 0.9923\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0322 - accuracy: 0.9896 - val_loss: 0.0314 - val_accuracy: 0.9913\n",
      "Test loss: 0.02965961955487728\n",
      "Test accuracy: 0.9894000291824341\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[39m=\u001b[39m HPO\u001b[39m.\u001b[39mDataset(task_id\u001b[39m=\u001b[39m\u001b[39m4278\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39;49m(dataset\u001b[39m.\u001b[39;49mget_input_dim()))\n\u001b[1;32m      5\u001b[0m \u001b[39m################## MODEL AND FUNCTION DEFINITION ####################\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_model\u001b[39m(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer, deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[39m# return epochs + batch_size\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "################## MODEL AND FUNCTION DEFINITION ####################\n",
    "\n",
    "def evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=True):\n",
    "\n",
    "    if deterministic:\n",
    "        reset_seeds()\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Scale images to the [0, 1] range\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "    # Make sure images have shape (28, 28, 1)\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model = create_model(learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return -score[1]\n",
    "    \n",
    "\n",
    "def blackboxfunction_grid(params):\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "def blackboxfunction_random(params):\n",
    "    \n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "def blackboxfunction_bayesian(params):\n",
    "    \n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = 10 ** params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "##################### Function for sparse grid search #####################\n",
    "\n",
    "class ExampleFunction(pysgpp.ScalarFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ExampleFunction, self).__init__(\n",
    "            len(hyperparameterspace.keys()))\n",
    "\n",
    "    def eval(self, x):\n",
    "        # index = int(x[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "        # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "        epochs = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[0]))\n",
    "\n",
    "        batch_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[1]))\n",
    "\n",
    "        # HPO.from_standard_log(hyperparameterspace_special[\"learning_rate\"][\n",
    "        model_learning_rate = HPO.from_standard_log(\n",
    "            hyperparameterspace_special[\"learning_rate\"][0], hyperparameterspace_special[\"learning_rate\"][1], x[2])\n",
    "\n",
    "        number_of_layers = 1  # int(HPO.from_standard(\n",
    "        # hyperparameterspace_special[\"number_layers\"][0], hyperparameterspace_special[\"number_layers\"][1], x[3]))\n",
    "\n",
    "        neurons_per_layer = 40  # int(HPO.from_standard(\n",
    "        # hyperparameterspace_special[\"neurons_per_layer\"][0], hyperparameterspace_special[\"neurons_per_layer\"][1], x[4]))\n",
    "\n",
    "        epochs = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[0]))\n",
    "\n",
    "        batch_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[1]))\n",
    "\n",
    "        learning_rate = HPO.from_standard_log(\n",
    "            hyperparameterspace_special[\"learning_rate\"][0], hyperparameterspace_special[\"learning_rate\"][1], x[2])\n",
    "\n",
    "        number_conv_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_conv_layers\"][0], hyperparameterspace_special[\"number_conv_layers\"][1], x[3]))\n",
    "\n",
    "        number_fc_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_fc_layers\"][0], hyperparameterspace_special[\"number_fc_layers\"][1], x[4]))\n",
    "\n",
    "        kernel_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"kernel_size\"][0], hyperparameterspace_special[\"kernel_size\"][1], x[5]))\n",
    "\n",
    "        pool_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"pool_size\"][0], hyperparameterspace_special[\"pool_size\"][1], x[6]))\n",
    "\n",
    "        neurons_per_fc_layer = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"neurons_per_fc_layer\"][0], hyperparameterspace_special[\"neurons_per_fc_layer\"][1], x[7]))\n",
    "\n",
    "        dropout_prob = HPO.from_standard(\n",
    "            hyperparameterspace_special[\"dropout_prob\"][0], hyperparameterspace_special[\"dropout_prob\"][1], x[8])\n",
    "\n",
    "        return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "\n",
    "RESULTS_GRID = \"{\"\n",
    "RESULTS_RANDOM = \"{\"\n",
    "RESULTS_BAYESIAN = \"{\"\n",
    "RESULTS_SPARSE = \"{\"\n",
    "\n",
    "dataset = HPO.Dataset([], [])\n",
    "\n",
    "##### For each dataset: run models with different budget #####\n",
    "for BUDGET in BUDGETS:\n",
    "\n",
    "    print(\"\\n################################################## Current Budget:\",\n",
    "            BUDGET, \"##################################################\")\n",
    "\n",
    "    ############################## GRID SEARCH #######################\n",
    "    print(\"\\nPerforming grid search\")\n",
    "    optimization = HPO.GridSearchOptimization(\n",
    "        dataset, blackboxfunction_grid, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "    result, cost = optimization.fit()\n",
    "\n",
    "    index_best = 0\n",
    "    for m in range(len(result)):\n",
    "        if result[m][1] < result[index_best][1]:\n",
    "            index_best = m\n",
    "\n",
    "    best_score = result[index_best][1]\n",
    "    best_params = result[index_best][0]\n",
    "    \n",
    "\n",
    "    print(\"Best score with Grid search:\", best_score)\n",
    "\n",
    "    RESULTS_GRID += \"(\" + str(cost) + \",\" + str(-best_score) + \")\"\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    # ########################### RANDOM SEARCH #######################\n",
    "    print(\"\\nPerforming random search\")\n",
    "\n",
    "    optimization = HPO.RandomSearchOptimization(\n",
    "        dataset, blackboxfunction_random, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "    result, cost = optimization.fit()\n",
    "\n",
    "    index_best = 0\n",
    "    for m in range(len(result)):\n",
    "        if result[m][1] < result[index_best][1]:\n",
    "            index_best = m\n",
    "\n",
    "    best_score = result[index_best][1]\n",
    "    best_params = result[index_best][0]\n",
    "    \n",
    "\n",
    "    print(\"Best score with Random search:\", best_score)\n",
    "\n",
    "    RESULTS_RANDOM += \"(\" + str(cost) + \",\" + str(-best_score) + \")\"\n",
    "    \n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    ########################### BAYESIAN OPT #####################\n",
    "    print(\"\\nPerforming bayesian optimization\")\n",
    "\n",
    "    optimization = HPO.BayesianOptimization(\n",
    "        dataset, blackboxfunction_bayesian, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE)\n",
    "\n",
    "    result, cost = optimization.fit()\n",
    "\n",
    "    index_best = 0\n",
    "    for m in range(len(result)):\n",
    "        if result[m][1] < result[index_best][1]:\n",
    "            index_best = m\n",
    "\n",
    "    best_score = result[index_best][1]\n",
    "    best_params = result[index_best][0]\n",
    "    \n",
    "\n",
    "    print(\"Best score with Bayesian Optimization:\", best_score)\n",
    "\n",
    "\n",
    "    RESULTS_BAYESIAN += \"(\" + str(BUDGET) + \",\" + str(-best_score) + \")\"\n",
    "    \n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    ########################### SPARSE OPT ############################\n",
    "\n",
    "    print(\"\\nPerforming sparse search\")\n",
    "\n",
    "    f = ExampleFunction()\n",
    "\n",
    "    optimization = HPO.SparseGridSearchOptimization(\n",
    "        dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=SPARSE_PARAMS[0], adaptivity=SPARSE_PARAMS[1], optimizer=SPARSE_PARAMS[2])\n",
    "\n",
    "    [fX0, fX1, fX2], cost = optimization.fit()\n",
    "\n",
    "    cost = cost + 2\n",
    "    bestFX = fX0 \n",
    "    if fX1 < bestFX:\n",
    "        bestFX = fX1 \n",
    "    if fX2 < bestFX:\n",
    "        bestFX = fX2\n",
    "\n",
    "    RESULTS_SPARSE += \"(\" + str(cost) + \",\" + str(bestFX) + \")\"\n",
    "        \n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "\n",
    "print(\"GRID SEARCH\")\n",
    "print(RESULTS_GRID+\"}\")\n",
    "\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(RESULTS_RANDOM+\"}\")\n",
    "\n",
    "print(\"BAYESIAN SEARCH\")\n",
    "print(RESULTS_BAYESIAN+\"}\")\n",
    "\n",
    "print(\"SPARSE SEARCH\")\n",
    "print(RESULTS_SPARSE+\"}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
