{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 18:09:49.556013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-28 18:09:50.939170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "\n",
    "import HPO\n",
    "import pysgpp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D, Conv2D\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from numpy.random import seed\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "seed(2)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.random.set_seed(3)\n",
    "\n",
    "VERBOSE = 1\n",
    "CV = 2 #[(slice(None), slice(None))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter space definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameterspace = {\n",
    "    'epochs': [\"interval-int\", 1, 10],\n",
    "    'batch_size': [\"interval-int\", 200, 1000],\n",
    "    'learning_rate': [\"interval-log\", 0.000000001, 0.1],\n",
    "    'number_conv_layers': [\"interval-int\", 1, 3],\n",
    "    'number_fc_layers': [\"interval-int\", 1, 3],\n",
    "    'kernel_size': [\"interval-int\", 1, 4],\n",
    "    'pool_size': [\"interval-int\", 1, 3],\n",
    "    'neurons_per_fc_layer': [\"interval-int\", 1, 7],\n",
    "    'dropout_prob': [\"interval\", 0, 1]\n",
    "}\n",
    "\n",
    "hyperparameterspace_special = {}\n",
    "for key in hyperparameterspace.keys():\n",
    "    liste = []\n",
    "    for i in range(1, len(hyperparameterspace[key])):\n",
    "        liste.append(hyperparameterspace[key][i])\n",
    "    hyperparameterspace_special[key] = liste\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "\n",
    "def create_model(learning_rate=1e-4, number_conv_layers=2, number_fc_layers=0, kernel_size=3, pool_size=2, neurons_per_fc_layer=4, dropout_prob=0.5):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "\n",
    "    for _ in range(number_conv_layers):\n",
    "        model.add(layers.Conv2D(32, kernel_size=(kernel_size, kernel_size), activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(pool_size, pool_size), padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(dropout_prob))\n",
    "\n",
    "    for _ in range(number_fc_layers):\n",
    "        model.add(layers.Dense(neurons_per_fc_layer, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"Current_tests/\"+time.strftime(\"%H_%M_%S\", time.localtime())\n",
    "\n",
    "SPARSE_PARAMS = [2, 0.85, \"gradient_descent\"]\n",
    "\n",
    "BUDGETS = [5, 30, 50, 80, 110]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################## Current Budget: 5 ##################################################\n",
      "\n",
      "Performing grid search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 18:09:53.980479: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score with Grid search: -0.1006999984383583\n",
      "With Hyperparameters: \n",
      "epochs: 5\n",
      "batch_size: 600\n",
      "learning_rate: 9.999999999999997e-06\n",
      "number_conv_layers: 2\n",
      "number_fc_layers: 2\n",
      "kernel_size: 2\n",
      "pool_size: 2\n",
      "neurons_per_fc_layer: 4\n",
      "dropout_prob: 0.5\n",
      "Best score with Grid search: -0.1006999984383583\n",
      "\n",
      "Performing random search\n",
      "With Hyperparameters: \n",
      "epochs: 8\n",
      "batch_size: 949\n",
      "learning_rate: 0.00022801756022107158\n",
      "number_conv_layers: 2\n",
      "number_fc_layers: 1\n",
      "kernel_size: 1\n",
      "pool_size: 1\n",
      "neurons_per_fc_layer: 6\n",
      "dropout_prob: 0.6852195003967595\n",
      "Best score with Random search: -0.7871000170707703\n",
      "\n",
      "Performing bayesian optimization\n",
      "Iterations took 924.0834830729982 seconds\n",
      "With Hyperparameters: \n",
      "epochs: 6.0\n",
      "batch_size: 881.0\n",
      "learning_rate: -5.362489706295694\n",
      "number_conv_layers: 2.0\n",
      "number_fc_layers: 1.0\n",
      "kernel_size: 2.0\n",
      "pool_size: 1.0\n",
      "neurons_per_fc_layer: 6.0\n",
      "dropout_prob: 0.7879292338255043\n",
      "Best score with Bayesian Optimization: -0.19499999284744263\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n",
      "Done in 74700ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 0ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 0ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 1ms.\n",
      "With Hyperparameters: \n",
      "epochs: 0.5\n",
      "batch_size: 0.5\n",
      "learning_rate: 0.5\n",
      "number_conv_layers: 0.5\n",
      "number_fc_layers: 0.5\n",
      "kernel_size: 0.5\n",
      "pool_size: 0.5\n",
      "neurons_per_fc_layer: 0.5\n",
      "dropout_prob: 0.5\n",
      "GRID SEARCH\n",
      "{(1,0.1006999984383583)}\n",
      "RANDOM SEARCH\n",
      "{(5,0.7871000170707703)}\n",
      "BAYESIAN SEARCH\n",
      "{(5,0.19499999284744263)}\n",
      "SPARSE SEARCH\n",
      "{(3,0.3700999915599823)}\n",
      "\n",
      "################################################## Current Budget: 30 ##################################################\n",
      "\n",
      "Performing random search\n",
      "With Hyperparameters: \n",
      "epochs: 2\n",
      "batch_size: 826\n",
      "learning_rate: 0.018547321809577612\n",
      "number_conv_layers: 1\n",
      "number_fc_layers: 1\n",
      "kernel_size: 1\n",
      "pool_size: 2\n",
      "neurons_per_fc_layer: 6\n",
      "dropout_prob: 0.83169258900655\n",
      "Best score with Random search: -0.863099992275238\n",
      "\n",
      "Performing bayesian optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations took 3983.574222969997 seconds\n",
      "With Hyperparameters: \n",
      "epochs: 9.0\n",
      "batch_size: 954.0\n",
      "learning_rate: -1.9342325051839593\n",
      "number_conv_layers: 1.0\n",
      "number_fc_layers: 1.0\n",
      "kernel_size: 3.0\n",
      "pool_size: 1.0\n",
      "neurons_per_fc_layer: 5.0\n",
      "dropout_prob: 0.1853176459376844\n",
      "Best score with Bayesian Optimization: -0.9649999737739563\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n",
      "Done in 1522976ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 0ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 0ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 1ms.\n",
      "With Hyperparameters: \n",
      "epochs: 0.5\n",
      "batch_size: 0.5\n",
      "learning_rate: 0.75\n",
      "number_conv_layers: 0.5\n",
      "number_fc_layers: 0.5\n",
      "kernel_size: 0.5\n",
      "pool_size: 0.5\n",
      "neurons_per_fc_layer: 0.5\n",
      "dropout_prob: 0.5\n",
      "GRID SEARCH\n",
      "{(1,0.1006999984383583)}\n",
      "RANDOM SEARCH\n",
      "{(5,0.7871000170707703)(30,0.863099992275238)}\n",
      "BAYESIAN SEARCH\n",
      "{(5,0.19499999284744263)(30,0.9649999737739563)}\n",
      "SPARSE SEARCH\n",
      "{(3,0.3700999915599823)(21,0.8806999921798706)}\n",
      "\n",
      "################################################## Current Budget: 50 ##################################################\n",
      "\n",
      "Performing random search\n",
      "With Hyperparameters: \n",
      "epochs: 8\n",
      "batch_size: 403\n",
      "learning_rate: 0.017011227262955873\n",
      "number_conv_layers: 2\n",
      "number_fc_layers: 1\n",
      "kernel_size: 3\n",
      "pool_size: 2\n",
      "neurons_per_fc_layer: 2\n",
      "dropout_prob: 0.29880022894515046\n",
      "Best score with Random search: -0.9151999950408936\n",
      "\n",
      "Performing bayesian optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations took 5309.931044963028 seconds\n",
      "With Hyperparameters: \n",
      "epochs: 5.0\n",
      "batch_size: 208.0\n",
      "learning_rate: -1.7161293616444961\n",
      "number_conv_layers: 2.0\n",
      "number_fc_layers: 1.0\n",
      "kernel_size: 3.0\n",
      "pool_size: 2.0\n",
      "neurons_per_fc_layer: 3.0\n",
      "dropout_prob: 0.29932692481820444\n",
      "Best score with Bayesian Optimization: -0.9501000046730042\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n",
      "Done in 2939597ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 0ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 0ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 2ms.\n",
      "With Hyperparameters: \n",
      "epochs: 0.5\n",
      "batch_size: 0.5\n",
      "learning_rate: 0.875\n",
      "number_conv_layers: 0.5\n",
      "number_fc_layers: 0.5\n",
      "kernel_size: 0.5\n",
      "pool_size: 0.5\n",
      "neurons_per_fc_layer: 0.5\n",
      "dropout_prob: 0.5\n",
      "GRID SEARCH\n",
      "{(1,0.1006999984383583)}\n",
      "RANDOM SEARCH\n",
      "{(5,0.7871000170707703)(30,0.863099992275238)(50,0.9151999950408936)}\n",
      "BAYESIAN SEARCH\n",
      "{(5,0.19499999284744263)(30,0.9649999737739563)(50,0.9501000046730042)}\n",
      "SPARSE SEARCH\n",
      "{(3,0.3700999915599823)(21,0.8806999921798706)(39,0.9319000244140625)}\n",
      "\n",
      "################################################## Current Budget: 80 ##################################################\n",
      "\n",
      "Performing random search\n",
      "With Hyperparameters: \n",
      "epochs: 9\n",
      "batch_size: 975\n",
      "learning_rate: 0.017277833428114713\n",
      "number_conv_layers: 2\n",
      "number_fc_layers: 1\n",
      "kernel_size: 3\n",
      "pool_size: 1\n",
      "neurons_per_fc_layer: 6\n",
      "dropout_prob: 0.6185135669120855\n",
      "Best score with Random search: -0.974399983882904\n",
      "\n",
      "Performing bayesian optimization\n",
      "Iterations took 10406.588716233062 seconds\n",
      "With Hyperparameters: \n",
      "epochs: 6.24971397965232\n",
      "batch_size: 584.6300363152563\n",
      "learning_rate: -2.1746155501063043\n",
      "number_conv_layers: 2.0261525767538155\n",
      "number_fc_layers: 1.9419462873321687\n",
      "kernel_size: 3.0759763402633773\n",
      "pool_size: 1.9977530166157595\n",
      "neurons_per_fc_layer: 5.295331876521429\n",
      "dropout_prob: 0.28147338015457346\n",
      "Best score with Bayesian Optimization: -0.9753999710083008\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n",
      "Done in 5890124ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 0ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 1ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 3ms.\n",
      "With Hyperparameters: \n",
      "epochs: 0.5\n",
      "batch_size: 0.25\n",
      "learning_rate: 0.875\n",
      "number_conv_layers: 0.5\n",
      "number_fc_layers: 0.5\n",
      "kernel_size: 0.5\n",
      "pool_size: 0.5\n",
      "neurons_per_fc_layer: 0.75\n",
      "dropout_prob: 0.5\n",
      "GRID SEARCH\n",
      "{(1,0.1006999984383583)}\n",
      "RANDOM SEARCH\n",
      "{(5,0.7871000170707703)(30,0.863099992275238)(50,0.9151999950408936)(80,0.974399983882904)}\n",
      "BAYESIAN SEARCH\n",
      "{(5,0.19499999284744263)(30,0.9649999737739563)(50,0.9501000046730042)(80,0.9753999710083008)}\n",
      "SPARSE SEARCH\n",
      "{(3,0.3700999915599823)(21,0.8806999921798706)(39,0.9319000244140625)(75,0.9732999801635742)}\n",
      "\n",
      "################################################## Current Budget: 110 ##################################################\n",
      "\n",
      "Performing random search\n",
      "With Hyperparameters: \n",
      "epochs: 9\n",
      "batch_size: 975\n",
      "learning_rate: 0.017277833428114713\n",
      "number_conv_layers: 2\n",
      "number_fc_layers: 1\n",
      "kernel_size: 3\n",
      "pool_size: 1\n",
      "neurons_per_fc_layer: 6\n",
      "dropout_prob: 0.6185135669120855\n",
      "Best score with Random search: -0.974399983882904\n",
      "\n",
      "Performing bayesian optimization\n",
      "Iterations took 13752.811435781012 seconds\n",
      "With Hyperparameters: \n",
      "epochs: 7.0\n",
      "batch_size: 248.0\n",
      "learning_rate: -2.1845245661821098\n",
      "number_conv_layers: 2.0\n",
      "number_fc_layers: 1.0\n",
      "kernel_size: 2.0\n",
      "pool_size: 2.0\n",
      "neurons_per_fc_layer: 4.0\n",
      "dropout_prob: 0.029150708276918347\n",
      "Best score with Bayesian Optimization: -0.9714999794960022\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n",
      "Done in 7201707ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 1ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 2ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 3ms.\n",
      "With Hyperparameters: \n",
      "epochs: 0.75\n",
      "batch_size: 0.25\n",
      "learning_rate: 0.875\n",
      "number_conv_layers: 0.5\n",
      "number_fc_layers: 0.5\n",
      "kernel_size: 0.5\n",
      "pool_size: 0.5\n",
      "neurons_per_fc_layer: 0.75\n",
      "dropout_prob: 0.5\n",
      "GRID SEARCH\n",
      "{(1,0.1006999984383583)}\n",
      "RANDOM SEARCH\n",
      "{(5,0.7871000170707703)(30,0.863099992275238)(50,0.9151999950408936)(80,0.974399983882904)(110,0.974399983882904)}\n",
      "BAYESIAN SEARCH\n",
      "{(5,0.19499999284744263)(30,0.9649999737739563)(50,0.9501000046730042)(80,0.9753999710083008)(110,0.9714999794960022)}\n",
      "SPARSE SEARCH\n",
      "{(3,0.3700999915599823)(21,0.8806999921798706)(39,0.9319000244140625)(75,0.9732999801635742)(93,0.9753999710083008)}\n",
      "GRID SEARCH\n",
      "{(1,0.1006999984383583)}\n",
      "RANDOM SEARCH\n",
      "{(5,0.7871000170707703)(30,0.863099992275238)(50,0.9151999950408936)(80,0.974399983882904)(110,0.974399983882904)}\n",
      "BAYESIAN SEARCH\n",
      "{(5,0.19499999284744263)(30,0.9649999737739563)(50,0.9501000046730042)(80,0.9753999710083008)(110,0.9714999794960022)}\n",
      "SPARSE SEARCH\n",
      "{(3,0.3700999915599823)(21,0.8806999921798706)(39,0.9319000244140625)(75,0.9732999801635742)(93,0.9753999710083008)}\n"
     ]
    }
   ],
   "source": [
    "################## MODEL AND FUNCTION DEFINITION ####################\n",
    "\n",
    "def evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=True):\n",
    "\n",
    "    # if too many layers and resulting image has no values left, return accuracy 0\n",
    "\n",
    "    current_size = 28\n",
    "    for _ in range(number_conv_layers + 1):\n",
    "        current_size = (current_size - kernel_size) + 1\n",
    "        current_size = (current_size - pool_size) + 1\n",
    "\n",
    "    if current_size <= 1:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    if deterministic:\n",
    "        reset_seeds()\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Scale images to the [0, 1] range\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "    # Make sure images have shape (28, 28, 1)\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model = create_model(learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "    model.fit(x_train, y_train, verbose=0, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=False)\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    K.clear_session()\n",
    "    del model \n",
    "    K.clear_session()\n",
    "\n",
    "    return -score[1]    \n",
    "\n",
    "    \n",
    "\n",
    "def blackboxfunction_grid(params):\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "def blackboxfunction_random(params):\n",
    "    \n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "def blackboxfunction_bayesian(params):\n",
    "    \n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = 10 ** params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "##################### Function for sparse grid search #####################\n",
    "\n",
    "class ExampleFunction(pysgpp.ScalarFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ExampleFunction, self).__init__(\n",
    "            len(hyperparameterspace.keys()))\n",
    "\n",
    "    def eval(self, x):\n",
    "\n",
    "        epochs = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[0]))\n",
    "\n",
    "        batch_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[1]))\n",
    "\n",
    "        learning_rate = HPO.from_standard_log(\n",
    "            hyperparameterspace_special[\"learning_rate\"][0], hyperparameterspace_special[\"learning_rate\"][1], x[2])\n",
    "\n",
    "        number_conv_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_conv_layers\"][0], hyperparameterspace_special[\"number_conv_layers\"][1], x[3]))\n",
    "\n",
    "        number_fc_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_fc_layers\"][0], hyperparameterspace_special[\"number_fc_layers\"][1], x[4]))\n",
    "\n",
    "        kernel_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"kernel_size\"][0], hyperparameterspace_special[\"kernel_size\"][1], x[5]))\n",
    "\n",
    "        pool_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"pool_size\"][0], hyperparameterspace_special[\"pool_size\"][1], x[6]))\n",
    "\n",
    "        neurons_per_fc_layer = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"neurons_per_fc_layer\"][0], hyperparameterspace_special[\"neurons_per_fc_layer\"][1], x[7]))\n",
    "\n",
    "        dropout_prob = HPO.from_standard(\n",
    "            hyperparameterspace_special[\"dropout_prob\"][0], hyperparameterspace_special[\"dropout_prob\"][1], x[8])\n",
    "\n",
    "        return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "\n",
    "\n",
    "RESULTS_GRID = \"{\"\n",
    "RESULTS_RANDOM = \"{\"\n",
    "RESULTS_BAYESIAN = \"{\"\n",
    "RESULTS_SPARSE = \"{\"\n",
    "\n",
    "dataset = HPO.Dataset([], [])\n",
    "\n",
    "##### For each dataset: run models with different budget #####\n",
    "for BUDGET in BUDGETS:\n",
    "\n",
    "    print(\"\\n################################################## Current Budget:\",\n",
    "            BUDGET, \"##################################################\")\n",
    "\n",
    "    ############################## GRID SEARCH #######################\n",
    "    if BUDGET == 5:\n",
    "\n",
    "        print(\"\\nPerforming grid search\")\n",
    "        optimization = HPO.GridSearchOptimization(\n",
    "            dataset, blackboxfunction_grid, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "        result, cost = optimization.fit()\n",
    "\n",
    "        index_best = 0\n",
    "        for m in range(len(result)):\n",
    "            if result[m][1] < result[index_best][1]:\n",
    "                index_best = m\n",
    "\n",
    "        best_score = result[index_best][1]\n",
    "        best_params = result[index_best][0]\n",
    "        \n",
    "\n",
    "        print(\"Best score with Grid search:\", best_score)\n",
    "\n",
    "        if VERBOSE > 0:\n",
    "            print(\"With Hyperparameters: \")\n",
    "            m = 0\n",
    "            for key in hyperparameterspace.keys():\n",
    "                if hyperparameterspace[key][0] == \"list\":\n",
    "                    index = int(\n",
    "                        best_params[m]*(len(hyperparameterspace_special[key])-1))\n",
    "                    print(key + \": \" +\n",
    "                        str(hyperparameterspace_special[key][index]))\n",
    "                else:\n",
    "                    print(key + \": \" + str(best_params[m]))\n",
    "                m += 1\n",
    "\n",
    "        print(\"Best score with Grid search:\", best_score)\n",
    "\n",
    "        RESULTS_GRID += \"(\" + str(cost) + \",\" + str(-best_score) + \")\"\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "    # ########################### RANDOM SEARCH #######################\n",
    "    print(\"\\nPerforming random search\")\n",
    "\n",
    "    optimization = HPO.RandomSearchOptimization(\n",
    "        dataset, blackboxfunction_random, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "    result, cost = optimization.fit()\n",
    "\n",
    "    index_best = 0\n",
    "    for m in range(len(result)):\n",
    "        if result[m][1] < result[index_best][1]:\n",
    "            index_best = m\n",
    "\n",
    "    best_score = result[index_best][1]\n",
    "    best_params = result[index_best][0]\n",
    "    \n",
    "    if VERBOSE > 0:\n",
    "        print(\"With Hyperparameters: \")\n",
    "        m = 0\n",
    "        for key in hyperparameterspace.keys():\n",
    "            if hyperparameterspace[key][0] == \"list\":\n",
    "                index = int(\n",
    "                    best_params[m]*(len(hyperparameterspace_special[key])-1))\n",
    "                print(key + \": \" +\n",
    "                      str(hyperparameterspace_special[key][index]))\n",
    "            else:\n",
    "                print(key + \": \" + str(best_params[m]))\n",
    "            m += 1\n",
    "\n",
    "    print(\"Best score with Random search:\", best_score)\n",
    "\n",
    "    RESULTS_RANDOM += \"(\" + str(cost) + \",\" + str(-best_score) + \")\"\n",
    "    \n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    ########################### BAYESIAN OPT #####################\n",
    "    print(\"\\nPerforming bayesian optimization\")\n",
    "\n",
    "    optimization = HPO.BayesianOptimization(\n",
    "        dataset, blackboxfunction_bayesian, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE)\n",
    "\n",
    "    result, cost = optimization.fit()\n",
    "\n",
    "    index_best = 0\n",
    "    for m in range(len(result)):\n",
    "        if result[m][1] < result[index_best][1]:\n",
    "            index_best = m\n",
    "\n",
    "    best_score = result[index_best][1]\n",
    "    best_params = result[index_best][0]\n",
    "\n",
    "    \n",
    "    \n",
    "    if VERBOSE > 0:\n",
    "        print(\"With Hyperparameters: \")\n",
    "        m = 0\n",
    "        for key in hyperparameterspace.keys():\n",
    "            if hyperparameterspace[key][0] == \"list\":\n",
    "                index = int(\n",
    "                    best_params[m]*(len(hyperparameterspace_special[key])-1))\n",
    "                print(key + \": \" +\n",
    "                      str(hyperparameterspace_special[key][index]))\n",
    "            else:\n",
    "                print(key + \": \" + str(best_params[m]))\n",
    "            m += 1\n",
    "    \n",
    "\n",
    "    print(\"Best score with Bayesian Optimization:\", best_score)\n",
    "\n",
    "\n",
    "    RESULTS_BAYESIAN += \"(\" + str(BUDGET) + \",\" + str(-best_score) + \")\"\n",
    "    \n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    ########################### SPARSE OPT ############################\n",
    "\n",
    "    print(\"\\nPerforming sparse search\")\n",
    "\n",
    "    f = ExampleFunction()\n",
    "\n",
    "    optimization = HPO.SparseGridSearchOptimization(\n",
    "        dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=SPARSE_PARAMS[0], adaptivity=SPARSE_PARAMS[1], optimizer=SPARSE_PARAMS[2])\n",
    "\n",
    "    [X0, fX0, X1, fX1, X2, fX2], cost = optimization.fit()\n",
    "\n",
    "    cost = cost + 2\n",
    "    bestFX = fX0 \n",
    "    bestX = X0\n",
    "    if fX1 < bestFX:\n",
    "        bestFX = fX1 \n",
    "        bestX = X1 \n",
    "    if fX2 < bestFX:\n",
    "        bestFX = fX2\n",
    "        bestX = X2\n",
    "\n",
    "    RESULTS_SPARSE += \"(\" + str(cost) + \",\" + str(-bestFX) + \")\"\n",
    "\n",
    "    if VERBOSE > 0:\n",
    "        print(\"With Hyperparameters: \")\n",
    "        m = 0\n",
    "        for key in hyperparameterspace.keys():\n",
    "            if hyperparameterspace[key][0] == \"list\":\n",
    "                index = int(\n",
    "                    X0[m]*(len(hyperparameterspace_special[key])-1))\n",
    "                print(key + \": \" +\n",
    "                      str(hyperparameterspace_special[key][index]))\n",
    "            else:\n",
    "                print(key + \": \" + str(X0[m]))\n",
    "            m += 1\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    print(\"GRID SEARCH\")\n",
    "    print(RESULTS_GRID+\"}\")\n",
    "\n",
    "    print(\"RANDOM SEARCH\")\n",
    "    print(RESULTS_RANDOM+\"}\")\n",
    "\n",
    "    print(\"BAYESIAN SEARCH\")\n",
    "    print(RESULTS_BAYESIAN+\"}\")\n",
    "\n",
    "    print(\"SPARSE SEARCH\")\n",
    "    print(RESULTS_SPARSE+\"}\")\n",
    "\n",
    "\n",
    "print(\"GRID SEARCH\")\n",
    "print(RESULTS_GRID+\"}\")\n",
    "\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(RESULTS_RANDOM+\"}\")\n",
    "\n",
    "print(\"BAYESIAN SEARCH\")\n",
    "print(RESULTS_BAYESIAN+\"}\")\n",
    "\n",
    "print(\"SPARSE SEARCH\")\n",
    "print(RESULTS_SPARSE+\"}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 400 0.01 2 2 2 2 5 0.5\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "\n",
    "import HPO\n",
    "import pysgpp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D, Conv2D\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from numpy.random import seed\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "seed(2)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.random.set_seed(3)\n",
    "\n",
    "VERBOSE = 1\n",
    "CV = 2 #[(slice(None), slice(None))]\n",
    "\n",
    "\n",
    "hyperparameterspace = {\n",
    "    'epochs': [\"interval-int\", 1, 10],\n",
    "    'batch_size': [\"interval-int\", 200, 1000],\n",
    "    'learning_rate': [\"interval-log\", 0.000000001, 0.1],\n",
    "    'number_conv_layers': [\"interval-int\", 1, 3],\n",
    "    'number_fc_layers': [\"interval-int\", 1, 3],\n",
    "    'kernel_size': [\"interval-int\", 1, 4],\n",
    "    'pool_size': [\"interval-int\", 1, 3],\n",
    "    'neurons_per_fc_layer': [\"interval-int\", 1, 7],\n",
    "    'dropout_prob': [\"interval\", 0, 1]\n",
    "}\n",
    "\n",
    "hyperparameterspace_special = {}\n",
    "for key in hyperparameterspace.keys():\n",
    "    liste = []\n",
    "    for i in range(1, len(hyperparameterspace[key])):\n",
    "        liste.append(hyperparameterspace[key][i])\n",
    "    hyperparameterspace_special[key] = liste\n",
    "\n",
    "\n",
    "\n",
    "epochs = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], 0.75))\n",
    "\n",
    "batch_size = int(HPO.from_standard(\n",
    "    hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], 0.25))\n",
    "\n",
    "learning_rate = HPO.from_standard_log(\n",
    "    hyperparameterspace_special[\"learning_rate\"][0], hyperparameterspace_special[\"learning_rate\"][1], 0.875)\n",
    "\n",
    "number_conv_layers = int(HPO.from_standard(\n",
    "    hyperparameterspace_special[\"number_conv_layers\"][0], hyperparameterspace_special[\"number_conv_layers\"][1], 0.5))\n",
    "\n",
    "number_fc_layers = int(HPO.from_standard(\n",
    "    hyperparameterspace_special[\"number_fc_layers\"][0], hyperparameterspace_special[\"number_fc_layers\"][1], 0.5))\n",
    "\n",
    "kernel_size = int(HPO.from_standard(\n",
    "    hyperparameterspace_special[\"kernel_size\"][0], hyperparameterspace_special[\"kernel_size\"][1], 0.5))\n",
    "\n",
    "pool_size = int(HPO.from_standard(\n",
    "    hyperparameterspace_special[\"pool_size\"][0], hyperparameterspace_special[\"pool_size\"][1], 0.5))\n",
    "\n",
    "neurons_per_fc_layer = int(HPO.from_standard(\n",
    "    hyperparameterspace_special[\"neurons_per_fc_layer\"][0], hyperparameterspace_special[\"neurons_per_fc_layer\"][1], 0.75))\n",
    "\n",
    "dropout_prob = HPO.from_standard(\n",
    "    hyperparameterspace_special[\"dropout_prob\"][0], hyperparameterspace_special[\"dropout_prob\"][1], 0.5)\n",
    "\n",
    "\n",
    "print(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
