{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 14:51:37.036834: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-17 14:51:38.168063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-17 14:51:40.337217: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_52587/3848610976.py:32: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "\n",
    "import HPO\n",
    "import pysgpp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D, Conv2D\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from numpy.random import seed\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "random.seed(1)\n",
    "seed(2)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "def reset_seeds():\n",
    "    os.environ['PYTHONHASHSEED']=str(0)\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.random.set_seed(3)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "VERBOSE = 1\n",
    "CV = 2 #[(slice(None), slice(None))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter space definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameterspace = {\n",
    "    'epochs': [\"interval-int\", 1, 10],\n",
    "    'batch_size': [\"interval-int\", 200, 1000],\n",
    "    'learning_rate': [\"interval-log\", 0.000000001, 0.1],\n",
    "    'number_conv_layers': [\"interval-int\", 1, 3],\n",
    "    'number_fc_layers': [\"interval-int\", 1, 3],\n",
    "    'kernel_size': [\"interval-int\", 1, 4],\n",
    "    'pool_size': [\"interval-int\", 1, 3],\n",
    "    'neurons_per_fc_layer': [\"interval-int\", 1, 7],\n",
    "    'dropout_prob': [\"interval\", 0, 0.999]\n",
    "}\n",
    "\n",
    "hyperparameterspace_special = {}\n",
    "for key in hyperparameterspace.keys():\n",
    "    liste = []\n",
    "    for i in range(1, len(hyperparameterspace[key])):\n",
    "        liste.append(hyperparameterspace[key][i])\n",
    "    hyperparameterspace_special[key] = liste\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "\n",
    "def create_model(learning_rate=1e-4, number_conv_layers=2, number_fc_layers=0, kernel_size=3, pool_size=2, neurons_per_fc_layer=4, dropout_prob=0.5):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "\n",
    "    for _ in range(number_conv_layers):\n",
    "        model.add(layers.Conv2D(32, kernel_size=(kernel_size, kernel_size), activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(pool_size, pool_size), padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(dropout_prob, seed=0))\n",
    "\n",
    "    for _ in range(number_fc_layers):\n",
    "        model.add(layers.Dense(neurons_per_fc_layer, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"Current_tests/\"+time.strftime(\"%H_%M_%S\", time.localtime())\n",
    "\n",
    "SPARSE_PARAMS = [2, 0.85, \"gradient_descent\"]\n",
    "\n",
    "BUDGETS = [5, 30, 50, 80, 110]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################## Current Budget: 5 ##################################################\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 149977ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 0ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 0ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 0ms.\n",
      "[5.00000000000000000000e-01, 5.00000000000000000000e-01, 5.00000000000000000000e-01, 5.00000000000000000000e-01, 5.00000000000000000000e-01, 5.00000000000000000000e-01, 5.00000000000000000000e-01, 5.00000000000000000000e-01, 5.00000000000000000000e-01] -0.1006999984383583 [4.65643551248526825114e-310, 4.65643551254257986606e-310, 5.43472210425371198594e-323, 0.00000000000000000000e+00] -0.1006999984383583 [4.65543622850771831041e-310, 8.39615708179144853984e-02, 7.09739583346426922139e-02, 7.85308687158802198880e-02, 4.38753567831636326346e-01, 1.16363813041150787608e-01, 8.51600269957661137887e-01, 9.40488166438209094444e-01, 3.31969262551328414546e-01] -0.14480000734329224\n",
      "1 267 3.6965081382307976e-09 1 1 1 2 6 0.3316372932887771\n",
      "GRID SEARCH\n",
      "{}\n",
      "RANDOM SEARCH\n",
      "{}\n",
      "BAYESIAN SEARCH\n",
      "{}\n",
      "SPARSE SEARCH\n",
      "{(3,0.14480000734329224)}\n",
      "\n",
      "################################################## Current Budget: 30 ##################################################\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n"
     ]
    }
   ],
   "source": [
    "################## MODEL AND FUNCTION DEFINITION ####################\n",
    "\n",
    "def evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=True):\n",
    "\n",
    "    # if too many layers and resulting image has no values left, return accuracy 0\n",
    "\n",
    "    # current_size = 28\n",
    "    # for _ in range(number_conv_layers + 1):\n",
    "    #     current_size = (current_size - kernel_size) + 1\n",
    "    #     current_size = (current_size - pool_size) + 1\n",
    "\n",
    "    # if current_size <= 1:\n",
    "    #     return 0\n",
    "\n",
    "\n",
    "    if deterministic:\n",
    "        reset_seeds()\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Scale images to the [0, 1] range\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "    # Make sure images have shape (28, 28, 1)\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model = create_model(learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "    model.fit(x_train, y_train, verbose=0, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=False)\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    K.clear_session()\n",
    "    del model \n",
    "    K.clear_session()\n",
    "\n",
    "    return -score[1]    \n",
    "\n",
    "    \n",
    "\n",
    "def blackboxfunction_grid(params):\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "def blackboxfunction_random(params):\n",
    "    \n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "def blackboxfunction_bayesian(params):\n",
    "    \n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = 10 ** params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "##################### Function for sparse grid search #####################\n",
    "\n",
    "class ExampleFunction(pysgpp.ScalarFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ExampleFunction, self).__init__(\n",
    "            len(hyperparameterspace.keys()))\n",
    "\n",
    "    def eval(self, x):\n",
    "\n",
    "        epochs = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[0]))\n",
    "\n",
    "        batch_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[1]))\n",
    "\n",
    "        learning_rate = HPO.from_standard_log(\n",
    "            hyperparameterspace_special[\"learning_rate\"][0], hyperparameterspace_special[\"learning_rate\"][1], x[2])\n",
    "\n",
    "        number_conv_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_conv_layers\"][0], hyperparameterspace_special[\"number_conv_layers\"][1], x[3]))\n",
    "\n",
    "        number_fc_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_fc_layers\"][0], hyperparameterspace_special[\"number_fc_layers\"][1], x[4]))\n",
    "\n",
    "        kernel_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"kernel_size\"][0], hyperparameterspace_special[\"kernel_size\"][1], x[5]))\n",
    "\n",
    "        pool_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"pool_size\"][0], hyperparameterspace_special[\"pool_size\"][1], x[6]))\n",
    "\n",
    "        neurons_per_fc_layer = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"neurons_per_fc_layer\"][0], hyperparameterspace_special[\"neurons_per_fc_layer\"][1], x[7]))\n",
    "\n",
    "        dropout_prob = HPO.from_standard(\n",
    "            hyperparameterspace_special[\"dropout_prob\"][0], hyperparameterspace_special[\"dropout_prob\"][1], x[8])\n",
    "\n",
    "        return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "\n",
    "\n",
    "RESULTS_GRID = \"{\"\n",
    "RESULTS_RANDOM = \"{\"\n",
    "RESULTS_BAYESIAN = \"{\"\n",
    "RESULTS_SPARSE = \"{\"\n",
    "\n",
    "dataset = HPO.Dataset([], [])\n",
    "\n",
    "##### For each dataset: run models with different budget #####\n",
    "for BUDGET in BUDGETS:\n",
    "\n",
    "    print(\"\\n################################################## Current Budget:\",\n",
    "            BUDGET, \"##################################################\")\n",
    "\n",
    "    ############################## GRID SEARCH #######################\n",
    "    # reset_seeds()\n",
    "    # if BUDGET == 5:\n",
    "\n",
    "    #     print(\"\\nPerforming grid search\")\n",
    "    #     optimization = HPO.GridSearchOptimization(\n",
    "    #         dataset, blackboxfunction_grid, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "    #     result, cost = optimization.fit()\n",
    "\n",
    "    #     index_best = 0\n",
    "    #     for m in range(len(result)):\n",
    "    #         if result[m][1] < result[index_best][1]:\n",
    "    #             index_best = m\n",
    "\n",
    "    #     best_score = result[index_best][1]\n",
    "    #     best_params = result[index_best][0]\n",
    "        \n",
    "\n",
    "    #     print(\"Best score with Grid search:\", best_score)\n",
    "\n",
    "    #     if VERBOSE > 0:\n",
    "    #         print(\"With Hyperparameters: \")\n",
    "    #         m = 0\n",
    "    #         for key in hyperparameterspace.keys():\n",
    "    #             if hyperparameterspace[key][0] == \"list\":\n",
    "    #                 index = int(\n",
    "    #                     best_params[m]*(len(hyperparameterspace_special[key])-1))\n",
    "    #                 print(key + \": \" +\n",
    "    #                     str(hyperparameterspace_special[key][index]))\n",
    "    #             else:\n",
    "    #                 print(key + \": \" + str(best_params[m]))\n",
    "    #             m += 1\n",
    "\n",
    "    #     print(\"Best score with Grid search:\", best_score)\n",
    "\n",
    "    #     RESULTS_GRID += \"(\" + str(cost) + \",\" + str(-best_score) + \")\"\n",
    "\n",
    "    #     K.clear_session()\n",
    "\n",
    "    # # ########################### RANDOM SEARCH #######################\n",
    "    # reset_seeds()\n",
    "    # print(\"\\nPerforming random search\")\n",
    "\n",
    "    # optimization = HPO.RandomSearchOptimization(\n",
    "    #     dataset, blackboxfunction_random, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "    # result, cost = optimization.fit()\n",
    "\n",
    "    # index_best = 0\n",
    "    # for m in range(len(result)):\n",
    "    #     if result[m][1] < result[index_best][1]:\n",
    "    #         index_best = m\n",
    "\n",
    "    # best_score = result[index_best][1]\n",
    "    # best_params = result[index_best][0]\n",
    "    \n",
    "    # if VERBOSE > 0:\n",
    "    #     print(\"With Hyperparameters: \")\n",
    "    #     m = 0\n",
    "    #     for key in hyperparameterspace.keys():\n",
    "    #         if hyperparameterspace[key][0] == \"list\":\n",
    "    #             index = int(\n",
    "    #                 best_params[m]*(len(hyperparameterspace_special[key])-1))\n",
    "    #             print(key + \": \" +\n",
    "    #                   str(hyperparameterspace_special[key][index]))\n",
    "    #         else:\n",
    "    #             print(key + \": \" + str(best_params[m]))\n",
    "    #         m += 1\n",
    "\n",
    "    # print(\"Best score with Random search:\", best_score)\n",
    "\n",
    "    # RESULTS_RANDOM += \"(\" + str(cost) + \",\" + str(-best_score) + \")\"\n",
    "    \n",
    "\n",
    "    # K.clear_session()\n",
    "\n",
    "    # ########################### BAYESIAN OPT #####################\n",
    "    # reset_seeds()\n",
    "    # print(\"\\nPerforming bayesian optimization\")\n",
    "\n",
    "    # optimization = HPO.BayesianOptimization(\n",
    "    #     dataset, blackboxfunction_bayesian, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE)\n",
    "\n",
    "    # result, cost = optimization.fit()\n",
    "\n",
    "    # index_best = 0\n",
    "    # for m in range(len(result)):\n",
    "    #     if result[m][1] < result[index_best][1]:\n",
    "    #         index_best = m\n",
    "\n",
    "    # best_score = result[index_best][1]\n",
    "    # best_params = result[index_best][0]\n",
    "\n",
    "    \n",
    "    \n",
    "    # if VERBOSE > 0:\n",
    "    #     print(\"With Hyperparameters: \")\n",
    "    #     m = 0\n",
    "    #     for key in hyperparameterspace.keys():\n",
    "    #         if hyperparameterspace[key][0] == \"list\":\n",
    "    #             index = int(\n",
    "    #                 best_params[m]*(len(hyperparameterspace_special[key])-1))\n",
    "    #             print(key + \": \" +\n",
    "    #                   str(hyperparameterspace_special[key][index]))\n",
    "    #         else:\n",
    "    #             print(key + \": \" + str(best_params[m]))\n",
    "    #         m += 1\n",
    "    \n",
    "\n",
    "    # print(\"Best score with Bayesian Optimization:\", best_score)\n",
    "\n",
    "\n",
    "    # RESULTS_BAYESIAN += \"(\" + str(BUDGET) + \",\" + str(-best_score) + \")\"\n",
    "    \n",
    "\n",
    "    # K.clear_session()\n",
    "\n",
    "    ########################### SPARSE OPT ############################\n",
    "    reset_seeds()\n",
    "\n",
    "    print(\"\\nPerforming sparse search\")\n",
    "\n",
    "    f = ExampleFunction()\n",
    "\n",
    "    optimization = HPO.SparseGridSearchOptimization(\n",
    "        dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=SPARSE_PARAMS[0], adaptivity=SPARSE_PARAMS[1], optimizer=SPARSE_PARAMS[2])\n",
    "\n",
    "    [X0, fX0, X1, fX1, X2, fX2], cost = optimization.fit()\n",
    "    print(X0, fX0, X1, fX1, X2, fX2)\n",
    "\n",
    "    cost = cost + 2\n",
    "    bestFX = fX0 \n",
    "    bestX = X0\n",
    "    if fX1 < bestFX:\n",
    "        bestFX = fX1 \n",
    "        bestX = X1 \n",
    "    if fX2 < bestFX:\n",
    "        bestFX = fX2\n",
    "        bestX = X2\n",
    "\n",
    "    RESULTS_SPARSE += \"(\" + str(cost) + \",\" + str(-bestFX) + \")\"\n",
    "\n",
    "    if VERBOSE > 0:\n",
    "        epochs = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], bestX[0]))\n",
    "\n",
    "        batch_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], bestX[1]))\n",
    "\n",
    "        learning_rate = HPO.from_standard_log(\n",
    "            hyperparameterspace_special[\"learning_rate\"][0], hyperparameterspace_special[\"learning_rate\"][1], bestX[2])\n",
    "\n",
    "        number_conv_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_conv_layers\"][0], hyperparameterspace_special[\"number_conv_layers\"][1], bestX[3]))\n",
    "\n",
    "        number_fc_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_fc_layers\"][0], hyperparameterspace_special[\"number_fc_layers\"][1], bestX[4]))\n",
    "\n",
    "        kernel_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"kernel_size\"][0], hyperparameterspace_special[\"kernel_size\"][1], bestX[5]))\n",
    "\n",
    "        pool_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"pool_size\"][0], hyperparameterspace_special[\"pool_size\"][1], bestX[6]))\n",
    "\n",
    "        neurons_per_fc_layer = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"neurons_per_fc_layer\"][0], hyperparameterspace_special[\"neurons_per_fc_layer\"][1], bestX[7]))\n",
    "\n",
    "        dropout_prob = HPO.from_standard(\n",
    "            hyperparameterspace_special[\"dropout_prob\"][0], hyperparameterspace_special[\"dropout_prob\"][1], bestX[8])\n",
    "\n",
    "\n",
    "        print(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    print(\"GRID SEARCH\")\n",
    "    print(RESULTS_GRID+\"}\")\n",
    "\n",
    "    print(\"RANDOM SEARCH\")\n",
    "    print(RESULTS_RANDOM+\"}\")\n",
    "\n",
    "    print(\"BAYESIAN SEARCH\")\n",
    "    print(RESULTS_BAYESIAN+\"}\")\n",
    "\n",
    "    print(\"SPARSE SEARCH\")\n",
    "    print(RESULTS_SPARSE+\"}\")\n",
    "\n",
    "\n",
    "print(\"GRID SEARCH\")\n",
    "print(RESULTS_GRID+\"}\")\n",
    "\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(RESULTS_RANDOM+\"}\")\n",
    "\n",
    "print(\"BAYESIAN SEARCH\")\n",
    "print(RESULTS_BAYESIAN+\"}\")\n",
    "\n",
    "print(\"SPARSE SEARCH\")\n",
    "print(RESULTS_SPARSE+\"}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
