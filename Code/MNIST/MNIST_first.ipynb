{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 22:13:51.067504: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-26 22:13:52.497590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "\n",
    "import HPO\n",
    "import pysgpp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D, Conv2D\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from numpy.random import seed\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "seed(2)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.random.set_seed(3)\n",
    "\n",
    "VERBOSE = 1\n",
    "CV = 2 #[(slice(None), slice(None))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter space definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameterspace = {\n",
    "    'epochs': [\"interval-int\", 1, 10],\n",
    "    'batch_size': [\"interval-int\", 200, 2050],\n",
    "    'learning_rate': [\"interval-log\", 0.000000001, 0.1],\n",
    "    'number_conv_layers': [\"interval-int\", 1, 4],\n",
    "    'number_fc_layers': [\"interval-int\", 1, 4],\n",
    "    'kernel_size': [\"interval-int\", 3, 5],\n",
    "    'pool_size': [\"interval-int\", 1, 3],\n",
    "    'neurons_per_fc_layer': [\"interval-int\", 1, 10],\n",
    "    'dropout_prob': [\"interval\", 0, 1]\n",
    "}\n",
    "\n",
    "hyperparameterspace_special = {}\n",
    "for key in hyperparameterspace.keys():\n",
    "    liste = []\n",
    "    for i in range(1, len(hyperparameterspace[key])):\n",
    "        liste.append(hyperparameterspace[key][i])\n",
    "    hyperparameterspace_special[key] = liste\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "\n",
    "def create_model(learning_rate=1e-4, number_conv_layers=2, number_fc_layers=0, kernel_size=3, pool_size=2, neurons_per_fc_layer=4, dropout_prob=0.5):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "\n",
    "    for _ in range(number_conv_layers):\n",
    "        model.add(layers.Conv2D(32, kernel_size=(kernel_size, kernel_size), activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(pool_size, pool_size), padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(dropout_prob))\n",
    "\n",
    "    for _ in range(number_fc_layers):\n",
    "        model.add(layers.Dense(neurons_per_fc_layer, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"Current_tests/\"+time.strftime(\"%H_%M_%S\", time.localtime())\n",
    "\n",
    "SPARSE_PARAMS = [2, 0.85, \"gradient_descent\"]\n",
    "\n",
    "BUDGETS = [3, 5, 10, 30, 50, 70, 90, 110]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################## Current Budget: 3 ##################################################\n",
      "\n",
      "Performing grid search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 22:13:55.202583: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score with Grid search: -0.12549999356269836\n",
      "With Hyperparameters: \n",
      "epochs: 5\n",
      "batch_size: 1125\n",
      "learning_rate: 9.999999999999997e-06\n",
      "number_conv_layers: 2\n",
      "number_fc_layers: 2\n",
      "kernel_size: 4\n",
      "pool_size: 2\n",
      "neurons_per_fc_layer: 5\n",
      "dropout_prob: 0.5\n",
      "Best score with Grid search: -0.12549999356269836\n",
      "\n",
      "Performing random search\n",
      "With Hyperparameters: \n",
      "epochs: 8\n",
      "batch_size: 949\n",
      "learning_rate: 0.00022801756022107158\n",
      "number_conv_layers: 2\n",
      "number_fc_layers: 3\n",
      "kernel_size: 3\n",
      "pool_size: 1\n",
      "neurons_per_fc_layer: 6\n",
      "dropout_prob: 0.6852195003967595\n",
      "Best score with Random search: -0.6018000245094299\n",
      "\n",
      "Performing bayesian optimization\n",
      "Iterations took 1380.5690248129977 seconds\n",
      "With Hyperparameters: \n",
      "epochs: 7.0\n",
      "batch_size: 689.0\n",
      "learning_rate: -4.7939778468317735\n",
      "number_conv_layers: 3.0\n",
      "number_fc_layers: 3.0\n",
      "kernel_size: 3.0\n",
      "pool_size: 1.0\n",
      "neurons_per_fc_layer: 8.0\n",
      "dropout_prob: 0.7715949980443677\n",
      "Best score with Bayesian Optimization: -0.18870000541210175\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n",
      "Done in 78625ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 0ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 0ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 1ms.\n",
      "With Hyperparameters: \n",
      "epochs: 0.5\n",
      "batch_size: 0.5\n",
      "learning_rate: 0.5\n",
      "number_conv_layers: 0.5\n",
      "number_fc_layers: 0.5\n",
      "kernel_size: 0.5\n",
      "pool_size: 0.5\n",
      "neurons_per_fc_layer: 0.5\n",
      "dropout_prob: 0.5\n",
      "GRID SEARCH\n",
      "{(1,0.12549999356269836)}\n",
      "RANDOM SEARCH\n",
      "{(3,0.6018000245094299)}\n",
      "BAYESIAN SEARCH\n",
      "{(3,0.18870000541210175)}\n",
      "SPARSE SEARCH\n",
      "{(3,0.12549999356269836)}\n",
      "\n",
      "################################################## Current Budget: 5 ##################################################\n",
      "\n",
      "Performing random search\n",
      "With Hyperparameters: \n",
      "epochs: 8\n",
      "batch_size: 949\n",
      "learning_rate: 0.00022801756022107158\n",
      "number_conv_layers: 2\n",
      "number_fc_layers: 3\n",
      "kernel_size: 3\n",
      "pool_size: 1\n",
      "neurons_per_fc_layer: 6\n",
      "dropout_prob: 0.6852195003967595\n",
      "Best score with Random search: -0.6134999990463257\n",
      "\n",
      "Performing bayesian optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations took 1426.718432362999 seconds\n",
      "With Hyperparameters: \n",
      "epochs: 7.0\n",
      "batch_size: 867.0\n",
      "learning_rate: -4.296442305644123\n",
      "number_conv_layers: 3.0\n",
      "number_fc_layers: 1.0\n",
      "kernel_size: 4.0\n",
      "pool_size: 1.0\n",
      "neurons_per_fc_layer: 3.0\n",
      "dropout_prob: 0.7903735226179732\n",
      "Best score with Bayesian Optimization: -0.5080999732017517\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n",
      "Done in 78805ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 0ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 0ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 0ms.\n",
      "With Hyperparameters: \n",
      "epochs: 0.5\n",
      "batch_size: 0.5\n",
      "learning_rate: 0.5\n",
      "number_conv_layers: 0.5\n",
      "number_fc_layers: 0.5\n",
      "kernel_size: 0.5\n",
      "pool_size: 0.5\n",
      "neurons_per_fc_layer: 0.5\n",
      "dropout_prob: 0.5\n",
      "GRID SEARCH\n",
      "{(1,0.12549999356269836)}\n",
      "RANDOM SEARCH\n",
      "{(3,0.6018000245094299)(5,0.6134999990463257)}\n",
      "BAYESIAN SEARCH\n",
      "{(3,0.18870000541210175)(5,0.5080999732017517)}\n",
      "SPARSE SEARCH\n",
      "{(3,0.12549999356269836)(3,0.3776000142097473)}\n",
      "\n",
      "################################################## Current Budget: 10 ##################################################\n",
      "\n",
      "Performing random search\n",
      "With Hyperparameters: \n",
      "epochs: 8\n",
      "batch_size: 1545\n",
      "learning_rate: 0.0003590883056819284\n",
      "number_conv_layers: 1\n",
      "number_fc_layers: 1\n",
      "kernel_size: 4\n",
      "pool_size: 2\n",
      "neurons_per_fc_layer: 7\n",
      "dropout_prob: 0.6637946452197888\n",
      "Best score with Random search: -0.8327000141143799\n",
      "\n",
      "Performing bayesian optimization\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"max_pooling2d_2\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_2/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,1,32].\n\nCall arguments received by layer \"max_pooling2d_2\" (type MaxPooling2D):\n  • inputs=tf.Tensor(shape=(None, 1, 1, 32), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 248\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mPerforming bayesian optimization\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m optimization \u001b[39m=\u001b[39m HPO\u001b[39m.\u001b[39mBayesianOptimization(\n\u001b[1;32m    246\u001b[0m     dataset, blackboxfunction_bayesian, hyperparameterspace, budget\u001b[39m=\u001b[39mBUDGET, verbosity\u001b[39m=\u001b[39mVERBOSE)\n\u001b[0;32m--> 248\u001b[0m result, cost \u001b[39m=\u001b[39m optimization\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m    250\u001b[0m index_best \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(result)):\n",
      "File \u001b[0;32m~/Repos/Masterarbeit/Code/MNIST/../HPO.py:620\u001b[0m, in \u001b[0;36mBayesianOptimization.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 620\u001b[0m     \u001b[39mreturn\u001b[39;00m bayesian_optimisation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbudget, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhyperparameterspace_processed, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampling_scales, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbosity)\n",
      "File \u001b[0;32m~/Repos/Masterarbeit/Code/MNIST/../HPO.py:329\u001b[0m, in \u001b[0;36mbayesian_optimisation\u001b[0;34m(n_iters, sample_loss, bounds, sampling_scales, verbosity, gp_params, alpha, epsilon)\u001b[0m\n\u001b[1;32m    325\u001b[0m             next_sample\u001b[39m.\u001b[39mappend(stats\u001b[39m.\u001b[39muniform\u001b[39m.\u001b[39mrvs(\n\u001b[1;32m    326\u001b[0m                 bounds[dim, \u001b[39m0\u001b[39m], bounds[dim, \u001b[39m1\u001b[39m]))\n\u001b[1;32m    328\u001b[0m \u001b[39m# Sample loss for new set of parameters\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m cv_score \u001b[39m=\u001b[39m sample_loss(next_sample)\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m verbosity \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    331\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent score:\u001b[39m\u001b[39m\"\u001b[39m, cv_score)\n",
      "Cell \u001b[0;32mIn[5], line 111\u001b[0m, in \u001b[0;36mblackboxfunction_bayesian\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    107\u001b[0m neurons_per_fc_layer \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(params[\u001b[39m7\u001b[39m])\n\u001b[1;32m    109\u001b[0m dropout_prob \u001b[39m=\u001b[39m params[\u001b[39m8\u001b[39m]\n\u001b[0;32m--> 111\u001b[0m \u001b[39mreturn\u001b[39;00m evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic)\u001b[0m\n\u001b[1;32m     30\u001b[0m y_train \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(y_train, num_classes)\n\u001b[1;32m     31\u001b[0m y_test \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(y_test, num_classes)\n\u001b[0;32m---> 33\u001b[0m model \u001b[39m=\u001b[39m create_model(learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n\u001b[1;32m     35\u001b[0m model\u001b[39m.\u001b[39mfit(x_train, y_train, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, batch_size\u001b[39m=\u001b[39mbatch_size, epochs\u001b[39m=\u001b[39mepochs, validation_split\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(number_conv_layers):\n\u001b[1;32m     12\u001b[0m     model\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mConv2D(\u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m(kernel_size, kernel_size), activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m     model\u001b[39m.\u001b[39;49madd(layers\u001b[39m.\u001b[39;49mMaxPooling2D(pool_size\u001b[39m=\u001b[39;49m(pool_size, pool_size)))\n\u001b[1;32m     14\u001b[0m model\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mFlatten())\n\u001b[1;32m     15\u001b[0m model\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDropout(dropout_prob))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1973\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1970\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[1;32m   1971\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1972\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m-> 1973\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n\u001b[1;32m   1975\u001b[0m \u001b[39m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[1;32m   1976\u001b[0m \u001b[39m# TF_Operation.\u001b[39;00m\n\u001b[1;32m   1977\u001b[0m \u001b[39mif\u001b[39;00m extract_traceback:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"max_pooling2d_2\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_2/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,1,32].\n\nCall arguments received by layer \"max_pooling2d_2\" (type MaxPooling2D):\n  • inputs=tf.Tensor(shape=(None, 1, 1, 32), dtype=float32)"
     ]
    }
   ],
   "source": [
    "################## MODEL AND FUNCTION DEFINITION ####################\n",
    "\n",
    "def evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=True):\n",
    "\n",
    "    # if too many layers and resulting image has no values left, return accuracy 0\n",
    "\n",
    "    current_size = 28\n",
    "    for _ in range(number_conv_layers + 1):\n",
    "        current_size = (current_size - kernel_size) + 1\n",
    "        current_size = (current_size - pool_size) + 1\n",
    "\n",
    "    if current_size <= 1:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    if deterministic:\n",
    "        reset_seeds()\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Scale images to the [0, 1] range\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "    # Make sure images have shape (28, 28, 1)\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model = create_model(learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "    model.fit(x_train, y_train, verbose=0, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=False)\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    K.clear_session()\n",
    "    del model \n",
    "    K.clear_session()\n",
    "\n",
    "    return -score[1]\n",
    "\n",
    "    \n",
    "\n",
    "def blackboxfunction_grid(params):\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "def blackboxfunction_random(params):\n",
    "    \n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "def blackboxfunction_bayesian(params):\n",
    "    \n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = 10 ** params[2]\n",
    "\n",
    "    number_conv_layers = int(params[3])\n",
    "\n",
    "    number_fc_layers = int(params[4])\n",
    "\n",
    "    kernel_size = int(params[5])\n",
    "\n",
    "    pool_size = int(params[6])\n",
    "\n",
    "    neurons_per_fc_layer = int(params[7])\n",
    "\n",
    "    dropout_prob = params[8]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "##################### Function for sparse grid search #####################\n",
    "\n",
    "class ExampleFunction(pysgpp.ScalarFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ExampleFunction, self).__init__(\n",
    "            len(hyperparameterspace.keys()))\n",
    "\n",
    "    def eval(self, x):\n",
    "\n",
    "        epochs = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[0]))\n",
    "\n",
    "        batch_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[1]))\n",
    "\n",
    "        learning_rate = HPO.from_standard_log(\n",
    "            hyperparameterspace_special[\"learning_rate\"][0], hyperparameterspace_special[\"learning_rate\"][1], x[2])\n",
    "\n",
    "        number_conv_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_conv_layers\"][0], hyperparameterspace_special[\"number_conv_layers\"][1], x[3]))\n",
    "\n",
    "        number_fc_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_fc_layers\"][0], hyperparameterspace_special[\"number_fc_layers\"][1], x[4]))\n",
    "\n",
    "        kernel_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"kernel_size\"][0], hyperparameterspace_special[\"kernel_size\"][1], x[5]))\n",
    "\n",
    "        pool_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"pool_size\"][0], hyperparameterspace_special[\"pool_size\"][1], x[6]))\n",
    "\n",
    "        neurons_per_fc_layer = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"neurons_per_fc_layer\"][0], hyperparameterspace_special[\"neurons_per_fc_layer\"][1], x[7]))\n",
    "\n",
    "        dropout_prob = HPO.from_standard(\n",
    "            hyperparameterspace_special[\"dropout_prob\"][0], hyperparameterspace_special[\"dropout_prob\"][1], x[8])\n",
    "\n",
    "        return evaluate_model(epochs, batch_size, learning_rate, number_conv_layers, number_fc_layers, kernel_size, pool_size, neurons_per_fc_layer, dropout_prob)\n",
    "\n",
    "\n",
    "\n",
    "RESULTS_GRID = \"{\"\n",
    "RESULTS_RANDOM = \"{\"\n",
    "RESULTS_BAYESIAN = \"{\"\n",
    "RESULTS_SPARSE = \"{\"\n",
    "\n",
    "dataset = HPO.Dataset([], [])\n",
    "\n",
    "##### For each dataset: run models with different budget #####\n",
    "for BUDGET in BUDGETS:\n",
    "\n",
    "    print(\"\\n################################################## Current Budget:\",\n",
    "            BUDGET, \"##################################################\")\n",
    "\n",
    "    ############################## GRID SEARCH #######################\n",
    "    if BUDGET == 3:\n",
    "\n",
    "        print(\"\\nPerforming grid search\")\n",
    "        optimization = HPO.GridSearchOptimization(\n",
    "            dataset, blackboxfunction_grid, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "        result, cost = optimization.fit()\n",
    "\n",
    "        index_best = 0\n",
    "        for m in range(len(result)):\n",
    "            if result[m][1] < result[index_best][1]:\n",
    "                index_best = m\n",
    "\n",
    "        best_score = result[index_best][1]\n",
    "        best_params = result[index_best][0]\n",
    "        \n",
    "\n",
    "        print(\"Best score with Grid search:\", best_score)\n",
    "\n",
    "        if VERBOSE > 0:\n",
    "            print(\"With Hyperparameters: \")\n",
    "            m = 0\n",
    "            for key in hyperparameterspace.keys():\n",
    "                if hyperparameterspace[key][0] == \"list\":\n",
    "                    index = int(\n",
    "                        best_params[m]*(len(hyperparameterspace_special[key])-1))\n",
    "                    print(key + \": \" +\n",
    "                        str(hyperparameterspace_special[key][index]))\n",
    "                else:\n",
    "                    print(key + \": \" + str(best_params[m]))\n",
    "                m += 1\n",
    "\n",
    "        print(\"Best score with Grid search:\", best_score)\n",
    "\n",
    "        RESULTS_GRID += \"(\" + str(cost) + \",\" + str(-best_score) + \")\"\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "    # ########################### RANDOM SEARCH #######################\n",
    "    print(\"\\nPerforming random search\")\n",
    "\n",
    "    optimization = HPO.RandomSearchOptimization(\n",
    "        dataset, blackboxfunction_random, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "    result, cost = optimization.fit()\n",
    "\n",
    "    index_best = 0\n",
    "    for m in range(len(result)):\n",
    "        if result[m][1] < result[index_best][1]:\n",
    "            index_best = m\n",
    "\n",
    "    best_score = result[index_best][1]\n",
    "    best_params = result[index_best][0]\n",
    "    \n",
    "    if VERBOSE > 0:\n",
    "        print(\"With Hyperparameters: \")\n",
    "        m = 0\n",
    "        for key in hyperparameterspace.keys():\n",
    "            if hyperparameterspace[key][0] == \"list\":\n",
    "                index = int(\n",
    "                    best_params[m]*(len(hyperparameterspace_special[key])-1))\n",
    "                print(key + \": \" +\n",
    "                      str(hyperparameterspace_special[key][index]))\n",
    "            else:\n",
    "                print(key + \": \" + str(best_params[m]))\n",
    "            m += 1\n",
    "\n",
    "    print(\"Best score with Random search:\", best_score)\n",
    "\n",
    "    RESULTS_RANDOM += \"(\" + str(cost) + \",\" + str(-best_score) + \")\"\n",
    "    \n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    ########################### BAYESIAN OPT #####################\n",
    "    print(\"\\nPerforming bayesian optimization\")\n",
    "\n",
    "    optimization = HPO.BayesianOptimization(\n",
    "        dataset, blackboxfunction_bayesian, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE)\n",
    "\n",
    "    result, cost = optimization.fit()\n",
    "\n",
    "    index_best = 0\n",
    "    for m in range(len(result)):\n",
    "        if result[m][1] < result[index_best][1]:\n",
    "            index_best = m\n",
    "\n",
    "    best_score = result[index_best][1]\n",
    "    best_params = result[index_best][0]\n",
    "\n",
    "    \n",
    "    \n",
    "    if VERBOSE > 0:\n",
    "        print(\"With Hyperparameters: \")\n",
    "        m = 0\n",
    "        for key in hyperparameterspace.keys():\n",
    "            if hyperparameterspace[key][0] == \"list\":\n",
    "                index = int(\n",
    "                    best_params[m]*(len(hyperparameterspace_special[key])-1))\n",
    "                print(key + \": \" +\n",
    "                      str(hyperparameterspace_special[key][index]))\n",
    "            else:\n",
    "                print(key + \": \" + str(best_params[m]))\n",
    "            m += 1\n",
    "    \n",
    "\n",
    "    print(\"Best score with Bayesian Optimization:\", best_score)\n",
    "\n",
    "\n",
    "    RESULTS_BAYESIAN += \"(\" + str(BUDGET) + \",\" + str(-best_score) + \")\"\n",
    "    \n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    ########################### SPARSE OPT ############################\n",
    "\n",
    "    print(\"\\nPerforming sparse search\")\n",
    "\n",
    "    f = ExampleFunction()\n",
    "\n",
    "    optimization = HPO.SparseGridSearchOptimization(\n",
    "        dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=SPARSE_PARAMS[0], adaptivity=SPARSE_PARAMS[1], optimizer=SPARSE_PARAMS[2])\n",
    "\n",
    "    [X0, fX0, X1, fX1, X2, fX2], cost = optimization.fit()\n",
    "\n",
    "    cost = cost + 2\n",
    "    bestFX = fX0 \n",
    "    bestX = X0\n",
    "    if fX1 < bestFX:\n",
    "        bestFX = fX1 \n",
    "        bestX = X1 \n",
    "    if fX2 < bestFX:\n",
    "        bestFX = fX2\n",
    "        bestX = X2\n",
    "\n",
    "    RESULTS_SPARSE += \"(\" + str(cost) + \",\" + str(-bestFX) + \")\"\n",
    "\n",
    "    if VERBOSE > 0:\n",
    "        print(\"With Hyperparameters: \")\n",
    "        m = 0\n",
    "        for key in hyperparameterspace.keys():\n",
    "            if hyperparameterspace[key][0] == \"list\":\n",
    "                index = int(\n",
    "                    X0[m]*(len(hyperparameterspace_special[key])-1))\n",
    "                print(key + \": \" +\n",
    "                      str(hyperparameterspace_special[key][index]))\n",
    "            else:\n",
    "                print(key + \": \" + str(X0[m]))\n",
    "            m += 1\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    print(\"GRID SEARCH\")\n",
    "    print(RESULTS_GRID+\"}\")\n",
    "\n",
    "    print(\"RANDOM SEARCH\")\n",
    "    print(RESULTS_RANDOM+\"}\")\n",
    "\n",
    "    print(\"BAYESIAN SEARCH\")\n",
    "    print(RESULTS_BAYESIAN+\"}\")\n",
    "\n",
    "    print(\"SPARSE SEARCH\")\n",
    "    print(RESULTS_SPARSE+\"}\")\n",
    "\n",
    "\n",
    "print(\"GRID SEARCH\")\n",
    "print(RESULTS_GRID+\"}\")\n",
    "\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(RESULTS_RANDOM+\"}\")\n",
    "\n",
    "print(\"BAYESIAN SEARCH\")\n",
    "print(RESULTS_BAYESIAN+\"}\")\n",
    "\n",
    "print(\"SPARSE SEARCH\")\n",
    "print(RESULTS_SPARSE+\"}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
