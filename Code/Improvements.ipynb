{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Experiments\n",
    "\n",
    "6 different datasets are used with ids [31, 1464, 334, 333, 1504]\n",
    "The different optimizers for sparse grid optimization are compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "\n",
    "from openml import tasks\n",
    "\n",
    "import HPO\n",
    "\n",
    "import pysgpp\n",
    "\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from scikeras.wrappers import KerasRegressor, KerasClassifier\n",
    "\n",
    "\n",
    "def to_standard(lower, upper, value):\n",
    "    return (value-lower)/(upper-lower)\n",
    "\n",
    "\n",
    "def from_standard(lower, upper, value):\n",
    "    return value*(upper-lower)+lower\n",
    "\n",
    "BUDGET = 50\n",
    "VERBOSE = 0\n",
    "CV = 2\n",
    "SCORING = 'neg_mean_squared_error'\n",
    "TESTING = False\n",
    "\n",
    "DATASETS = []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter space definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameterspace = {\n",
    "    'loss': [\"list\", 'mean_absolute_error', 'mean_squared_error'],\n",
    "    'epochs': [\"interval-int\", 1, 50],\n",
    "    'batch_size': [\"interval-int\", 40, 160],\n",
    "    'optimizer': [\"list\", 'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'],\n",
    "    'optimizer__learning_rate': [\"interval\", 0.0000001, 0.01]\n",
    "}\n",
    "\n",
    "hyperparameterspace_special = {}\n",
    "for key in hyperparameterspace.keys():\n",
    "    liste = []\n",
    "    for i in range(1, len(hyperparameterspace[key])):\n",
    "        liste.append(hyperparameterspace[key][i])\n",
    "    hyperparameterspace_special[key] = liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [233214, 233211, 359935]#, 359952, 359940, 359931, 317614, 359949, 359934, 359946, 359938, 359932, 359943]\n",
    "\n",
    "def relu_advanced(x):\n",
    "    return K.relu(x, max_value=250)\n",
    "\n",
    "ACTIVATION_FUNCTION = relu_advanced\n",
    "\n",
    "\n",
    "RESULTS_datasets = [[] for _ in range(len(ids))]\n",
    "\n",
    "valid_datasets = 0\n",
    "for i in range(len(ids)):\n",
    "\n",
    "    DATASETS.append(str(ids[i]))\n",
    "    \n",
    "    task = tasks.get_task(ids[i])\n",
    "\n",
    "    # Get dataset by ID\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    print(\"Current dataset:\", i, \"of\", len(ids), \"with name:\", dataset.name)\n",
    "\n",
    "    # Get the data itself as a dataframe (or otherwise)\n",
    "    data, target, _, _ = dataset.get_data(dataset.default_target_attribute, dataset_format=\"array\")\n",
    "    if np.isnan(data).any() or np.isnan(target).any():\n",
    "        print(\"NaN detected, skipping dataset\")\n",
    "        continue\n",
    "\n",
    "    X = torch.Tensor(data[:1000])\n",
    "    Y = torch.Tensor(target[:1000])\n",
    "\n",
    "    dataset = HPO.Dataset(X, Y)\n",
    "\n",
    "    class ExampleFunction(pysgpp.ScalarFunction):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(ExampleFunction, self).__init__(len(hyperparameterspace.keys()))\n",
    "\n",
    "\n",
    "        def eval(self, x):\n",
    "            index = int(x[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "            loss = hyperparameterspace_special[\"loss\"][index]\n",
    "            \n",
    "            epochs = int(from_standard(hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[1]))\n",
    "\n",
    "            batch_size = int(from_standard(hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[2]))\n",
    "\n",
    "            index = int(x[3]*(len(hyperparameterspace_special[\"optimizer\"])-1))\n",
    "            model_optimizer = hyperparameterspace_special[\"optimizer\"][index]\n",
    "\n",
    "            model_learning_rate = from_standard(hyperparameterspace_special[\"optimizer__learning_rate\"][0], hyperparameterspace_special[\"optimizer__learning_rate\"][1], x[4])\n",
    "            \n",
    "            # Function to create model, required for KerasClassifier\n",
    "            def create_model():\n",
    "                # create model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(20, input_shape=(len(dataset.get_X()[0]),), activation=ACTIVATION_FUNCTION))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dense(20, activation=ACTIVATION_FUNCTION))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dense(1, activation=ACTIVATION_FUNCTION))\n",
    "                # Compile model\n",
    "                if model_optimizer == 'SGD':\n",
    "                    optimizer = keras.optimizers.SGD(learning_rate=model_learning_rate)\n",
    "                elif model_optimizer == 'RMSprop':\n",
    "                    optimizer = keras.optimizers.RMSprop(learning_rate=model_learning_rate)\n",
    "                elif model_optimizer =='Adagrad':\n",
    "                    optimizer = keras.optimizers.Adagrad(learning_rate=model_learning_rate)\n",
    "                elif model_optimizer =='Adadelta':\n",
    "                    optimizer = keras.optimizers.Adadelta(learning_rate=model_learning_rate)\n",
    "                elif model_optimizer =='Adam':\n",
    "                    optimizer = keras.optimizers.Adam(learning_rate=model_learning_rate)\n",
    "                elif model_optimizer =='Adamax':\n",
    "                    optimizer = keras.optimizers.Adamax(learning_rate=model_learning_rate)\n",
    "                elif model_optimizer == 'Nadam':\n",
    "                    optimizer = keras.optimizers.Nadam(learning_rate=model_learning_rate)\n",
    "\n",
    "                model.compile(loss=loss, optimizer=optimizer,)\n",
    "                return model\n",
    "\n",
    "            model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "            model.fit(dataset.get_X_train(), dataset.get_Y_train(), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "            if TESTING:\n",
    "                Y_predicted = model.predict(dataset.get_X_test())\n",
    "                if np.any(np.isnan(Y_predicted)):\n",
    "                    return 1000\n",
    "                return sklearn.metrics.mean_squared_error(dataset.get_Y_test().tolist(), Y_predicted)\n",
    "            else:\n",
    "                Y_predicted = model.predict(dataset.get_X_validation())\n",
    "                if np.any(np.isnan(Y_predicted)):\n",
    "                    return 1000\n",
    "                return sklearn.metrics.mean_squared_error(dataset.get_Y_validation().tolist(), Y_predicted)\n",
    "              \n",
    "\n",
    "    available_optimizers = [\"adaptive_gradient_descent\", \"adaptive_newton\", \"bfgs\", \"differential_evolution\", \"gradient_descent\", \"nlcg\", \"nelder_mead\", \"newton\", \"rprop\"] # \"cmaes\", \n",
    "\n",
    "    results_temp = []\n",
    "    results_opt_temp = []\n",
    "\n",
    "    for j in range(len(available_optimizers)):\n",
    "        print(\"Current optimizer:\", j, available_optimizers[j])\n",
    "\n",
    "        sparse_params=[2, 0.95, available_optimizers[j]]\n",
    "\n",
    "        f = ExampleFunction()\n",
    "\n",
    "        optimization = HPO.SparseGridSearchOptimization(dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=sparse_params[0], adaptivity=sparse_params[1], optimizer=sparse_params[2])\n",
    "\n",
    "        result = optimization.fit()\n",
    "\n",
    "\n",
    "        TESTING = True\n",
    "        results_temp.append(f.eval(result[0]))\n",
    "        results_temp.append(f.eval(result[1]))\n",
    "        TESTING = False\n",
    "\n",
    "    for m in range(len(results_temp)):\n",
    "        RESULTS_datasets[i].append(results_temp[m])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(RESULTS_datasets)\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    print(\"Dataset with id\", ids[i])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    opts = []\n",
    "    k = 0\n",
    "    for optimizer_name in available_optimizers:\n",
    "        opts.append(str(k))\n",
    "        opts.append(str(k) + \"(o.)\")\n",
    "        k += 1\n",
    "\n",
    "    ax.bar(opts, RESULTS_datasets[i])\n",
    "    plt.ylim(0.9*min(RESULTS_datasets[i]), None)\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
