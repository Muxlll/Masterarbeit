{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline experiment\n",
    "\n",
    "Experiment to compare the 4 Optimization algorithms before trying to improve sparse search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 14:54:18.326071: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 14:54:19.408836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "\n",
    "from openml import tasks\n",
    "\n",
    "import HPO\n",
    "\n",
    "import pysgpp\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from scikeras.wrappers import KerasRegressor, KerasClassifier\n",
    "\n",
    "\n",
    "def to_standard(lower, upper, value):\n",
    "    return (value-lower)/(upper-lower)\n",
    "\n",
    "\n",
    "def from_standard(lower, upper, value):\n",
    "    return value*(upper-lower)+lower\n",
    "\n",
    "VERBOSE = 0\n",
    "CV = 2\n",
    "TESTING = False\n",
    "\n",
    "ITER = 1\n",
    "\n",
    "DATASETS = []\n",
    "\n",
    "GRID_RESULT = []\n",
    "RANDOM_RESULT = []\n",
    "BAYESIAN_RESULT = []\n",
    "SPARSE_RESULT = []\n",
    "SPARSE_RESULT_OPTIMIZED = []\n",
    "\n",
    "GRID_COST = []\n",
    "RANDOM_COST = []\n",
    "BAYESIAN_COST = []\n",
    "SPARSE_COST = []\n",
    "SPARSE_COST_OPTIMIZED = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter space definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameterspace = {\n",
    "    'loss': [\"list\", 'mean_absolute_error', 'mean_squared_error'],\n",
    "    'epochs': [\"interval-int\", 1, 40],\n",
    "    'batch_size': [\"interval-int\", 40, 160],\n",
    "    'optimizer__learning_rate': [\"interval-log\", 0.0000001, 0.001]\n",
    "}\n",
    "\n",
    "hyperparameterspace_special = {}\n",
    "for key in hyperparameterspace.keys():\n",
    "    liste = []\n",
    "    for i in range(1, len(hyperparameterspace[key])):\n",
    "        liste.append(hyperparameterspace[key][i])\n",
    "    hyperparameterspace_special[key] = liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dataset: 0 of 1 with name: Santander_transaction_value\n",
      "Performing grid search\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 159\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming grid search\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m model \u001b[38;5;241m=\u001b[39m KerasRegressor(model\u001b[38;5;241m=\u001b[39mcreate_model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m optimization \u001b[38;5;241m=\u001b[39m \u001b[43mHPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGridSearchOptimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameterspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbudget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBUDGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m result, cost \u001b[38;5;241m=\u001b[39m optimization\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    163\u001b[0m Y_predicted \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mpredict(dataset\u001b[38;5;241m.\u001b[39mget_X_test())\n",
      "File \u001b[0;32m~/Repos/Masterarbeit/Code/HPO.py:374\u001b[0m, in \u001b[0;36mGridSearchOptimization.__init__\u001b[0;34m(self, dataset, model, hyperparameterspace, budget, verbosity, cv, scoring)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameterspace_processed[key] \u001b[38;5;241m=\u001b[39m (upper\u001b[38;5;241m+\u001b[39mlower)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparam_per_dimension\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(param_per_dimension):\n\u001b[1;32m    376\u001b[0m     param_list\u001b[38;5;241m.\u001b[39mappend(math\u001b[38;5;241m.\u001b[39mexp(math\u001b[38;5;241m.\u001b[39mlog(lower) \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m*\u001b[39m step))\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "ids = [233214]#, 233211, 359935, 359952, 359940, 359931, 317614, 359949, 359934, 359946, 359938, 359932, 359943]\n",
    "\n",
    "def relu_advanced(x):\n",
    "    return K.relu(x)\n",
    "\n",
    "ACTIVATION_FUNCTION = relu_advanced\n",
    "\n",
    "valid_datasets = 0\n",
    "for i in range(len(ids)):\n",
    "    task = tasks.get_task(ids[i])\n",
    "\n",
    "    # Get dataset by ID\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    print(\"Current dataset:\", i, \"of\", len(ids), \"with name:\", dataset.name)\n",
    "\n",
    "    # Get the data itself as a dataframe (or otherwise)\n",
    "    data, target, categorical_indicator, names = dataset.get_data(dataset.default_target_attribute, dataset_format=\"array\")\n",
    "\n",
    "    # split into categorical and numerical features\n",
    "    categorical_features = [[x[i] for i in range(len(x)) if categorical_indicator[i]] for x in data]\n",
    "    numerical_features = [[x[i] for i in range(len(x)) if not categorical_indicator[i]] for x in data]\n",
    "    \n",
    "    if any(categorical_indicator):\n",
    "        # one hot encoding of the categorical one\n",
    "        encoder = OneHotEncoder(sparse_output=False).fit(categorical_features)\n",
    "        transformed = encoder.transform(categorical_features)\n",
    "\n",
    "        # bring back together\n",
    "        data = [numerical_features[i] + transformed[i].tolist() for i in range(len(numerical_features))]\n",
    "    else:\n",
    "        data = numerical_features\n",
    "\n",
    "\n",
    "    # additional scaling \n",
    "    scaler = StandardScaler().fit(data)\n",
    "    data = scaler.transform(data)\n",
    "\n",
    "    scaler = StandardScaler().fit(target.reshape(-1,1))\n",
    "    target = scaler.transform(target.reshape(-1,1))\n",
    "\n",
    "    X = torch.Tensor(data)\n",
    "    Y = torch.Tensor(target)\n",
    "\n",
    "    dataset = HPO.Dataset(X, Y)\n",
    "\n",
    "    ################## MODEL AND FUNCTION DEFINITION ####################\n",
    "\n",
    "    # Function to create model, required for KerasClassifier\n",
    "    def create_model(): \n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_shape=(len(dataset.get_X()[0]),), activation=ACTIVATION_FUNCTION))\n",
    "        model.add(Dense(20, activation=ACTIVATION_FUNCTION))\n",
    "        model.add(Dense(1, activation=None))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def blackboxfunction(params):\n",
    "        index = int(params[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "        loss = hyperparameterspace_special[\"loss\"][index]\n",
    "        \n",
    "        epochs = int(params[1])\n",
    "\n",
    "        batch_size = int(params[2])\n",
    "\n",
    "        model_learning_rate = params[3]\n",
    "\n",
    "        # Function to create model, required for KerasClassifier\n",
    "        def create_model():\n",
    "            # create model\n",
    "            model = Sequential()\n",
    "            model.add(Dense(20, input_shape=(len(dataset.get_X()[0]),), activation=ACTIVATION_FUNCTION))\n",
    "            model.add(Dense(20, activation=ACTIVATION_FUNCTION))\n",
    "            model.add(Dense(1, activation=None))\n",
    "            # Compile model\n",
    "            \n",
    "            optimizer = keras.optimizers.Adam(learning_rate=model_learning_rate)\n",
    "\n",
    "            model.compile(loss=loss, optimizer=optimizer)\n",
    "            return model\n",
    "\n",
    "        model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "        model.fit(dataset.get_X_train(), dataset.get_Y_train(), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "        if TESTING:\n",
    "            print(\"Test set is being used\")\n",
    "            Y_predicted = model.predict(dataset.get_X_test())\n",
    "            # if np.any(np.isnan(Y_predicted)):\n",
    "            #     return -1000\n",
    "            return -sklearn.metrics.mean_squared_error(dataset.get_Y_test(), Y_predicted)\n",
    "        else:\n",
    "            Y_predicted = model.predict(dataset.get_X_validation())\n",
    "            #if np.any(np.isnan(Y_predicted)):\n",
    "            #    return -1000\n",
    "            return -sklearn.metrics.mean_squared_error(dataset.get_Y_validation(), Y_predicted)\n",
    "        \n",
    "    ##################### Function for sparse grid search #####################\n",
    "\n",
    "    class ExampleFunction(pysgpp.ScalarFunction):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(ExampleFunction, self).__init__(len(hyperparameterspace.keys()))\n",
    "\n",
    "\n",
    "        def eval(self, x):\n",
    "            index = int(x[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "            loss = hyperparameterspace_special[\"loss\"][index]\n",
    "            \n",
    "            epochs = int(from_standard(hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[1]))\n",
    "\n",
    "            batch_size = int(from_standard(hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[2]))\n",
    "\n",
    "            model_learning_rate = from_standard(hyperparameterspace_special[\"optimizer__learning_rate\"][0], hyperparameterspace_special[\"optimizer__learning_rate\"][1], x[3])\n",
    "            \n",
    "            # Function to create model, required for KerasClassifier\n",
    "            def create_model():\n",
    "                # create model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(20, input_shape=(len(dataset.get_X()[0]),), activation=ACTIVATION_FUNCTION))\n",
    "                model.add(Dense(20, activation=ACTIVATION_FUNCTION))\n",
    "                model.add(Dense(1, activation=None))\n",
    "                # Compile model\n",
    "                \n",
    "                optimizer = keras.optimizers.Adam(learning_rate=model_learning_rate)\n",
    "\n",
    "                model.compile(loss=loss, optimizer=optimizer,)\n",
    "                return model\n",
    "\n",
    "            model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "            model.fit(dataset.get_X_train(), dataset.get_Y_train(), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "            if TESTING:\n",
    "                Y_predicted = model.predict(dataset.get_X_test())\n",
    "                # if np.any(np.isnan(Y_predicted)):\n",
    "                #     return 1000\n",
    "                return sklearn.metrics.mean_squared_error(dataset.get_Y_test().tolist(), Y_predicted)\n",
    "            else:\n",
    "                Y_predicted = model.predict(dataset.get_X_validation())\n",
    "                # if np.any(np.isnan(Y_predicted)):\n",
    "                #     return 1000\n",
    "                return sklearn.metrics.mean_squared_error(dataset.get_Y_validation().tolist(), Y_predicted)\n",
    "            \n",
    "\n",
    "\n",
    "    ##### For each dataset: run models with different budget #####\n",
    "\n",
    "    for j in range(ITER):\n",
    "        BUDGET = 5 + j * 30       \n",
    "                \n",
    "\n",
    "        ############################## GRID SEARCH #######################\n",
    "        print(\"Performing grid search\")\n",
    "        \n",
    "        model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "        optimization = HPO.GridSearchOptimization(dataset, model, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "        result, cost = optimization.fit()\n",
    "        \n",
    "        Y_predicted = result.predict(dataset.get_X_test())\n",
    "\n",
    "        GRID_RESULT.append(sklearn.metrics.mean_squared_error(dataset.get_Y_test(), Y_predicted))\n",
    "        GRID_COST.append(cost)\n",
    "\n",
    "        ########################### RANDOM SEARCH #######################\n",
    "        print(\"Performing random search\")\n",
    "        \n",
    "        model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "        optimization = HPO.RandomSearchOptimization(dataset, model, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "        result, cost = optimization.fit()\n",
    "\n",
    "        Y_predicted = result.predict(dataset.get_X_test())\n",
    "\n",
    "        RANDOM_RESULT.append(sklearn.metrics.mean_squared_error(dataset.get_Y_test(), Y_predicted))\n",
    "        RANDOM_COST.append(cost)\n",
    "\n",
    "        ########################### BAYESIAN OPT ##################### \n",
    "        print(\"Performing bayesian optimization\")\n",
    "\n",
    "        optimization = HPO.BayesianOptimization(dataset, blackboxfunction, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE)\n",
    "\n",
    "\n",
    "        result = optimization.fit()\n",
    "        \n",
    "        index_best = 0\n",
    "        for i in range(len(result[1])):\n",
    "            if result[1][i] == max(result[1]):\n",
    "                index_best = i\n",
    "\n",
    "        best_score = result[1][index_best]\n",
    "        best_params = result[0][index_best]\n",
    "\n",
    "        TESTING = True\n",
    "        BAYESIAN_RESULT.append(-blackboxfunction(best_params))\n",
    "        TESTING = False\n",
    "\n",
    "        BAYESIAN_COST.append(BUDGET)\n",
    "\n",
    "\n",
    "        ########################### SPARSE OPT ############################\n",
    "        print(\"Performing sparse search\")\n",
    "\n",
    "        f = ExampleFunction()\n",
    "\n",
    "        optimization = HPO.SparseGridSearchOptimization(dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=2, adaptivity=0.95, optimizer=\"gradient_descent\")\n",
    "\n",
    "        result = optimization.fit()\n",
    "\n",
    "        TESTING = True\n",
    "        SPARSE_RESULT.append(f.eval(result[0]))\n",
    "        SPARSE_RESULT_OPTIMIZED.append(f.eval(result[1]))\n",
    "        TESTING = False\n",
    "\n",
    "        SPARSE_COST.append(BUDGET)\n",
    "        SPARSE_COST_OPTIMIZED.append(BUDGET)\n",
    "\n",
    "    valid_datasets += 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GRID_RESULT)\n",
    "print(RANDOM_RESULT)\n",
    "print(BAYESIAN_RESULT)\n",
    "print(SPARSE_RESULT)\n",
    "print(SPARSE_RESULT_OPTIMIZED)\n",
    "count = 0\n",
    "for i in range(valid_datasets):\n",
    "    print(\"Current dataset:\", i, \"with name id:\", ids[i])\n",
    "    for j in range(ITER):\n",
    "        plt.plot(GRID_COST[count], GRID_RESULT[count], '+', color='black')\n",
    "        plt.plot(RANDOM_COST[count], RANDOM_RESULT[count], 'x', color='red')\n",
    "        plt.plot(BAYESIAN_COST[count], BAYESIAN_RESULT[count], '.', color='blue')\n",
    "        plt.plot(SPARSE_COST[count], SPARSE_RESULT[count], '+', color='purple')\n",
    "        plt.plot(SPARSE_COST_OPTIMIZED[count], SPARSE_RESULT_OPTIMIZED[count], 'x', color='pink')\n",
    "        plt.xlabel(\"Cost\")\n",
    "        plt.ylabel(\"Result (mean squared error)\")\n",
    "        #plt.yscale(\"log\")\n",
    "        plt.legend([\"Grid search\", \"Random search\", \"Bayesian Opt\", \"Sparse search\", \"Sparse search (opt)\"], bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "        count += 1\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
