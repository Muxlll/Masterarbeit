{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Experiments\n",
    "\n",
    "First experiment: adaptivity of the sparse grid is tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HPO\n",
    "\n",
    "import pysgpp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "VERBOSE = 1\n",
    "CV = 3 #[(slice(None), slice(None))]\n",
    "\n",
    "DATASETS = []\n",
    "\n",
    "RESULT0 = []\n",
    "RESULT1 = []\n",
    "RESULT2 = []\n",
    "RESULT3 = []\n",
    "RESULT4 = []\n",
    "\n",
    "RESULT0_OPT = []\n",
    "RESULT1_OPT = []\n",
    "RESULT2_OPT = []\n",
    "RESULT3_OPT = []\n",
    "RESULT4_OPT = []\n",
    "\n",
    "COST0 = []\n",
    "COST1 = []\n",
    "COST2 = []\n",
    "COST3 = []\n",
    "COST4 = []\n",
    "\n",
    "COST0_OPT = []\n",
    "COST1_OPT = []\n",
    "COST2_OPT = []\n",
    "COST3_OPT = []\n",
    "COST4_OPT = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter space definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 5\n",
    "\n",
    "hyperparameterspace = {\n",
    "    'epochs': [\"interval-int\", 1, 40],\n",
    "    'batch_size': [\"interval-int\", 1, 200],\n",
    "    'learning_rate': [\"interval-log\", 0.000000001, 0.1],\n",
    "    'number_layers': [\"interval-int\", 1, 20],\n",
    "    'neurons_per_layer': [\"interval-int\", 1, 50]\n",
    "}\n",
    "\n",
    "hyperparameterspace_special = {}\n",
    "for key in hyperparameterspace.keys():\n",
    "    liste = []\n",
    "    for i in range(1, len(hyperparameterspace[key])):\n",
    "        liste.append(hyperparameterspace[key][i])\n",
    "    hyperparameterspace_special[key] = liste\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_advanced(x):\n",
    "    return K.relu(x)\n",
    "\n",
    "\n",
    "ACTIVATION_FUNCTION = relu_advanced\n",
    "\n",
    "INITIALIZER = tf.keras.initializers.RandomNormal(stddev=0.05, seed=42)\n",
    "\n",
    "\n",
    "def create_model(learning_rate=0.0001, input_dim=10, number_layers=1, neurons_per_layer=20):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons_per_layer, input_shape=(input_dim,), activation=ACTIVATION_FUNCTION,\n",
    "                    kernel_initializer=INITIALIZER, bias_initializer=INITIALIZER))\n",
    "    for _ in range(number_layers):\n",
    "        model.add(Dense(neurons_per_layer, input_shape=(input_dim,), activation=ACTIVATION_FUNCTION,\n",
    "                        kernel_initializer=INITIALIZER, bias_initializer=INITIALIZER))\n",
    "    model.add(Dense(1, activation=None))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [233211]#, 359952, 359931, 359949, 359938]\n",
    "# [359940, 317614, 359934, 359946, 359932, 233214, 359943]\n",
    "\n",
    "ADAPTIVITIES = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "valid_datasets = 0\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    print(\"######################################################################################################################################################\")\n",
    "    print(\"Current Dataset:\", (i+1), \"of\", len(ids), \"with id:\", ids[i])\n",
    "\n",
    "    dataset = HPO.Dataset(task_id=ids[i])\n",
    "\n",
    "    print(\"The average value for target is:\", sum(\n",
    "        dataset.get_Y()/len(dataset.get_Y())))\n",
    "    print(\"Min target:\", min(dataset.get_Y()),\n",
    "          \"Max target:\", max(dataset.get_Y()))\n",
    "\n",
    "    current_dataset_0 = []\n",
    "    current_dataset_1 = []\n",
    "    current_dataset_2 = []\n",
    "    current_dataset_3 = []\n",
    "    current_dataset_4 = []\n",
    "\n",
    "    current_dataset_0_opt = []\n",
    "    current_dataset_1_opt = []\n",
    "    current_dataset_2_opt = []\n",
    "    current_dataset_3_opt = []\n",
    "    current_dataset_4_opt = []\n",
    "\n",
    "    current_dataset_cost_0 = []\n",
    "    current_dataset_cost_1 = []\n",
    "    current_dataset_cost_2 = []\n",
    "    current_dataset_cost_3 = []\n",
    "    current_dataset_cost_4 = []\n",
    "\n",
    "    current_dataset_cost_0_opt = []\n",
    "    current_dataset_cost_1_opt = []\n",
    "    current_dataset_cost_2_opt = []\n",
    "    current_dataset_cost_3_opt = []\n",
    "    current_dataset_cost_4_opt = []\n",
    "\n",
    "    ################## MODEL AND FUNCTION DEFINITION ####################\n",
    "\n",
    "    def evaluate_model(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer):\n",
    "\n",
    "        # return epochs + batch_size + learning_rate + number_of_layers + neurons_per_layer\n",
    "\n",
    "        kfold = KFold(n_splits=CV)\n",
    "\n",
    "        split = (kfold.split(dataset.get_X(), dataset.get_Y()))\n",
    "\n",
    "        values = []\n",
    "\n",
    "        numeric_features = [not x for x in dataset.get_categorical_indicator()]\n",
    "        numeric_transformer = Pipeline(\n",
    "            steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                   (\"scaler\", StandardScaler())]\n",
    "        )\n",
    "\n",
    "        categorical_transformer = Pipeline(\n",
    "            steps=[\n",
    "                (\"encoder\", OneHotEncoder(\n",
    "                    handle_unknown=\"infrequent_if_exist\", sparse_output=False)),\n",
    "                # (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", numeric_transformer, numeric_features),\n",
    "                (\"cat\", categorical_transformer,\n",
    "                 dataset.get_categorical_indicator()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i, (train_index, test_index) in enumerate(split):\n",
    "            X_train = dataset.get_X()[train_index]\n",
    "            Y_train = dataset.get_Y()[train_index]\n",
    "\n",
    "            X_val = dataset.get_X()[test_index]\n",
    "            Y_val = dataset.get_Y()[test_index]\n",
    "\n",
    "            preprocessor.fit(X_train, Y_train)\n",
    "\n",
    "            X_train = preprocessor.transform(X_train)\n",
    "            X_val = preprocessor.transform(X_val)\n",
    "\n",
    "            regressor = TransformedTargetRegressor(regressor=KerasRegressor(model=create_model,\n",
    "                                                                            learning_rate=learning_rate,\n",
    "                                                                            input_dim=len(\n",
    "                                                                                X_train[0]),\n",
    "                                                                            number_layers=number_of_layers,\n",
    "                                                                            neurons_per_layer=neurons_per_layer,\n",
    "                                                                            verbose=0),\n",
    "                                                   transformer=StandardScaler())\n",
    "\n",
    "            regressor.fit(X_train, Y_train, epochs=epochs,\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "            Y_predicted = regressor.predict(X_val)\n",
    "            # error = sklearn.metrics.mean_absolute_error(Y_predicted, Y_val)\n",
    "            error = sklearn.metrics.mean_absolute_percentage_error(\n",
    "                Y_predicted, Y_val)\n",
    "            values.append(error)\n",
    "\n",
    "            K.clear_session()\n",
    "\n",
    "        result = sum(values)/len(values)\n",
    "        return result\n",
    "    \n",
    "    ##################### Function for sparse grid search #####################\n",
    "\n",
    "    class ExampleFunction(pysgpp.ScalarFunction):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(ExampleFunction, self).__init__(\n",
    "                len(hyperparameterspace.keys()))\n",
    "\n",
    "        def eval(self, x):\n",
    "            # index = int(x[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "            # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "            epochs = int(HPO.from_standard(\n",
    "                hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[0]))\n",
    "\n",
    "            batch_size = int(HPO.from_standard(\n",
    "                hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[1]))\n",
    "\n",
    "            model_learning_rate = HPO.from_standard_log(hyperparameterspace_special[\"learning_rate\"][\n",
    "                                                        0], hyperparameterspace_special[\"learning_rate\"][1], x[2])\n",
    "\n",
    "            number_of_layers = int(HPO.from_standard(\n",
    "                hyperparameterspace_special[\"number_layers\"][0], hyperparameterspace_special[\"number_layers\"][1], x[3]))\n",
    "\n",
    "            neurons_per_layer = int(HPO.from_standard(\n",
    "                hyperparameterspace_special[\"neurons_per_layer\"][0], hyperparameterspace_special[\"neurons_per_layer\"][1], x[4]))\n",
    "\n",
    "            return evaluate_model(epochs, batch_size, model_learning_rate, number_of_layers, neurons_per_layer)\n",
    "\n",
    "    ##### For each dataset: run models with different budget #####\n",
    "    BUDGET = 1\n",
    "    for j in range(ITER):\n",
    "        \n",
    "        BUDGET = (j+1) * 10 #BUDGET + 2 ** len(hyperparameterspace.keys())\n",
    "\n",
    "        print(\"\\n################################################## Current Budget:\",\n",
    "              BUDGET, \"##################################################\")\n",
    "\n",
    "        ########################### SPARSE OPT ############################\n",
    "        print(\"\\nPerforming sparse search with adaptivity \", ADAPTIVITIES[0])\n",
    "\n",
    "        f = ExampleFunction()\n",
    "\n",
    "        optimization = HPO.SparseGridSearchOptimization(\n",
    "            dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=3, adaptivity=ADAPTIVITIES[0], optimizer=\"gradient_descent\")\n",
    "\n",
    "        result = optimization.fit()\n",
    "\n",
    "        print(\"Best score with Sparse Search:\", result[0][1], \"optimized:\", result[0][3])\n",
    "\n",
    "\n",
    "        current_dataset_0.append(result[0][1])\n",
    "        current_dataset_0_opt.append(result[0][3])\n",
    "\n",
    "        current_dataset_cost_0.append(result[1])\n",
    "        current_dataset_cost_0_opt.append(result[1]+1)\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        ########################### SPARSE OPT ############################\n",
    "        print(\"\\nPerforming sparse search with adaptivity \", ADAPTIVITIES[1])\n",
    "\n",
    "        f = ExampleFunction()\n",
    "\n",
    "        optimization = HPO.SparseGridSearchOptimization(\n",
    "            dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=3, adaptivity=ADAPTIVITIES[1], optimizer=\"gradient_descent\")\n",
    "\n",
    "        result = optimization.fit()\n",
    "\n",
    "        print(\"Best score with Sparse Search:\", result[0][1], \"optimized:\", result[0][3])\n",
    "\n",
    "\n",
    "        current_dataset_1.append(result[0][1])\n",
    "        current_dataset_1_opt.append(result[0][3])\n",
    "\n",
    "        current_dataset_cost_1.append(result[1])\n",
    "        current_dataset_cost_1_opt.append(result[1]+1)\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        ########################### SPARSE OPT ############################\n",
    "        print(\"\\nPerforming sparse search with adaptivity \", ADAPTIVITIES[2])\n",
    "\n",
    "        f = ExampleFunction()\n",
    "\n",
    "        optimization = HPO.SparseGridSearchOptimization(\n",
    "            dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=3, adaptivity=ADAPTIVITIES[2], optimizer=\"gradient_descent\")\n",
    "\n",
    "        result = optimization.fit()\n",
    "\n",
    "        print(\"Best score with Sparse Search:\", result[0][1], \"optimized:\", result[0][3])\n",
    "\n",
    "\n",
    "        current_dataset_2.append(result[0][1])\n",
    "        current_dataset_2_opt.append(result[0][3])\n",
    "\n",
    "        current_dataset_cost_2.append(result[1])\n",
    "        current_dataset_cost_2_opt.append(result[1]+1)\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        ########################### SPARSE OPT ############################\n",
    "        print(\"\\nPerforming sparse search with adaptivity \", ADAPTIVITIES[3])\n",
    "\n",
    "        f = ExampleFunction()\n",
    "\n",
    "        optimization = HPO.SparseGridSearchOptimization(\n",
    "            dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=3, adaptivity=ADAPTIVITIES[3], optimizer=\"gradient_descent\")\n",
    "\n",
    "        result = optimization.fit()\n",
    "\n",
    "        print(\"Best score with Sparse Search:\", result[0][1], \"optimized:\", result[0][3])\n",
    "\n",
    "\n",
    "        current_dataset_3.append(result[0][1])\n",
    "        current_dataset_3_opt.append(result[0][3])\n",
    "\n",
    "        current_dataset_cost_3.append(result[1])\n",
    "        current_dataset_cost_3_opt.append(result[1]+1)\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        ########################### SPARSE OPT ############################\n",
    "        print(\"\\nPerforming sparse search with adaptivity \", ADAPTIVITIES[4])\n",
    "\n",
    "        f = ExampleFunction()\n",
    "\n",
    "        optimization = HPO.SparseGridSearchOptimization(\n",
    "            dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=3, adaptivity=ADAPTIVITIES[4], optimizer=\"gradient_descent\")\n",
    "\n",
    "        result = optimization.fit()\n",
    "\n",
    "        print(\"Best score with Sparse Search:\", result[0][1], \"optimized:\", result[0][3])\n",
    "\n",
    "\n",
    "        current_dataset_4.append(result[0][1])\n",
    "        current_dataset_4_opt.append(result[0][3])\n",
    "\n",
    "        current_dataset_cost_4.append(result[1])\n",
    "        current_dataset_cost_4_opt.append(result[1]+1)\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    RESULT0.append(current_dataset_0)\n",
    "    RESULT1.append(current_dataset_1)\n",
    "    RESULT2.append(current_dataset_2)\n",
    "    RESULT3.append(current_dataset_3)\n",
    "    RESULT4.append(current_dataset_4)\n",
    "\n",
    "    RESULT0_OPT.append(current_dataset_0_opt)\n",
    "    RESULT1_OPT.append(current_dataset_1_opt)\n",
    "    RESULT2_OPT.append(current_dataset_2_opt)\n",
    "    RESULT3_OPT.append(current_dataset_3_opt)\n",
    "    RESULT4_OPT.append(current_dataset_4_opt)\n",
    "\n",
    "    COST0.append(current_dataset_cost_0)\n",
    "    COST1.append(current_dataset_cost_1)\n",
    "    COST2.append(current_dataset_cost_2)\n",
    "    COST3.append(current_dataset_cost_3)\n",
    "    COST4.append(current_dataset_cost_4)\n",
    "\n",
    "    COST0_OPT.append(current_dataset_cost_0_opt)\n",
    "    COST1_OPT.append(current_dataset_cost_1_opt)\n",
    "    COST2_OPT.append(current_dataset_cost_2_opt)\n",
    "    COST3_OPT.append(current_dataset_cost_3_opt)\n",
    "    COST4_OPT.append(current_dataset_cost_4_opt)\n",
    "\n",
    "\n",
    "    print(\"###################### Current dataset\", ids[i], \"######################\")\n",
    "    \n",
    "    dataset = HPO.Dataset(task_id=ids[i])\n",
    "    \n",
    "    print(\"Target average:\", sum(\n",
    "        dataset.get_Y()/len(dataset.get_Y())))\n",
    "    print(\"Min target:\", min(dataset.get_Y()),\n",
    "          \"Max target:\", max(dataset.get_Y()))\n",
    "\n",
    "    # plotting the points \n",
    "    plt.plot(COST0[i], RESULT0[i], '.-', color='black', label=\"Adapt.:\" + str(ADAPTIVITIES[0]))\n",
    "    plt.plot(COST1[i], RESULT1[i], '.-', color='red', label=\"Adapt.:\" + str(ADAPTIVITIES[1]))\n",
    "    plt.plot(COST2[i], RESULT2[i], '.-', color='blue', label=\"Adapt.:\" + str(ADAPTIVITIES[2]))\n",
    "    plt.plot(COST3[i], RESULT3[i], '.-', color='purple', label=\"Adapt.:\" + str(ADAPTIVITIES[3]))\n",
    "    plt.plot(COST4[i], RESULT4[i], '.-', color='pink', label=\"Adapt.:\" + str(ADAPTIVITIES[4]))\n",
    "    \n",
    "    # naming the x axis\n",
    "    plt.xlabel('Function evaluations')\n",
    "    # naming the y axis\n",
    "    plt.ylabel('Result')\n",
    "    \n",
    "    # show a legend on the plot\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Current_tests/task_id\"+str(ids[i])+\"(not_optimized)\")\n",
    "    # function to show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # plotting the points \n",
    "    plt.plot(COST0_OPT[i], RESULT0_OPT[i], '.-', color='black', label=\"Adapt.:\" + str(ADAPTIVITIES[0]))\n",
    "    plt.plot(COST1_OPT[i], RESULT1_OPT[i], '.-', color='red', label=\"Adapt.:\" + str(ADAPTIVITIES[1]))\n",
    "    plt.plot(COST2_OPT[i], RESULT2_OPT[i], '.-', color='blue', label=\"Adapt.:\" + str(ADAPTIVITIES[2]))\n",
    "    plt.plot(COST3_OPT[i], RESULT3_OPT[i], '.-', color='purple', label=\"Adapt.:\" + str(ADAPTIVITIES[3]))\n",
    "    plt.plot(COST4_OPT[i], RESULT4_OPT[i], '.-', color='pink', label=\"Adapt.:\" + str(ADAPTIVITIES[4]))\n",
    "    \n",
    "    # naming the x axis\n",
    "    plt.xlabel('Function evaluations')\n",
    "    # naming the y axis\n",
    "    plt.ylabel('Result')\n",
    "    \n",
    "    # show a legend on the plot\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Current_tests/task_id\"+str(ids[i])+\"(optimized)\")\n",
    "    # function to show the plot\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
