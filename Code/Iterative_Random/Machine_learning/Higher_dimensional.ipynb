{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline experiment\n",
    "\n",
    "Experiment to compare the 4 Optimization algorithms before trying to improve sparse search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 09:25:08.898495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-24 09:25:10.520528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "import HPO\n",
    "import pysgpp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from numpy.random import seed\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from matplotlib import cm\n",
    "import matplotlib \n",
    "\n",
    "import operator\n",
    "\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "random.seed(1)\n",
    "seed(2)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.random.set_seed(3)\n",
    "\n",
    "VERBOSE = 0\n",
    "\n",
    "SPARSE_RESULT = []\n",
    "SPARSE_RESULT_OPTIMIZED = []\n",
    "\n",
    "SPARSE_COST = []\n",
    "SPARSE_COST_OPTIMIZED = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate=0.0001, input_dim=10, number_layers=1, neurons_per_layer=20, dropout_prob=0.2):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(int(neurons_per_layer), input_shape=(input_dim,), activation='relu'))\n",
    "    for _ in range(int(number_layers)):\n",
    "        #model.add(Dropout(dropout_prob, seed=0))    \n",
    "        model.add(Dense(int(neurons_per_layer), activation='relu'))\n",
    "    model.add(Dense(1, activation=None))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameterspace = {\n",
    "    'epochs': [\"interval-int\", 1, 30],\n",
    "    'batch_size': [\"interval-int\", 100, 1000],\n",
    "    'learning rate': [\"interval-log\", 1e-10, 1e-1],\n",
    "    'number_layers': [\"interval-int\", 1, 10],\n",
    "    'neurons_per_layer': [\"interval-int\", 1, 40],\n",
    "    'dropout_prob': [\"interval\", 0, 0.999]\n",
    "}\n",
    "\n",
    "hyperparameterspace_special = {}\n",
    "for key in hyperparameterspace.keys():\n",
    "    liste = []\n",
    "    for i in range(1, len(hyperparameterspace[key])):\n",
    "        liste.append(hyperparameterspace[key][i])\n",
    "    hyperparameterspace_special[key] = liste"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(50,0.08873636480420828)}\n",
      "\n",
      "{(50,0.10816064774990082)}\n",
      "\n",
      "{(50,0.10400958489626647)}\n",
      "\n",
      "{(50,0.08873636480420828)}(70,0.0822725860401988)}\n",
      "\n",
      "{(50,0.10816064774990082)}(70,0.104046599753201)}\n",
      "\n",
      "{(50,0.10400958489626647)}(70,0.10320316888391971)}\n",
      "\n",
      "{(50,0.08873636480420828)}(70,0.0822725860401988)}(90,0.08675474766641855)}\n",
      "\n",
      "{(50,0.10816064774990082)}(70,0.104046599753201)}(90,0.09519372526556254)}\n",
      "\n",
      "{(50,0.10400958489626647)}(70,0.10320316888391971)}(90,0.08913029441609979)}\n",
      "\n",
      "{(50,0.08873636480420828)}(70,0.0822725860401988)}(90,0.08675474766641855)}(110,0.09002064857631922)}\n",
      "\n",
      "{(50,0.10816064774990082)}(70,0.104046599753201)}(90,0.09519372526556254)}(110,0.1040460741147399)}\n",
      "\n",
      "{(50,0.10400958489626647)}(70,0.10320316888391971)}(90,0.08913029441609979)}(110,0.10749386306852102)}\n",
      "\n",
      "{(50,0.08873636480420828)}(70,0.0822725860401988)}(90,0.08675474766641855)}(110,0.09002064857631922)}(130,0.09970792215317488)}\n",
      "\n",
      "{(50,0.10816064774990082)}(70,0.104046599753201)}(90,0.09519372526556254)}(110,0.1040460741147399)}(130,0.08887130990624428)}\n",
      "\n",
      "{(50,0.10400958489626647)}(70,0.10320316888391971)}(90,0.08913029441609979)}(110,0.10749386306852102)}(130,0.08567627016454935)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################## MODEL AND FUNCTION DEFINITION ####################\n",
    "CV = 2\n",
    "dataset = HPO.Dataset(task_id=359938)\n",
    "\n",
    "def evaluate_model(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer, dropout_prob, deterministic=True):\n",
    "\n",
    "    #return epochs + batch_size + learning_rate + number_of_layers + neurons_per_layer\n",
    "\n",
    "\n",
    "    kfold = KFold(n_splits=CV)\n",
    "\n",
    "    split = (kfold.split(dataset.get_X(), dataset.get_Y()))\n",
    "\n",
    "    values = []\n",
    "\n",
    "    numeric_features = [not x for x in dataset.get_categorical_indicator()]\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"encoder\", OneHotEncoder(\n",
    "                handle_unknown=\"infrequent_if_exist\", sparse_output=False)),\n",
    "            # (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer,\n",
    "                dataset.get_categorical_indicator()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(split):\n",
    "\n",
    "        if deterministic:\n",
    "            reset_seeds()\n",
    "\n",
    "        X_train = dataset.get_X()[train_index]\n",
    "        Y_train = dataset.get_Y()[train_index]\n",
    "\n",
    "        X_val = dataset.get_X()[test_index]\n",
    "        Y_val = dataset.get_Y()[test_index]\n",
    "\n",
    "        preprocessor.fit(X_train, Y_train)\n",
    "\n",
    "        X_train = preprocessor.transform(X_train)\n",
    "        X_val = preprocessor.transform(X_val)\n",
    "\n",
    "        regressor = KerasRegressor(model=create_model,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    input_dim=len(\n",
    "                                        X_train[0]),\n",
    "                                    number_layers=number_of_layers,\n",
    "                                    neurons_per_layer=neurons_per_layer,\n",
    "                                    dropout_prob=dropout_prob,\n",
    "                                    verbose=0)\n",
    "\n",
    "        regressor = TransformedTargetRegressor(regressor=regressor,\n",
    "                                                transformer=StandardScaler())\n",
    "\n",
    "        regressor.fit(X_train, Y_train, epochs=int(epochs),\n",
    "                        batch_size=int(batch_size), shuffle=False)\n",
    "\n",
    "        Y_predicted = regressor.predict(X_val)\n",
    "        # error = sklearn.metrics.mean_absolute_error(Y_predicted, Y_val)\n",
    "        error = sklearn.metrics.mean_absolute_percentage_error(\n",
    "            Y_predicted, Y_val)\n",
    "        values.append(error)\n",
    "\n",
    "        del regressor\n",
    "        K.clear_session()\n",
    "\n",
    "    result = sum(values)/len(values)\n",
    "    return result\n",
    "\n",
    "def function(coordinates):\n",
    "        \n",
    "    return evaluate_model(epochs=coordinates[0], batch_size=coordinates[1], learning_rate=coordinates[2], number_of_layers=coordinates[3], neurons_per_layer=coordinates[4], dropout_prob=coordinates[5], deterministic=False)\n",
    "\n",
    "\n",
    "N_ITERS = 10\n",
    "\n",
    "ALT_1 = \"{\"\n",
    "ALT_2 = \"{\"\n",
    "ALT_3 = \"{\"\n",
    "\n",
    "BUDGETS = [50, 70, 90, 110, 130]\n",
    "\n",
    "for budget in BUDGETS:\n",
    "\n",
    "    summe = 0\n",
    "    for i in range(N_ITERS):\n",
    "\n",
    "        optimizer = HPO.IterativeRandomOptimization(dataset, function, \n",
    "                                                            hyperparameterspace, budget, 0, 0.9, \n",
    "                                                            init_points=30, \n",
    "                                                            alternative=0, ref_per_step=4)\n",
    "        points = optimizer.fit()\n",
    "\n",
    "        points.sort(key=operator.attrgetter('value'))\n",
    "        summe += points[0].get_value()\n",
    "\n",
    "    ALT_1 += \"(\" + str(budget) + \",\" + str(summe/N_ITERS) + \")\"\n",
    "\n",
    "\n",
    "    summe = 0\n",
    "    for i in range(N_ITERS):\n",
    "\n",
    "        optimizer = HPO.IterativeRandomOptimization(dataset, function, \n",
    "                                                            hyperparameterspace, budget, 0, 0.8, \n",
    "                                                            init_points=30, \n",
    "                                                            alternative=1, ref_per_step=4)\n",
    "        points = optimizer.fit()\n",
    "\n",
    "        points.sort(key=operator.attrgetter('value'))\n",
    "        summe += points[0].get_value()\n",
    "\n",
    "    ALT_2 += \"(\" + str(budget) + \",\" + str(summe/N_ITERS) + \")\"\n",
    "\n",
    "\n",
    "    summe = 0\n",
    "    for i in range(N_ITERS):\n",
    "\n",
    "        optimizer = HPO.IterativeRandomOptimization(dataset, function, \n",
    "                                                            hyperparameterspace, budget, 0, 0.9, \n",
    "                                                            init_points=30, \n",
    "                                                            alternative=2, ref_per_step=4)\n",
    "        points = optimizer.fit()\n",
    "\n",
    "        points.sort(key=operator.attrgetter('value'))\n",
    "        summe += points[0].get_value()\n",
    "\n",
    "    ALT_3 += \"(\" + str(budget) + \",\" + str(summe/N_ITERS) + \")\"\n",
    "\n",
    "\n",
    "\n",
    "    ALT_1 += \"}\"\n",
    "    ALT_2 += \"}\"\n",
    "    ALT_3 += \"}\"\n",
    "\n",
    "    print(ALT_1+\"\\n\")\n",
    "    print(ALT_2+\"\\n\")\n",
    "    print(ALT_3+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search\n",
      "Best score with Grid search: 0.21640029549598694\n",
      "64,0.21640029549598694\n",
      "\n",
      "Performing random search\n",
      "Best score with Random search: 0.05757850594818592\n",
      "130,0.05757850594818592\n",
      "\n",
      "Performing bayesian optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/maxi/.local/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score with Bayesian Optimization: 0.07449636794626713\n",
      "130,0.07449636794626713\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n",
      "Done in 411367ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 2ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 0ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 1ms.\n",
      "Sparse grid search\n",
      "123,0.06948636285960674\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = HPO.Dataset(task_id=359938)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer, dropout_prob, deterministic=True):\n",
    "\n",
    "    #return epochs + batch_size + learning_rate + number_of_layers + neurons_per_layer\n",
    "\n",
    "\n",
    "    kfold = KFold(n_splits=CV)\n",
    "\n",
    "    split = (kfold.split(dataset.get_X(), dataset.get_Y()))\n",
    "\n",
    "    values = []\n",
    "\n",
    "    numeric_features = [not x for x in dataset.get_categorical_indicator()]\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"encoder\", OneHotEncoder(\n",
    "                handle_unknown=\"infrequent_if_exist\", sparse_output=False)),\n",
    "            # (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer,\n",
    "                dataset.get_categorical_indicator()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(split):\n",
    "\n",
    "        if deterministic:\n",
    "            reset_seeds()\n",
    "\n",
    "        X_train = dataset.get_X()[train_index]\n",
    "        Y_train = dataset.get_Y()[train_index]\n",
    "\n",
    "        X_val = dataset.get_X()[test_index]\n",
    "        Y_val = dataset.get_Y()[test_index]\n",
    "\n",
    "        preprocessor.fit(X_train, Y_train)\n",
    "\n",
    "        X_train = preprocessor.transform(X_train)\n",
    "        X_val = preprocessor.transform(X_val)\n",
    "\n",
    "        regressor = KerasRegressor(model=create_model,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    input_dim=len(\n",
    "                                        X_train[0]),\n",
    "                                    number_layers=number_of_layers,\n",
    "                                    neurons_per_layer=neurons_per_layer,\n",
    "                                    dropout_prob=dropout_prob,\n",
    "                                    verbose=0)\n",
    "\n",
    "        regressor = TransformedTargetRegressor(regressor=regressor,\n",
    "                                                transformer=StandardScaler())\n",
    "\n",
    "        regressor.fit(X_train, Y_train, epochs=int(epochs),\n",
    "                        batch_size=int(batch_size), shuffle=False)\n",
    "\n",
    "        Y_predicted = regressor.predict(X_val)\n",
    "        # error = sklearn.metrics.mean_absolute_error(Y_predicted, Y_val)\n",
    "        error = sklearn.metrics.mean_absolute_percentage_error(\n",
    "            Y_predicted, Y_val)\n",
    "        values.append(error)\n",
    "\n",
    "        del regressor\n",
    "        K.clear_session()\n",
    "\n",
    "    result = sum(values)/len(values)\n",
    "    return result\n",
    "\n",
    "CV = 2\n",
    "\n",
    "def blackboxfunction_grid(params):\n",
    "    # index = int(params[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "    # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_of_layers = int(params[3])\n",
    "\n",
    "    neurons_per_layer = int(params[4])\n",
    "\n",
    "    dropout_prob = params[5]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer, dropout_prob)\n",
    "\n",
    "def blackboxfunction_random(params):\n",
    "    # index = int(params[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "    # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_of_layers = int(params[3])\n",
    "\n",
    "    neurons_per_layer = int(params[4])\n",
    "\n",
    "    dropout_prob = params[5]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "def blackboxfunction_bayesian(params):\n",
    "    # index = int(params[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "    # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    model_learning_rate = 10 ** (params[2])\n",
    "\n",
    "    number_of_layers = int(params[3])\n",
    "\n",
    "    neurons_per_layer = int(params[4])\n",
    "\n",
    "    dropout_prob = params[5]\n",
    "\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, model_learning_rate, number_of_layers, neurons_per_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "##################### Function for sparse grid search #####################\n",
    "\n",
    "class ExampleFunction(pysgpp.ScalarFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ExampleFunction, self).__init__(\n",
    "            len(hyperparameterspace.keys()))\n",
    "\n",
    "    def eval(self, x):\n",
    "        # index = int(x[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "        # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "        epochs = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[0]))\n",
    "\n",
    "        batch_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[1]))\n",
    "\n",
    "        # HPO.from_standard_log(hyperparameterspace_special[\"learning_rate\"][\n",
    "        model_learning_rate = HPO.from_standard_log(\n",
    "            hyperparameterspace_special[\"learning rate\"][0], hyperparameterspace_special[\"learning rate\"][1], x[2])\n",
    "\n",
    "        number_of_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_layers\"][0], hyperparameterspace_special[\"number_layers\"][1], x[3]))\n",
    "\n",
    "        neurons_per_layer = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"neurons_per_layer\"][0], hyperparameterspace_special[\"neurons_per_layer\"][1], x[4]))\n",
    "        \n",
    "        dropout_prob = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"dropout_prob\"][0], hyperparameterspace_special[\"dropout_prob\"][1], x[5]))\n",
    "\n",
    "        return evaluate_model(epochs, batch_size, model_learning_rate, number_of_layers, neurons_per_layer, dropout_prob)\n",
    "    \n",
    "\n",
    "BUDGET = 130\n",
    "\n",
    "print(\"\\nPerforming grid search\")\n",
    "optimization = HPO.GridSearchOptimization(\n",
    "    dataset, blackboxfunction_grid, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "result, cost = optimization.fit()\n",
    "\n",
    "index_best = 0\n",
    "for m in range(len(result)):\n",
    "    if result[m][1] < result[index_best][1]:\n",
    "        index_best = m\n",
    "\n",
    "best_score = result[index_best][1]\n",
    "best_params = result[index_best][0]\n",
    "\n",
    "\n",
    "print(\"Best score with Grid search:\", best_score)\n",
    "\n",
    "print(str(cost) + \",\" + str(best_score))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# ########################### RANDOM SEARCH #######################\n",
    "print(\"\\nPerforming random search\")\n",
    "\n",
    "optimization = HPO.RandomSearchOptimization(\n",
    "    dataset, blackboxfunction_random, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "result, cost = optimization.fit()\n",
    "\n",
    "index_best = 0\n",
    "for m in range(len(result)):\n",
    "    if result[m][1] < result[index_best][1]:\n",
    "        index_best = m\n",
    "\n",
    "best_score = result[index_best][1]\n",
    "best_params = result[index_best][0]\n",
    "\n",
    "\n",
    "print(\"Best score with Random search:\", best_score)\n",
    "\n",
    "print(str(cost) + \",\" + str(best_score))\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "########################### BAYESIAN OPT #####################\n",
    "print(\"\\nPerforming bayesian optimization\")\n",
    "\n",
    "optimization = HPO.BayesianOptimization(\n",
    "    dataset, blackboxfunction_bayesian, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE)\n",
    "\n",
    "result, cost = optimization.fit()\n",
    "\n",
    "index_best = 0\n",
    "for m in range(len(result)):\n",
    "    if result[m][1] < result[index_best][1]:\n",
    "        index_best = m\n",
    "\n",
    "best_score = result[index_best][1]\n",
    "best_params = result[index_best][0]\n",
    "\n",
    "\n",
    "print(\"Best score with Bayesian Optimization:\", best_score)\n",
    "\n",
    "\n",
    "print(str(BUDGET) + \",\" + str(best_score))\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "########################### SPARSE OPT ############################\n",
    "\n",
    "print(\"\\nPerforming sparse search\")\n",
    "\n",
    "f = ExampleFunction()\n",
    "\n",
    "optimization = HPO.SparseGridSearchOptimization(\n",
    "    dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=2, adaptivity=0.85, optimizer=\"gradient_descent\")\n",
    "\n",
    "[X0, fX0, X1, fX1, X2, fX2], cost = optimization.fit()\n",
    "\n",
    "cost = cost + 2\n",
    "bestFX = fX0 \n",
    "if fX1 < bestFX:\n",
    "    bestFX = fX1 \n",
    "if fX2 < bestFX:\n",
    "    bestFX = fX2\n",
    "\n",
    "print(\"Sparse grid search\")\n",
    "print(str(cost) + \",\" + str(bestFX))\n",
    "    \n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(150,0.10400954969227313)}\n",
      "\n",
      "{(150,0.08087892914190889)}\n",
      "\n",
      "{(150,0.10335624758154154)}\n",
      "\n",
      "{(150,0.10400954969227313)}(200,0.07590454872697591)}\n",
      "\n",
      "{(150,0.08087892914190889)}(200,0.0986378276720643)}\n",
      "\n",
      "{(150,0.10335624758154154)}(200,0.0914656520821154)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_ITERS = 10\n",
    "\n",
    "ALT_1 = \"{\"\n",
    "ALT_2 = \"{\"\n",
    "ALT_3 = \"{\"\n",
    "\n",
    "BUDGETS = [150, 200]\n",
    "\n",
    "for budget in BUDGETS:\n",
    "\n",
    "    summe = 0\n",
    "    for i in range(N_ITERS):\n",
    "\n",
    "        optimizer = HPO.IterativeRandomOptimization(dataset, function, \n",
    "                                                            hyperparameterspace, budget, 0, 0.9, \n",
    "                                                            init_points=30, \n",
    "                                                            alternative=0, ref_per_step=4)\n",
    "        points = optimizer.fit()\n",
    "\n",
    "        points.sort(key=operator.attrgetter('value'))\n",
    "        summe += points[0].get_value()\n",
    "\n",
    "    ALT_1 += \"(\" + str(budget) + \",\" + str(summe/N_ITERS) + \")\"\n",
    "\n",
    "\n",
    "    summe = 0\n",
    "    for i in range(N_ITERS):\n",
    "\n",
    "        optimizer = HPO.IterativeRandomOptimization(dataset, function, \n",
    "                                                            hyperparameterspace, budget, 0, 0.8, \n",
    "                                                            init_points=30, \n",
    "                                                            alternative=1, ref_per_step=4)\n",
    "        points = optimizer.fit()\n",
    "\n",
    "        points.sort(key=operator.attrgetter('value'))\n",
    "        summe += points[0].get_value()\n",
    "\n",
    "    ALT_2 += \"(\" + str(budget) + \",\" + str(summe/N_ITERS) + \")\"\n",
    "\n",
    "\n",
    "    summe = 0\n",
    "    for i in range(N_ITERS):\n",
    "\n",
    "        optimizer = HPO.IterativeRandomOptimization(dataset, function, \n",
    "                                                            hyperparameterspace, budget, 0, 0.9, \n",
    "                                                            init_points=30, \n",
    "                                                            alternative=2, ref_per_step=4)\n",
    "        points = optimizer.fit()\n",
    "\n",
    "        points.sort(key=operator.attrgetter('value'))\n",
    "        summe += points[0].get_value()\n",
    "\n",
    "    ALT_3 += \"(\" + str(budget) + \",\" + str(summe/N_ITERS) + \")\"\n",
    "\n",
    "\n",
    "\n",
    "    ALT_1 += \"}\"\n",
    "    ALT_2 += \"}\"\n",
    "    ALT_3 += \"}\"\n",
    "\n",
    "    print(ALT_1+\"\\n\")\n",
    "    print(ALT_2+\"\\n\")\n",
    "    print(ALT_3+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search\n",
      "Best score with Grid search: 0.21640029549598694\n",
      "64,0.21640029549598694\n",
      "\n",
      "Performing random search\n",
      "Best score with Random search: 0.05757850594818592\n",
      "200,0.05757850594818592\n",
      "\n",
      "Performing bayesian optimization\n",
      "Best score with Bayesian Optimization: 0.0765347108244896\n",
      "200,0.0765347108244896\n",
      "\n",
      "Performing sparse search\n",
      "Adaptive grid generation (Ritter-Novak)...\n",
      "Done in 674992ms.\n",
      "Solving linear system (automatic method)...\n",
      "Done in 6ms.\n",
      "Optimizing (gradient descent)...\n",
      "Done in 2ms.\n",
      "Optimizing (multi-start)...\n",
      "Done in 3ms.\n",
      "Sparse grid search\n",
      "195,0.06948636285960674\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = HPO.Dataset(task_id=359938)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer, dropout_prob, deterministic=True):\n",
    "\n",
    "    #return epochs + batch_size + learning_rate + number_of_layers + neurons_per_layer\n",
    "\n",
    "\n",
    "    kfold = KFold(n_splits=CV)\n",
    "\n",
    "    split = (kfold.split(dataset.get_X(), dataset.get_Y()))\n",
    "\n",
    "    values = []\n",
    "\n",
    "    numeric_features = [not x for x in dataset.get_categorical_indicator()]\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"encoder\", OneHotEncoder(\n",
    "                handle_unknown=\"infrequent_if_exist\", sparse_output=False)),\n",
    "            # (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer,\n",
    "                dataset.get_categorical_indicator()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(split):\n",
    "\n",
    "        if deterministic:\n",
    "            reset_seeds()\n",
    "\n",
    "        X_train = dataset.get_X()[train_index]\n",
    "        Y_train = dataset.get_Y()[train_index]\n",
    "\n",
    "        X_val = dataset.get_X()[test_index]\n",
    "        Y_val = dataset.get_Y()[test_index]\n",
    "\n",
    "        preprocessor.fit(X_train, Y_train)\n",
    "\n",
    "        X_train = preprocessor.transform(X_train)\n",
    "        X_val = preprocessor.transform(X_val)\n",
    "\n",
    "        regressor = KerasRegressor(model=create_model,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    input_dim=len(\n",
    "                                        X_train[0]),\n",
    "                                    number_layers=number_of_layers,\n",
    "                                    neurons_per_layer=neurons_per_layer,\n",
    "                                    dropout_prob=dropout_prob,\n",
    "                                    verbose=0)\n",
    "\n",
    "        regressor = TransformedTargetRegressor(regressor=regressor,\n",
    "                                                transformer=StandardScaler())\n",
    "\n",
    "        regressor.fit(X_train, Y_train, epochs=int(epochs),\n",
    "                        batch_size=int(batch_size), shuffle=False)\n",
    "\n",
    "        Y_predicted = regressor.predict(X_val)\n",
    "        # error = sklearn.metrics.mean_absolute_error(Y_predicted, Y_val)\n",
    "        error = sklearn.metrics.mean_absolute_percentage_error(\n",
    "            Y_predicted, Y_val)\n",
    "        values.append(error)\n",
    "\n",
    "        del regressor\n",
    "        K.clear_session()\n",
    "\n",
    "    result = sum(values)/len(values)\n",
    "    return result\n",
    "\n",
    "CV = 2\n",
    "\n",
    "def blackboxfunction_grid(params):\n",
    "    # index = int(params[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "    # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_of_layers = int(params[3])\n",
    "\n",
    "    neurons_per_layer = int(params[4])\n",
    "\n",
    "    dropout_prob = params[5]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer, dropout_prob)\n",
    "\n",
    "def blackboxfunction_random(params):\n",
    "    # index = int(params[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "    # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    learning_rate = params[2]\n",
    "\n",
    "    number_of_layers = int(params[3])\n",
    "\n",
    "    neurons_per_layer = int(params[4])\n",
    "\n",
    "    dropout_prob = params[5]\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "def blackboxfunction_bayesian(params):\n",
    "    # index = int(params[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "    # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "    epochs = int(params[0])\n",
    "\n",
    "    batch_size = int(params[1])\n",
    "\n",
    "    model_learning_rate = 10 ** (params[2])\n",
    "\n",
    "    number_of_layers = int(params[3])\n",
    "\n",
    "    neurons_per_layer = int(params[4])\n",
    "\n",
    "    dropout_prob = params[5]\n",
    "\n",
    "\n",
    "    return evaluate_model(epochs, batch_size, model_learning_rate, number_of_layers, neurons_per_layer, dropout_prob, deterministic=False)\n",
    "\n",
    "##################### Function for sparse grid search #####################\n",
    "\n",
    "class ExampleFunction(pysgpp.ScalarFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ExampleFunction, self).__init__(\n",
    "            len(hyperparameterspace.keys()))\n",
    "\n",
    "    def eval(self, x):\n",
    "        # index = int(x[0]*(len(hyperparameterspace_special[\"loss\"])-1))\n",
    "        # hyperparameterspace_special[\"loss\"][index]\n",
    "\n",
    "        epochs = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"epochs\"][0], hyperparameterspace_special[\"epochs\"][1], x[0]))\n",
    "\n",
    "        batch_size = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"batch_size\"][0], hyperparameterspace_special[\"batch_size\"][1], x[1]))\n",
    "\n",
    "        # HPO.from_standard_log(hyperparameterspace_special[\"learning_rate\"][\n",
    "        model_learning_rate = HPO.from_standard_log(\n",
    "            hyperparameterspace_special[\"learning rate\"][0], hyperparameterspace_special[\"learning rate\"][1], x[2])\n",
    "\n",
    "        number_of_layers = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"number_layers\"][0], hyperparameterspace_special[\"number_layers\"][1], x[3]))\n",
    "\n",
    "        neurons_per_layer = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"neurons_per_layer\"][0], hyperparameterspace_special[\"neurons_per_layer\"][1], x[4]))\n",
    "        \n",
    "        dropout_prob = int(HPO.from_standard(\n",
    "            hyperparameterspace_special[\"dropout_prob\"][0], hyperparameterspace_special[\"dropout_prob\"][1], x[5]))\n",
    "\n",
    "        return evaluate_model(epochs, batch_size, model_learning_rate, number_of_layers, neurons_per_layer, dropout_prob)\n",
    "    \n",
    "\n",
    "BUDGET = 200\n",
    "\n",
    "print(\"\\nPerforming grid search\")\n",
    "optimization = HPO.GridSearchOptimization(\n",
    "    dataset, blackboxfunction_grid, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "result, cost = optimization.fit()\n",
    "\n",
    "index_best = 0\n",
    "for m in range(len(result)):\n",
    "    if result[m][1] < result[index_best][1]:\n",
    "        index_best = m\n",
    "\n",
    "best_score = result[index_best][1]\n",
    "best_params = result[index_best][0]\n",
    "\n",
    "\n",
    "print(\"Best score with Grid search:\", best_score)\n",
    "\n",
    "print(str(cost) + \",\" + str(best_score))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# ########################### RANDOM SEARCH #######################\n",
    "print(\"\\nPerforming random search\")\n",
    "\n",
    "optimization = HPO.RandomSearchOptimization(\n",
    "    dataset, blackboxfunction_random, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, cv=CV)\n",
    "\n",
    "result, cost = optimization.fit()\n",
    "\n",
    "index_best = 0\n",
    "for m in range(len(result)):\n",
    "    if result[m][1] < result[index_best][1]:\n",
    "        index_best = m\n",
    "\n",
    "best_score = result[index_best][1]\n",
    "best_params = result[index_best][0]\n",
    "\n",
    "\n",
    "print(\"Best score with Random search:\", best_score)\n",
    "\n",
    "print(str(cost) + \",\" + str(best_score))\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "########################### BAYESIAN OPT #####################\n",
    "print(\"\\nPerforming bayesian optimization\")\n",
    "\n",
    "optimization = HPO.BayesianOptimization(\n",
    "    dataset, blackboxfunction_bayesian, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE)\n",
    "\n",
    "result, cost = optimization.fit()\n",
    "\n",
    "index_best = 0\n",
    "for m in range(len(result)):\n",
    "    if result[m][1] < result[index_best][1]:\n",
    "        index_best = m\n",
    "\n",
    "best_score = result[index_best][1]\n",
    "best_params = result[index_best][0]\n",
    "\n",
    "\n",
    "print(\"Best score with Bayesian Optimization:\", best_score)\n",
    "\n",
    "\n",
    "print(str(BUDGET) + \",\" + str(best_score))\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "########################### SPARSE OPT ############################\n",
    "\n",
    "print(\"\\nPerforming sparse search\")\n",
    "\n",
    "f = ExampleFunction()\n",
    "\n",
    "optimization = HPO.SparseGridSearchOptimization(\n",
    "    dataset, f, hyperparameterspace, budget=BUDGET, verbosity=VERBOSE, degree=2, adaptivity=0.85, optimizer=\"gradient_descent\")\n",
    "\n",
    "[X0, fX0, X1, fX1, X2, fX2], cost = optimization.fit()\n",
    "\n",
    "cost = cost + 2\n",
    "bestFX = fX0 \n",
    "if fX1 < bestFX:\n",
    "    bestFX = fX1 \n",
    "if fX2 < bestFX:\n",
    "    bestFX = fX2\n",
    "\n",
    "print(\"Sparse grid search\")\n",
    "print(str(cost) + \",\" + str(bestFX))\n",
    "    \n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 09:26:19.520423: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(150,0.0792172959074378)\n",
      "\n",
      "{(150,0.0792172959074378)(200,0.07392622400075197)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################## MODEL AND FUNCTION DEFINITION ####################\n",
    "CV = 2\n",
    "dataset = HPO.Dataset(task_id=359938)\n",
    "\n",
    "def evaluate_model(epochs, batch_size, learning_rate, number_of_layers, neurons_per_layer, dropout_prob, deterministic=True):\n",
    "\n",
    "    #return epochs + batch_size + learning_rate + number_of_layers + neurons_per_layer\n",
    "\n",
    "\n",
    "    kfold = KFold(n_splits=CV)\n",
    "\n",
    "    split = (kfold.split(dataset.get_X(), dataset.get_Y()))\n",
    "\n",
    "    values = []\n",
    "\n",
    "    numeric_features = [not x for x in dataset.get_categorical_indicator()]\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"encoder\", OneHotEncoder(\n",
    "                handle_unknown=\"infrequent_if_exist\", sparse_output=False)),\n",
    "            # (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer,\n",
    "                dataset.get_categorical_indicator()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(split):\n",
    "\n",
    "        if deterministic:\n",
    "            reset_seeds()\n",
    "\n",
    "        X_train = dataset.get_X()[train_index]\n",
    "        Y_train = dataset.get_Y()[train_index]\n",
    "\n",
    "        X_val = dataset.get_X()[test_index]\n",
    "        Y_val = dataset.get_Y()[test_index]\n",
    "\n",
    "        preprocessor.fit(X_train, Y_train)\n",
    "\n",
    "        X_train = preprocessor.transform(X_train)\n",
    "        X_val = preprocessor.transform(X_val)\n",
    "\n",
    "        regressor = KerasRegressor(model=create_model,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    input_dim=len(\n",
    "                                        X_train[0]),\n",
    "                                    number_layers=number_of_layers,\n",
    "                                    neurons_per_layer=neurons_per_layer,\n",
    "                                    dropout_prob=dropout_prob,\n",
    "                                    verbose=0)\n",
    "\n",
    "        regressor = TransformedTargetRegressor(regressor=regressor,\n",
    "                                                transformer=StandardScaler())\n",
    "\n",
    "        regressor.fit(X_train, Y_train, epochs=int(epochs),\n",
    "                        batch_size=int(batch_size), shuffle=False)\n",
    "\n",
    "        Y_predicted = regressor.predict(X_val)\n",
    "        # error = sklearn.metrics.mean_absolute_error(Y_predicted, Y_val)\n",
    "        error = sklearn.metrics.mean_absolute_percentage_error(\n",
    "            Y_predicted, Y_val)\n",
    "        values.append(error)\n",
    "\n",
    "        del regressor\n",
    "        K.clear_session()\n",
    "\n",
    "    result = sum(values)/len(values)\n",
    "    return result\n",
    "\n",
    "def function(coordinates):\n",
    "        \n",
    "    return evaluate_model(epochs=coordinates[0], batch_size=coordinates[1], learning_rate=coordinates[2], number_of_layers=coordinates[3], neurons_per_layer=coordinates[4], dropout_prob=coordinates[5], deterministic=False)\n",
    "\n",
    "\n",
    "\n",
    "ALT_1 = \"{\"\n",
    "ALT_2 = \"{\"\n",
    "ALT_3 = \"{\"\n",
    "\n",
    "BUDGETS = [20, 30, 50, 70, 90, 110, 130]\n",
    "BUDGETS = [150, 200]\n",
    "\n",
    "for budget in BUDGETS:\n",
    "\n",
    "    summe = 0\n",
    "    for i in range(N_ITERS):\n",
    "\n",
    "        optimizer = HPO.IterativeRandomOptimization(dataset, function, \n",
    "                                                            hyperparameterspace, budget, 0, 0.9, \n",
    "                                                            init_points=budget, \n",
    "                                                            alternative=0, ref_per_step=4)\n",
    "        points = optimizer.fit()\n",
    "\n",
    "        points.sort(key=operator.attrgetter('value'))\n",
    "        summe += points[0].get_value()\n",
    "\n",
    "    ALT_1 += \"(\" + str(budget) + \",\" + str(summe/N_ITERS) + \")\"\n",
    "\n",
    "\n",
    "\n",
    "    ALT_1 += \"\"\n",
    "\n",
    "    print(ALT_1+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
