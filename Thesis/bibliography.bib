@article{feurer2019hyperparameter,
	title={Hyperparameter optimization},
	author={Feurer, Matthias and Hutter, Frank},
	journal={Automated machine learning: Methods, systems, challenges},
	pages={3--33},
	year={2019},
	publisher={Springer International Publishing}
}

@article{bischl2021hyperparameter,
	title={Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges},
	author={Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and others},
	journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	pages={e1484},
	year={2021},
	publisher={Wiley Online Library}
}

@article{supervised_learning,
	title={Supervised learning},
	author={Cunningham, P{\'a}draig and Cord, Matthieu and Delany, Sarah Jane},
	journal={Machine learning techniques for multimedia: case studies on organization and retrieval},
	pages={21--49},
	year={2008},
	publisher={Springer}
}

@article{random_search,
	title={Random search for hyper-parameter optimization.},
	author={Bergstra, James and Bengio, Yoshua},
	journal={Journal of machine learning research},
	volume={13},
	number={2},
	year={2012}
}

@article{yang2020hyperparameter,
	title={On hyperparameter optimization of machine learning algorithms: Theory and practice},
	author={Yang, Li and Shami, Abdallah},
	journal={Neurocomputing},
	volume={415},
	pages={295--316},
	year={2020},
	publisher={Elsevier}
}

@article{snoek2012practical,
	title={Practical bayesian optimization of machine learning algorithms},
	author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	journal={Advances in neural information processing systems},
	volume={25},
	year={2012}
}

@article{garrido2020dealing,
	title={Dealing with categorical and integer-valued variables in bayesian optimization with gaussian processes},
	author={Garrido-Merch{\'a}n, Eduardo C and Hern{\'a}ndez-Lobato, Daniel},
	journal={Neurocomputing},
	volume={380},
	pages={20--35},
	year={2020},
	publisher={Elsevier}
}

@inproceedings{hutter2011sequential,
	title={Sequential model-based optimization for general algorithm configuration},
	author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
	booktitle={Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5},
	pages={507--523},
	year={2011},
	organization={Springer}
}

@article{andonie2019hyperparameter,
	title={Hyperparameter optimization in learning systems},
	author={Andonie, R{\u{a}}zvan},
	journal={Journal of Membrane Computing},
	volume={1},
	number={4},
	pages={279--291},
	year={2019},
	publisher={Springer}
}

@inproceedings{jamieson2016non,
	title={Non-stochastic best arm identification and hyperparameter optimization},
	author={Jamieson, Kevin and Talwalkar, Ameet},
	booktitle={Artificial intelligence and statistics},
	pages={240--248},
	year={2016},
	organization={PMLR}
}

@ARTICLE{8030298,
	author={Diaz, G. I. and Fokoue-Nkoutche, A. and Nannicini, G. and Samulowitz, H.},
	journal={IBM Journal of Research and Development}, 
	title={An effective algorithm for hyperparameter optimization of neural networks}, 
	year={2017},
	volume={61},
	number={4/5},
	pages={9:1-9:11},
	doi={10.1147/JRD.2017.2709578}
}

@inproceedings{smithson2016neural,
	title={Neural networks designing neural networks: multi-objective hyper-parameter optimization},
	author={Smithson, Sean C and Yang, Guang and Gross, Warren J and Meyer, Brett H},
	booktitle={2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	pages={1--8},
	year={2016},
	organization={IEEE}
}

@article{loshchilov2016cma,
	title={CMA-ES for hyperparameter optimization of deep neural networks},
	author={Loshchilov, Ilya and Hutter, Frank},
	journal={arXiv preprint arXiv:1604.07269},
	year={2016}
}

@article{wilson2018maximizing,
	title={Maximizing acquisition functions for Bayesian optimization},
	author={Wilson, James and Hutter, Frank and Deisenroth, Marc},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}

@article{b_splines,
	title={B-splines for sparse grids: Algorithms and application to higher-dimensional optimization},
	author={Valentin, Julian},
	journal={arXiv preprint arXiv:1910.05379},
	year={2019}
}


@phdthesis{pfluger2010spatially,
	title={Spatially adaptive sparse grids for high-dimensional problems},
	author={Pfl{\"u}ger, Dirk Michael},
	year={2010},
	school={Technische Universit{\"a}t M{\"u}nchen}
}

@inproceedings{garcke2013sparse,
	title={Sparse grids in a nutshell},
	author={Garcke, Jochen},
	booktitle={Sparse grids and applications},
	pages={57--80},
	year={2013},
	organization={Springer}
}

@incollection{obersteiner2022spatially,
	title={A spatially adaptive sparse grid combination technique for numerical quadrature},
	author={Obersteiner, Michael and Bungartz, Hans-Joachim},
	booktitle={Sparse Grids and Applications-Munich 2018},
	pages={161--185},
	year={2022},
	publisher={Springer}
}

@article{bungartz2004sparse,
	title={Sparse grids},
	author={Bungartz, Hans-Joachim and Griebel, Michael},
	journal={Acta numerica},
	volume={13},
	pages={147--269},
	year={2004},
	publisher={Cambridge University Press}
}

@inproceedings{zenger1991sparse,
	title={Sparse grids},
	author={Zenger, Christoph and Hackbusch, W},
	booktitle={Proceedings of the Research Workshop of the Israel Science Foundation on Multiscale Phenomenon, Modelling and Computation},
	pages={86},
	year={1991}
}

@article{griebel1990combination,
	title={A combination technique for the solution of sparse grid problems},
	author={Griebel, Michael and Schneider, Michael and Zenger, Christoph},
	year={1990},
	publisher={Citeseer}
}

@article{obersteiner2021generalized,
	title={A generalized spatially adaptive sparse grid combination technique with dimension-wise refinement},
	author={Obersteiner, Michael and Bungartz, Hans-Joachim},
	journal={SIAM Journal on Scientific Computing},
	volume={43},
	number={4},
	pages={A2381--A2403},
	year={2021},
	publisher={SIAM}
}

@article{hegland2002adaptive,
	title={Adaptive sparse grids},
	author={Hegland, Markus},
	journal={Anziam Journal},
	volume={44},
	pages={C335--C353},
	year={2002}
}

@phdthesis{bungartz1998finite,
	title={Finite elements of higher order on sparse grids},
	author={Bungartz, Hans-Joachim},
	year={1998},
	school={Technische Universit{\"a}t M{\"u}nchen}
}

@book{hollig2013approximation,
	title={Approximation and modeling with B-splines},
	author={H{\"o}llig, Klaus and H{\"o}rner, J{\"o}rg},
	year={2013},
	publisher={SIAM}
}

@article{ayodele2010types,
	title={Types of machine learning algorithms},
	author={Ayodele, Taiwo Oladipupo},
	journal={New advances in machine learning},
	volume={3},
	pages={19--48},
	year={2010},
	publisher={InTech Rijeka, Croatia}
}

@article{mahesh2020machine,
	title={Machine learning algorithms-a review},
	author={Mahesh, Batta},
	journal={International Journal of Science and Research (IJSR).[Internet]},
	volume={9},
	pages={381--386},
	year={2020}
}

@article{wang2016machine,
	title={Machine learning basics},
	author={Wang, H and Lei, ZeZXeZBePJ and Zhang, X and Zhou, B and Peng, J},
	journal={Deep learning},
	pages={98--164},
	year={2016}
}

@article{granmo2018tsetlin,
	title={The Tsetlin Machine--A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic},
	author={Granmo, Ole-Christoffer},
	journal={arXiv preprint arXiv:1804.01508},
	year={2018}
}

@article{noble2006support,
	title={What is a support vector machine?},
	author={Noble, William S},
	journal={Nature biotechnology},
	volume={24},
	number={12},
	pages={1565--1567},
	year={2006},
	publisher={Nature Publishing Group UK London}
}

@article{rokach2005decision,
	title={Decision trees},
	author={Rokach, Lior and Maimon, Oded},
	journal={Data mining and knowledge discovery handbook},
	pages={165--192},
	year={2005},
	publisher={Springer}
}

@article{bishop1994neural,
	title={Neural networks and their applications},
	author={Bishop, Chris M},
	journal={Review of scientific instruments},
	volume={65},
	number={6},
	pages={1803--1832},
	year={1994},
	publisher={American Institute of Physics}
}

@book{da2017artificial,
	title={Artificial neural network architectures and training processes},
	author={Da Silva, Ivan Nunes and Hernane Spatti, Danilo and Andrade Flauzino, Rogerio and Liboni, Luisa Helena Bartocci and dos Reis Alves, Silas Franco and da Silva, Ivan Nunes and Hernane Spatti, Danilo and Andrade Flauzino, Rogerio and Liboni, Luisa Helena Bartocci and dos Reis Alves, Silas Franco},
	year={2017},
	publisher={Springer}
}

@article{medsker2001recurrent,
	title={Recurrent neural networks},
	author={Medsker, Larry R and Jain, LC},
	journal={Design and Applications},
	volume={5},
	pages={64--67},
	year={2001}
}

@article{li2021survey,
	title={A survey of convolutional neural networks: analysis, applications, and prospects},
	author={Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
	journal={IEEE transactions on neural networks and learning systems},
	year={2021},
	publisher={IEEE}
}

@article{gu2018recent,
	title={Recent advances in convolutional neural networks},
	author={Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and others},
	journal={Pattern recognition},
	volume={77},
	pages={354--377},
	year={2018},
	publisher={Elsevier}
}

@article{o2015introduction,
	title={An introduction to convolutional neural networks},
	author={O'Shea, Keiron and Nash, Ryan},
	journal={arXiv preprint arXiv:1511.08458},
	year={2015}
}

@article{liu2017survey,
	title={A survey of deep neural network architectures and their applications},
	author={Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E},
	journal={Neurocomputing},
	volume={234},
	pages={11--26},
	year={2017},
	publisher={Elsevier}
}

@article{duan2016induction,
	title={Induction motor parameter estimation using sparse grid optimization algorithm},
	author={Duan, Fang and {\v{Z}}ivanovi{\'c}, Rastko and Al-Sarawi, Said and Mba, David},
	journal={IEEE Transactions on Industrial Informatics},
	volume={12},
	number={4},
	pages={1453--1461},
	year={2016},
	publisher={IEEE}
}

@article{valentin2018gradient,
	title={Gradient-based optimization with b-splines on sparse grids for solving forward-dynamics simulations of three-dimensional, continuum-mechanical musculoskeletal system models},
	author={Valentin, J and Sprenger, M and Pfl{\"u}ger, D and R{\"o}hrle, O},
	journal={International journal for numerical methods in biomedical engineering},
	volume={34},
	number={5},
	pages={e2965},
	year={2018},
	publisher={Wiley Online Library}
}

@article{sankaran2009stochastic,
	title={Stochastic optimization using a sparse grid collocation scheme},
	author={Sankaran, Sethuraman},
	journal={Probabilistic engineering mechanics},
	volume={24},
	number={3},
	pages={382--396},
	year={2009},
	publisher={Elsevier}
}

@article{hulsmann2013spagrow,
	title={SpaGrOW—A derivative-free optimization scheme for intermolecular force field parameters based on sparse grid methods},
	author={H{\"u}lsmann, Marco and Reith, Dirk},
	journal={Entropy},
	volume={15},
	number={9},
	pages={3640--3687},
	year={2013},
	publisher={MDPI}
}

@inproceedings{saska2007path,
	title={Path planning for formations using global optimization with sparse grids},
	author={Saska, M and Ferenczi, I and Hess, M and Schilling, K},
	booktitle={Proc. of The 13th IASTED International Conference on Robotics and Applications (RA 2007)},
	year={2007}
}

@inproceedings{donahue2009robust,
	title={Robust parameter identification with adaptive sparse grid-based optimization for nonlinear systems biology models},
	author={Donahue, Maia M and Buzzard, Gregery T and Rundell, Ann E},
	booktitle={2009 American Control Conference},
	pages={5055--5060},
	year={2009},
	organization={IEEE}
}

@article{chen2013derivative,
	title={A derivative-free optimization algorithm using sparse grid integration},
	author={Chen, Shengyuan and Wang, Xiaogang},
	year={2013},
	publisher={Scientific Research Publishing}
}

@article{novak1996global,
	title={Global optimization using hyperbolic cross points},
	author={Novak, Erich and Ritter, Klaus},
	journal={State of the art in global optimization: computational methods and applications},
	pages={19--33},
	year={1996},
	publisher={Springer}
}

@inproceedings{valentin2016hierarchical,
	title={Hierarchical gradient-based optimization with B-splines on sparse grids},
	author={Valentin, Julian and Pfl{\"u}ger, Dirk},
	booktitle={Sparse Grids and Applications-Stuttgart 2014},
	pages={315--336},
	year={2016},
	organization={Springer}
}

@book{yang2010engineering,
	title={Engineering optimization: an introduction with metaheuristic applications},
	author={Yang, Xin-She},
	year={2010},
	publisher={John Wiley \& Sons}
}

@article{whitley1996evaluating,
	title={Evaluating evolutionary algorithms},
	author={Whitley, Darrell and Rana, Soraya and Dzubera, John and Mathias, Keith E},
	journal={Artificial intelligence},
	volume={85},
	number={1-2},
	pages={245--276},
	year={1996},
	publisher={Elsevier}
}

@article{feurer-arxiv19a,
	author    = {Matthias Feurer and Jan N. van Rijn and Arlind Kadra and Pieter Gijsbers and Neeratyoy Mallik and Sahithya Ravi and Andreas Müller and Joaquin Vanschoren and Frank Hutter},
	title     = {OpenML-Python: an extensible Python API for OpenML},
	journal   = {arXiv:1911.02490},
	year      = {2019},
}

@article{scikit-learn,
	title={Scikit-learn: Machine Learning in {P}ython},
	author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
	and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
	and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
	Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}

@article{WU201926,
	title = {Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization},
	journal = {Journal of Electronic Science and Technology},
	volume = {17},
	number = {1},
	pages = {26-40},
	year = {2019},
	issn = {1674-862X},
	doi = {https://doi.org/10.11989/JEST.1674-862X.80904120},
	url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300047},
	author = {Jia Wu and Xiu-Yun Chen and Hao Zhang and Li-Dong Xiong and Hang Lei and Si-Hao Deng},
	keywords = {Bayesian optimization, Gaussian process, hyperparameter optimization, machine learning},
	abstract = {Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.}
}

@article{OpenML2013,
	author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
	title = {OpenML: Networked Science in Machine Learning},
	journal = {SIGKDD Explorations},
	volume = {15},
	number = {2},
	year = {2013},
	pages = {49--60},
	url = {http://doi.acm.org/10.1145/2641190.2641198},
	doi = {10.1145/2641190.2641198},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@article{MALAKOUTI2023200248,
	title = {Babysitting hyperparameter optimization and 10-fold-cross-validation to enhance the performance of ML methods in predicting wind speed and energy generation},
	journal = {Intelligent Systems with Applications},
	volume = {19},
	pages = {200248},
	year = {2023},
	issn = {2667-3053},
	doi = {https://doi.org/10.1016/j.iswa.2023.200248},
	url = {https://www.sciencedirect.com/science/article/pii/S266730532300073X},
	author = {Seyed Matin Malakouti},
	keywords = {Sotavento wind farm, Wind speed, Babysitting method, Light gradient boosting machine, Gradient boosting, Random forest, K- neighbors},
	abstract = {Academics have been interested in renewable energy for a long time, and research has been done on how to use it, collect it, manage it, make it more efficient, and find uses for it. all the different ways to use renewable energy have the potential to replace traditional fossil fuels slowly. in Galicia, Spain (43.354377 N, 7.881213 W), the Sotavento wind farm comprises 24 wind turbines with a total installed capacity of 17.56 MW. The babysitting method was used to optimize the Hyperparameter of the light gradient boosting machine, gradient boosting, random forest, and k-neighbor algorithms. Also, 10-fold cross-validation was used to enhance the performance and reliability of ML methods.}
}

@inproceedings{gorgolis2019hyperparameter,
	title={Hyperparameter optimization of LSTM network models through genetic algorithm},
	author={Gorgolis, Nikolaos and Hatzilygeroudis, Ioannis and Istenes, Zoltan and Gyenne, Lazlo--Grad},
	booktitle={2019 10th International Conference on Information, Intelligence, Systems and Applications (IISA)},
	pages={1--4},
	year={2019},
	organization={IEEE}
}

@article{MASRI1980353,
	title = {A global optimization algorithm using adaptive random search},
	journal = {Applied Mathematics and Computation},
	volume = {7},
	number = {4},
	pages = {353-375},
	year = {1980},
	issn = {0096-3003},
	doi = {https://doi.org/10.1016/0096-3003(80)90027-2},
	url = {https://www.sciencedirect.com/science/article/pii/0096300380900272},
	author = {S.F. Masri and G.A. Bekey and F.B. Safford},
	abstract = {A new random-search global optimization is described in which the variance of the step-size distribution is periodically optimized. By searching over a variance range of 8 to 10 decades, the algorithm finds the step-size distribution that yields the best local improvement in the criterion function. The variance search is then followed by a specified number of iterations of local random search where the step-size variance remains fixed. Periodic wide-range searches are introduced to ensure that the process does not stop at a local minimum. The sensitivity of the complete algorithm to various search parameters is investigated experimentally for a specific test problem. The ability of the method to locate global minima is illustrated by an example. The method also displays considerable problem independence, as demonstrated by two large and realistic example problems: (1) the identification of 25 parameters in a nonlinear model of a five-degrees-of-freedom mechanical dynamic system and (2) solution of a 24-parameter inverse problem required to identify a pulse train whose frequency spectrum matched a desired reference spectrum.}
}

@article{random_search_2,
	author = {Andradóttir, Sigrún and Prudius, Andrei A.},
	title = {Adaptive random search for continuous simulation optimization},
	journal = {Naval Research Logistics (NRL)},
	volume = {57},
	number = {6},
	pages = {583-604},
	keywords = {continuous stochastic optimization, local search, pure random search, global convergence in probability and almost surely, adaptive search with resampling, deterministic and stochastic shrinking ball methods},
	doi = {https://doi.org/10.1002/nav.20422},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.20422},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.20422},
	abstract = {Abstract We present, analyze, and compare three random search methods for solving stochastic optimization problems with uncountable feasible regions. Our adaptive search with resampling (ASR) approach is a framework for designing provably convergent algorithms that are adaptive and may consequently involve local search. The deterministic and stochastic shrinking ball (DSB and SSB) approaches are also convergent, but they are based on pure random search with the only difference being the estimator of the optimal solution [the DSB method was originally proposed and analyzed by Baumert and Smith]. The three methods use different techniques to reduce the effects of noise in the estimated objective function values. Our ASR method achieves this goal through resampling of already sampled points, whereas the DSB and SSB approaches address it by averaging observations in balls that shrink with time. We present conditions under which the three methods are convergent, both in probability and almost surely, and provide a limited computational study aimed at comparing the methods. Although further investigation is needed, our numerical results suggest that the ASR approach is promising, especially for difficult problems where the probability of identifying good solutions using pure random search is small. © 2010 Wiley Periodicals, Inc. Naval Research Logistics, 2010},
	year = {2010}
}

@article{hamzaccebi2006heuristic,
	title={A heuristic approach for finding the global minimum: Adaptive random search technique},
	author={Hamza{\c{c}}ebi, Co{\c{s}}kun and Kutay, Fevzi},
	journal={Applied Mathematics and Computation},
	volume={173},
	number={2},
	pages={1323--1333},
	year={2006},
	publisher={Elsevier}
}

@article{schumer1968adaptive,
	title={Adaptive step size random search},
	author={Schumer, MA and Steiglitz, Kenneth},
	journal={IEEE Transactions on Automatic Control},
	volume={13},
	number={3},
	pages={270--276},
	year={1968},
	publisher={IEEE}
}

@article{kelahan1978application,
	title={Application of the adaptive random search to discrete and mixed integer optimization},
	author={Kelahan, RC and Gaddy, JL},
	journal={International Journal for Numerical Methods in Engineering},
	volume={12},
	number={2},
	pages={289--298},
	year={1978},
	publisher={Wiley Online Library}
}

@article{tang1994adaptive,
	title={Adaptive partitioned random search to global optimization},
	author={Tang, Z Bo},
	journal={IEEE Transactions on Automatic Control},
	volume={39},
	number={11},
	pages={2235--2244},
	year={1994},
	publisher={IEEE}
}



