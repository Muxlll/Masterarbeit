\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\abx@aux@refcontext{none/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\babel@aux{american}{}
\BKM@entry{id=1,dest={446F632D5374617274},srcline={1}}{5C3337365C3337375C303030415C303030635C3030306B5C3030306E5C3030306F5C303030775C3030306C5C303030655C303030645C303030675C3030306D5C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{iii}{Doc-Start}\protected@file@percent }
\BKM@entry{id=2,dest={636861707465722A2E31},srcline={1}}{5C3337365C3337375C303030415C303030625C303030735C303030745C303030725C303030615C303030635C30303074}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Abstract}{iv}{chapter*.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\BKM@entry{id=3,dest={746F632E30},srcline={93}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=4,dest={636861707465722E31},srcline={4}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{MALAKOUTI2023200248}
\abx@aux@segm{0}{0}{MALAKOUTI2023200248}
\abx@aux@cite{0}{gorgolis2019hyperparameter}
\abx@aux@segm{0}{0}{gorgolis2019hyperparameter}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\BKM@entry{id=5,dest={636861707465722E32},srcline={4}}{5C3337365C3337375C303030535C303030745C303030615C303030745C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030415C303030725C30303074}
\abx@aux@cite{0}{wang2016machine}
\abx@aux@segm{0}{0}{wang2016machine}
\abx@aux@cite{0}{mahesh2020machine}
\abx@aux@segm{0}{0}{mahesh2020machine}
\abx@aux@cite{0}{ayodele2010types}
\abx@aux@segm{0}{0}{ayodele2010types}
\abx@aux@cite{0}{noble2006support}
\abx@aux@segm{0}{0}{noble2006support}
\abx@aux@cite{0}{granmo2018tsetlin}
\abx@aux@segm{0}{0}{granmo2018tsetlin}
\abx@aux@cite{0}{rokach2005decision}
\abx@aux@segm{0}{0}{rokach2005decision}
\BKM@entry{id=6,dest={73656374696F6E2E322E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{bishop1994neural}
\abx@aux@segm{0}{0}{bishop1994neural}
\abx@aux@cite{0}{da2017artificial}
\abx@aux@segm{0}{0}{da2017artificial}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}State of the Art}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:theoretical_background}{{2}{3}{State of the Art}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to Neural Networks}{3}{section.2.1}\protected@file@percent }
\newlabel{sec:neural_networks}{{2.1}{3}{Introduction to Neural Networks}{section.2.1}{}}
\abx@aux@cite{0}{da2017artificial}
\abx@aux@segm{0}{0}{da2017artificial}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Neural network consisting of only one perceptron. The output is computed according to Equation \ref  {eq:perceptron}.\relax }}{4}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:perceptron}{{2.1}{4}{Neural network consisting of only one perceptron. The output is computed according to Equation \ref {eq:perceptron}.\relax }{figure.caption.4}{}}
\newlabel{eq:perceptron}{{2.1}{4}{Introduction to Neural Networks}{equation.2.1.1}{}}
\abx@aux@cite{0}{da2017artificial}
\abx@aux@segm{0}{0}{da2017artificial}
\abx@aux@cite{0}{medsker2001recurrent}
\abx@aux@segm{0}{0}{medsker2001recurrent}
\abx@aux@cite{0}{li2021survey}
\abx@aux@segm{0}{0}{li2021survey}
\abx@aux@cite{0}{gu2018recent}
\abx@aux@segm{0}{0}{gu2018recent}
\abx@aux@cite{0}{o2015introduction}
\abx@aux@segm{0}{0}{o2015introduction}
\abx@aux@cite{0}{liu2017survey}
\abx@aux@segm{0}{0}{liu2017survey}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Neural network consisting of two hidden layers. The connections between the perceptrons are bidirectional. In the forward phase, the intermediate results are given to the neurons to the right and in the backward phase from right to left. \relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:network}{{2.2}{5}{Neural network consisting of two hidden layers. The connections between the perceptrons are bidirectional. In the forward phase, the intermediate results are given to the neurons to the right and in the backward phase from right to left. \relax }{figure.caption.5}{}}
\newlabel{eq:weights_update}{{2.2}{5}{Introduction to Neural Networks}{equation.2.1.2}{}}
\BKM@entry{id=7,dest={73656374696F6E2E322E32},srcline={68}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Hyperparameter Optimization}{6}{section.2.2}\protected@file@percent }
\newlabel{sec:hyperparameter_optimization}{{2.2}{6}{Hyperparameter Optimization}{section.2.2}{}}
\newlabel{eq:optimization}{{2.3}{6}{Hyperparameter Optimization}{equation.2.2.3}{}}
\abx@aux@cite{0}{supervised_learning}
\abx@aux@segm{0}{0}{supervised_learning}
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\abx@aux@cite{0}{bischl2021hyperparameter}
\abx@aux@segm{0}{0}{bischl2021hyperparameter}
\abx@aux@cite{0}{yang2020hyperparameter}
\abx@aux@segm{0}{0}{yang2020hyperparameter}
\BKM@entry{id=8,dest={73756273656374696F6E2E322E322E31},srcline={87}}{5C3337365C3337375C303030475C303030725C303030695C303030645C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=9,dest={73756273656374696F6E2E322E322E32},srcline={95}}{5C3337365C3337375C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\abx@aux@cite{0}{random_search}
\abx@aux@segm{0}{0}{random_search}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Grid Search}{7}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Random Search}{7}{subsection.2.2.2}\protected@file@percent }
\newlabel{Random_search}{{2.2.2}{7}{Random Search}{subsection.2.2.2}{}}
\BKM@entry{id=10,dest={73756273656374696F6E2E322E322E33},srcline={114}}{5C3337365C3337375C303030425C303030615C303030795C303030655C303030735C303030695C303030615C3030306E5C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{snoek2012practical}
\abx@aux@segm{0}{0}{snoek2012practical}
\abx@aux@cite{0}{bischl2021hyperparameter}
\abx@aux@segm{0}{0}{bischl2021hyperparameter}
\abx@aux@cite{0}{andonie2019hyperparameter}
\abx@aux@segm{0}{0}{andonie2019hyperparameter}
\abx@aux@cite{0}{garrido2020dealing}
\abx@aux@segm{0}{0}{garrido2020dealing}
\abx@aux@cite{0}{hutter2011sequential}
\abx@aux@segm{0}{0}{hutter2011sequential}
\abx@aux@cite{0}{wilson2018maximizing}
\abx@aux@segm{0}{0}{wilson2018maximizing}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison of grid search (left) and random search (right) in the two dimensional case. For both techniques, 9 different combinations are evaluated. In the left case, only 3 distinct values for each hyperparameter are set whereas there are 9 different values for each parameter in the random search. \relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:comparison_searches}{{2.3}{8}{Comparison of grid search (left) and random search (right) in the two dimensional case. For both techniques, 9 different combinations are evaluated. In the left case, only 3 distinct values for each hyperparameter are set whereas there are 9 different values for each parameter in the random search. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Bayesian Optimization}{8}{subsection.2.2.3}\protected@file@percent }
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Bayesian Optimization for a black box function f. In each iteration, the surrogate model is fitted on the current archive and an acquisition function is built. The optimum of this acquisition function is evaluated and added to the archive.\relax }}{10}{algorithm.1}\protected@file@percent }
\newlabel{alg:bayesian_opt}{{1}{10}{Bayesian Optimization for a black box function f. In each iteration, the surrogate model is fitted on the current archive and an acquisition function is built. The optimum of this acquisition function is evaluated and added to the archive.\relax }{algorithm.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  Schematic iteration steps of the bayesian optimization. The maximum of the acquisition function determines the next function evaluation (red dot in the middle). The goal is to find the minimum of the dashed line. The blue band is the uncertainty of the function. \relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:bayesian_optimization}{{2.4}{11}{Schematic iteration steps of the bayesian optimization. The maximum of the acquisition function determines the next function evaluation (red dot in the middle). The goal is to find the minimum of the dashed line. The blue band is the uncertainty of the function. \relax }{figure.caption.7}{}}
\BKM@entry{id=11,dest={73756273656374696F6E2E322E322E34},srcline={166}}{5C3337365C3337375C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\abx@aux@cite{0}{jamieson2016non}
\abx@aux@segm{0}{0}{jamieson2016non}
\abx@aux@cite{0}{8030298}
\abx@aux@segm{0}{0}{8030298}
\abx@aux@cite{0}{smithson2016neural}
\abx@aux@segm{0}{0}{smithson2016neural}
\abx@aux@cite{0}{loshchilov2016cma}
\abx@aux@segm{0}{0}{loshchilov2016cma}
\BKM@entry{id=12,dest={73656374696F6E2E322E33},srcline={170}}{5C3337365C3337375C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\BKM@entry{id=13,dest={73756273656374696F6E2E322E332E31},srcline={174}}{5C3337365C3337375C3030304E5C303030755C3030306D5C303030655C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Other Techniques}{12}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Sparse Grids}{12}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Numerical Approximation of Functions}{12}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the nodal basis. Level of the grid is 3 and hat functions are used. \relax }}{13}{figure.caption.8}\protected@file@percent }
\newlabel{fig:interpolant}{{2.5}{13}{Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the nodal basis. Level of the grid is 3 and hat functions are used. \relax }{figure.caption.8}{}}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Hierarchical subspaces up to level 3 on the left. On the right, nodal spaces up to level 3. The combination of $ W_1 $ up to $ W_3 $ is the same space as $ V_3 $. \relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:hierarchical_basis}{{2.6}{14}{Hierarchical subspaces up to level 3 on the left. On the right, nodal spaces up to level 3. The combination of $ W_1 $ up to $ W_3 $ is the same space as $ V_3 $. \relax }{figure.caption.9}{}}
\abx@aux@cite{0}{garcke2013sparse}
\abx@aux@segm{0}{0}{garcke2013sparse}
\abx@aux@cite{0}{garcke2013sparse}
\abx@aux@segm{0}{0}{garcke2013sparse}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the hierarchical basis. Level of the grid is 3 and hat functions are used. Taken from \blx@tocontentsinit {0}\cite {pfluger2010spatially}.\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig:weighted_sum_hierarchical}{{2.7}{15}{Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the hierarchical basis. Level of the grid is 3 and hat functions are used. Taken from \cite {pfluger2010spatially}.\relax }{figure.caption.10}{}}
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\BKM@entry{id=14,dest={73756273656374696F6E2E322E332E32},srcline={286}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{zenger1991sparse}
\abx@aux@segm{0}{0}{zenger1991sparse}
\abx@aux@cite{0}{bungartz2004sparse}
\abx@aux@segm{0}{0}{bungartz2004sparse}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces  Example of a basis function in two dimensions. It is constructed with the tensor product of two 1d hat functions. Taken from \blx@tocontentsinit {0}\cite {garcke2013sparse}. \relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:2d_basis}{{2.8}{16}{Example of a basis function in two dimensions. It is constructed with the tensor product of two 1d hat functions. Taken from \cite {garcke2013sparse}. \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Adaptive Sparse Grids}{16}{subsection.2.3.2}\protected@file@percent }
\abx@aux@cite{0}{obersteiner2022spatially}
\abx@aux@segm{0}{0}{obersteiner2022spatially}
\abx@aux@cite{0}{griebel1990combination}
\abx@aux@segm{0}{0}{griebel1990combination}
\abx@aux@cite{0}{obersteiner2021generalized}
\abx@aux@segm{0}{0}{obersteiner2021generalized}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces  Two dimensional example of a sparse grid with $ n = 3 $. Left, the subspaces $ W_{\vec  {l}} $ can be seen and on the right is the resulting sparse grid. \relax }}{17}{figure.caption.12}\protected@file@percent }
\newlabel{fig:sparse_grid}{{2.9}{17}{Two dimensional example of a sparse grid with $ n = 3 $. Left, the subspaces $ W_{\vec {l}} $ can be seen and on the right is the resulting sparse grid. \relax }{figure.caption.12}{}}
\abx@aux@cite{0}{hegland2002adaptive}
\abx@aux@segm{0}{0}{hegland2002adaptive}
\abx@aux@cite{0}{obersteiner2021generalized}
\abx@aux@segm{0}{0}{obersteiner2021generalized}
\abx@aux@cite{0}{obersteiner2021generalized}
\abx@aux@segm{0}{0}{obersteiner2021generalized}
\abx@aux@cite{0}{obersteiner2021generalized}
\abx@aux@segm{0}{0}{obersteiner2021generalized}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces  Example of the 2-dimensional combination technique. Here the blue regular grids are added and the red ones are subtracted. On the left, the normal combination technique can be seen and on the right is an dimension-adaptive version. \relax }}{18}{figure.caption.13}\protected@file@percent }
\newlabel{fig:combi_technique}{{2.10}{18}{Example of the 2-dimensional combination technique. Here the blue regular grids are added and the red ones are subtracted. On the left, the normal combination technique can be seen and on the right is an dimension-adaptive version. \relax }{figure.caption.13}{}}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\BKM@entry{id=15,dest={73756273656374696F6E2E322E332E33},srcline={357}}{5C3337365C3337375C303030425C303030615C303030735C303030695C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{bungartz1998finite}
\abx@aux@segm{0}{0}{bungartz1998finite}
\abx@aux@cite{0}{bungartz2004sparse}
\abx@aux@segm{0}{0}{bungartz2004sparse}
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\abx@aux@cite{0}{hollig2013approximation}
\abx@aux@segm{0}{0}{hollig2013approximation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces  Example of the spatially adaptive combination technique in two dimensions. Taken from \blx@tocontentsinit {0}\cite {obersteiner2021generalized}. \relax }}{19}{figure.caption.14}\protected@file@percent }
\newlabel{fig:spatially_adaptive_combi_technique}{{2.11}{19}{Example of the spatially adaptive combination technique in two dimensions. Taken from \cite {obersteiner2021generalized}. \relax }{figure.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces  Comparison of sparse grids, full grids, and the combination technique in terms of number of grid points and the accuracy. \relax }}{19}{table.caption.15}\protected@file@percent }
\newlabel{tab:comparison_grids}{{2.1}{19}{Comparison of sparse grids, full grids, and the combination technique in terms of number of grid points and the accuracy. \relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Basis Functions for Sparse Grids}{19}{subsection.2.3.3}\protected@file@percent }
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\BKM@entry{id=16,dest={73756273656374696F6E2E322E332E34},srcline={389}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Optimization with Sparse Grids}{20}{subsection.2.3.4}\protected@file@percent }
\newlabel{Optimization_algorithms}{{2.3.4}{20}{Optimization with Sparse Grids}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gradient-Free Methods}{21}{paragraph*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Nelder Mead Method}{21}{subparagraph*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Differential Evolution}{21}{subparagraph*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline CMA-ES}{21}{subparagraph*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gradient-Based Methods}{21}{paragraph*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Gradient Descent}{21}{subparagraph*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline NLCG}{21}{subparagraph*.29}\protected@file@percent }
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\abx@aux@cite{0}{valentin2018gradient}
\abx@aux@segm{0}{0}{valentin2018gradient}
\abx@aux@cite{0}{novak1996global}
\abx@aux@segm{0}{0}{novak1996global}
\abx@aux@cite{0}{duan2016induction}
\abx@aux@segm{0}{0}{duan2016induction}
\abx@aux@cite{0}{duan2016induction}
\abx@aux@segm{0}{0}{duan2016induction}
\abx@aux@cite{0}{duan2016induction}
\abx@aux@segm{0}{0}{duan2016induction}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Newton}{22}{subparagraph*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline BFGS}{22}{subparagraph*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Rprop}{22}{subparagraph*.35}\protected@file@percent }
\abx@aux@cite{0}{saska2007path}
\abx@aux@segm{0}{0}{saska2007path}
\abx@aux@cite{0}{duan2016induction}
\abx@aux@segm{0}{0}{duan2016induction}
\abx@aux@cite{0}{novak1996global}
\abx@aux@segm{0}{0}{novak1996global}
\abx@aux@cite{0}{hulsmann2013spagrow}
\abx@aux@segm{0}{0}{hulsmann2013spagrow}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces  HPC of level $ k = 5 $ (red stars) and full grid (black dots). A full grid would have $ 33^2 = 1089 $ and this grid has $ 147 $ points. Taken from \blx@tocontentsinit {0}\cite {duan2016induction}.\relax }}{23}{figure.caption.36}\protected@file@percent }
\newlabel{fig:hyperbolic}{{2.12}{23}{HPC of level $ k = 5 $ (red stars) and full grid (black dots). A full grid would have $ 33^2 = 1089 $ and this grid has $ 147 $ points. Taken from \cite {duan2016induction}.\relax }{figure.caption.36}{}}
\abx@aux@cite{0}{chen2013derivative}
\abx@aux@segm{0}{0}{chen2013derivative}
\abx@aux@cite{0}{sankaran2009stochastic}
\abx@aux@segm{0}{0}{sankaran2009stochastic}
\BKM@entry{id=17,dest={73656374696F6E2E322E34},srcline={461}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\abx@aux@cite{0}{hamzaccebi2006heuristic}
\abx@aux@segm{0}{0}{hamzaccebi2006heuristic}
\abx@aux@cite{0}{schilling1999applied}
\abx@aux@segm{0}{0}{schilling1999applied}
\abx@aux@cite{0}{MASRI1980353}
\abx@aux@segm{0}{0}{MASRI1980353}
\abx@aux@cite{0}{MASRI1980353}
\abx@aux@segm{0}{0}{MASRI1980353}
\abx@aux@cite{0}{random_search_2}
\abx@aux@segm{0}{0}{random_search_2}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Adaptive Random Search}{24}{section.2.4}\protected@file@percent }
\newlabel{Adaptive_random_search}{{2.4}{24}{Adaptive Random Search}{section.2.4}{}}
\abx@aux@cite{0}{schumer1968adaptive}
\abx@aux@segm{0}{0}{schumer1968adaptive}
\abx@aux@cite{0}{tang1994adaptive}
\abx@aux@segm{0}{0}{tang1994adaptive}
\abx@aux@cite{0}{kelahan1978application}
\abx@aux@segm{0}{0}{kelahan1978application}
\abx@aux@cite{0}{gall1966practical}
\abx@aux@segm{0}{0}{gall1966practical}
\BKM@entry{id=18,dest={636861707465722E33},srcline={4}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\BKM@entry{id=19,dest={73656374696F6E2E332E31},srcline={6}}{5C3337365C3337375C3030304D5C303030655C303030745C303030685C3030306F5C303030645C3030306F5C3030306C5C3030306F5C303030675C30303079}
\BKM@entry{id=20,dest={73756273656374696F6E2E332E312E31},srcline={10}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030475C303030725C303030695C303030645C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{valentin2016hierarchical}
\abx@aux@segm{0}{0}{valentin2016hierarchical}
\BKM@entry{id=21,dest={73756273656374696F6E2E332E312E32},srcline={15}}{5C3337365C3337375C303030495C303030745C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=22,dest={73756273656374696F6E2E332E312E33},srcline={19}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Hyperparameter Optimization with Sparse Grids}{26}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:main_part}{{3}{26}{Hyperparameter Optimization with Sparse Grids}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Methodology}{26}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Adaptive Grid Search with Sparse Grids}{26}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Iterative Adaptive Random Search}{26}{subsection.3.1.2}\protected@file@percent }
\BKM@entry{id=23,dest={73656374696F6E2E332E32},srcline={23}}{5C3337365C3337375C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=24,dest={73756273656374696F6E2E332E322E31},srcline={25}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{feurer-arxiv19a}
\abx@aux@segm{0}{0}{feurer-arxiv19a}
\abx@aux@cite{0}{valentin2016hierarchical}
\abx@aux@segm{0}{0}{valentin2016hierarchical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Evaluation Metrics}{27}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Sparse Grid Optimization of Functions}{27}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Implementation}{27}{subsection.3.2.1}\protected@file@percent }
\newlabel{subsec:Implementation}{{3.2.1}{27}{Implementation}{subsection.3.2.1}{}}
\BKM@entry{id=25,dest={73756273656374696F6E2E332E322E32},srcline={55}}{5C3337365C3337375C303030545C303030655C303030735C303030745C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{valentin2016hierarchical}
\abx@aux@segm{0}{0}{valentin2016hierarchical}
\abx@aux@cite{0}{whitley1996evaluating}
\abx@aux@segm{0}{0}{whitley1996evaluating}
\abx@aux@cite{0}{yang2010engineering}
\abx@aux@segm{0}{0}{yang2010engineering}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Test Functions}{28}{subsection.3.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  Three test functions and their properties.\relax }}{28}{table.caption.37}\protected@file@percent }
\newlabel{tab:test_functions}{{3.1}{28}{Three test functions and their properties.\relax }{table.caption.37}{}}
\abx@aux@cite{0}{yang2010engineering}
\abx@aux@segm{0}{0}{yang2010engineering}
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\newlabel{fig:Eggholder}{{3.1a}{29}{Eggholder.\relax }{figure.caption.38}{}}
\newlabel{sub@fig:Eggholder}{{a}{29}{Eggholder.\relax }{figure.caption.38}{}}
\newlabel{fig:Rosenbrock}{{3.1b}{29}{$ log_{10}(f) $ of Rosenbrock.\relax }{figure.caption.38}{}}
\newlabel{sub@fig:Rosenbrock}{{b}{29}{$ log_{10}(f) $ of Rosenbrock.\relax }{figure.caption.38}{}}
\newlabel{fig:Rastrigin}{{3.1c}{29}{Rastrigin.\relax }{figure.caption.38}{}}
\newlabel{sub@fig:Rastrigin}{{c}{29}{Rastrigin.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Test functions used for evaluating the sparse grid optimization. Each one is plotted with 200 samples in each dimension. Note that the function values of the Rosenbrock function are transformed with $ log_{10}(f(x)) $ for better visualization.\relax }}{29}{figure.caption.38}\protected@file@percent }
\newlabel{fig:test_functions_plot}{{3.1}{29}{Test functions used for evaluating the sparse grid optimization. Each one is plotted with 200 samples in each dimension. Note that the function values of the Rosenbrock function are transformed with $ log_{10}(f(x)) $ for better visualization.\relax }{figure.caption.38}{}}
\BKM@entry{id=26,dest={73756273656374696F6E2E332E322E33},srcline={125}}{5C3337365C3337375C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030645C303030695C303030665C303030665C303030655C303030725C303030655C3030306E5C303030745C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030695C303030745C303030695C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Sparse Grid Generation with different Adaptivities}{30}{subsection.3.2.3}\protected@file@percent }
\newlabel{fig:Egg_gamma_1}{{3.2a}{31}{$ \gamma = 1.0 $\relax }{figure.caption.39}{}}
\newlabel{sub@fig:Egg_gamma_1}{{a}{31}{$ \gamma = 1.0 $\relax }{figure.caption.39}{}}
\newlabel{fig:Egg_gamma_0_5}{{3.2b}{31}{$ \gamma = 0.5 $\relax }{figure.caption.39}{}}
\newlabel{sub@fig:Egg_gamma_0_5}{{b}{31}{$ \gamma = 0.5 $\relax }{figure.caption.39}{}}
\newlabel{fig:Egg_gamma_0}{{3.2c}{31}{$ \gamma = 0.0 $\relax }{figure.caption.39}{}}
\newlabel{sub@fig:Egg_gamma_0}{{c}{31}{$ \gamma = 0.0 $\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Eggholder function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }}{31}{figure.caption.39}\protected@file@percent }
\newlabel{fig:Eggholder_grid}{{3.2}{31}{Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Eggholder function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }{figure.caption.39}{}}
\newlabel{fig:Ros_gamma_1}{{3.3a}{32}{$ \gamma = 1.0 $\relax }{figure.caption.40}{}}
\newlabel{sub@fig:Ros_gamma_1}{{a}{32}{$ \gamma = 1.0 $\relax }{figure.caption.40}{}}
\newlabel{fig:Ros_gamma_0_5}{{3.3b}{32}{$ \gamma = 0.5 $\relax }{figure.caption.40}{}}
\newlabel{sub@fig:Ros_gamma_0_5}{{b}{32}{$ \gamma = 0.5 $\relax }{figure.caption.40}{}}
\newlabel{fig:Ros_gamma_0}{{3.3c}{32}{$ \gamma = 0.0 $\relax }{figure.caption.40}{}}
\newlabel{sub@fig:Ros_gamma_0}{{c}{32}{$ \gamma = 0.0 $\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rosenbrock function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }}{32}{figure.caption.40}\protected@file@percent }
\newlabel{fig:Rosenbrock_grid}{{3.3}{32}{Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rosenbrock function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }{figure.caption.40}{}}
\newlabel{fig:Ras_gamma_1}{{3.4a}{33}{$ \gamma = 1.0 $\relax }{figure.caption.41}{}}
\newlabel{sub@fig:Ras_gamma_1}{{a}{33}{$ \gamma = 1.0 $\relax }{figure.caption.41}{}}
\newlabel{fig:Ras_gamma_0_5}{{3.4b}{33}{$ \gamma = 0.5 $\relax }{figure.caption.41}{}}
\newlabel{sub@fig:Ras_gamma_0_5}{{b}{33}{$ \gamma = 0.5 $\relax }{figure.caption.41}{}}
\newlabel{fig:Ras_gamma_0}{{3.4c}{33}{$ \gamma = 0.0 $\relax }{figure.caption.41}{}}
\newlabel{sub@fig:Ras_gamma_0}{{c}{33}{$ \gamma = 0.0 $\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rastrigin function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }}{33}{figure.caption.41}\protected@file@percent }
\newlabel{fig:Rastrigin_grid}{{3.4}{33}{Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rastrigin function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces  Influence of the adaptivity parameter on the error (difference to the actual optimal value) of the optimum found by the sparse grid. \relax }}{34}{figure.caption.42}\protected@file@percent }
\newlabel{fig:Functions_results}{{3.5}{34}{Influence of the adaptivity parameter on the error (difference to the actual optimal value) of the optimum found by the sparse grid. \relax }{figure.caption.42}{}}
\BKM@entry{id=27,dest={73756273656374696F6E2E332E322E34},srcline={323}}{5C3337365C3337375C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Local and Global Optimization}{35}{subsection.3.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces  Optimization error for different algorithms depending on the number of grid points in two dimensions. The plots show the results with degree 2 (top left), degree 3 (top right) and degree 5 (bottom). The Rastrigin function was used. \relax }}{36}{figure.caption.43}\protected@file@percent }
\newlabel{fig:Optimizer_results}{{3.6}{36}{Optimization error for different algorithms depending on the number of grid points in two dimensions. The plots show the results with degree 2 (top left), degree 3 (top right) and degree 5 (bottom). The Rastrigin function was used. \relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Resulting optimal points from local (red points) and global (black points) optimization algorithm. In the background, the contour of the interpolated function is depicted with the corresponding sparse grid points in white. The left one has 5 grid points, the one in the center 77, and the right one 997. The degree of the B-splines on the sparse grid is 2. \relax }}{37}{figure.caption.44}\protected@file@percent }
\newlabel{fig:optimizers_visualized}{{3.7}{37}{Resulting optimal points from local (red points) and global (black points) optimization algorithm. In the background, the contour of the interpolated function is depicted with the corresponding sparse grid points in white. The left one has 5 grid points, the one in the center 77, and the right one 997. The degree of the B-splines on the sparse grid is 2. \relax }{figure.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  Overview over the exact solutions found by the local and global optimizer for the same problem as in Figure \ref  {fig:optimizers_visualized}. The actual optimum is at (0, 0), with function value 0 (see Table \ref  {tab:test_functions}).\relax }}{38}{table.caption.45}\protected@file@percent }
\newlabel{tab:optimizer_vis_function}{{3.2}{38}{Overview over the exact solutions found by the local and global optimizer for the same problem as in Figure \ref {fig:optimizers_visualized}. The actual optimum is at (0, 0), with function value 0 (see Table \ref {tab:test_functions}).\relax }{table.caption.45}{}}
\BKM@entry{id=28,dest={73656374696F6E2E332E33},srcline={485}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\BKM@entry{id=29,dest={73756273656374696F6E2E332E332E31},srcline={489}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030755C3030306D5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{feurer-arxiv19a}
\abx@aux@segm{0}{0}{feurer-arxiv19a}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces  Error of the optimization depending on the number of grid points used for different dimensions of the Rastrigini function. \relax }}{39}{figure.caption.46}\protected@file@percent }
\newlabel{fig:Dimensions_results}{{3.8}{39}{Error of the optimization depending on the number of grid points used for different dimensions of the Rastrigini function. \relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Hyperparameter Optimization with Sparse Grids}{39}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Optimum Approximation with Sparse Grids}{40}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces  2-layer (with 30 neurons each) fully connected network evaluation on the diamonds dataset depending on the number of epochs and learning rate of the Adam optimizer. The result is plotted with $ log_{10}(\text  {Result}) $ for better visualization. \relax }}{40}{figure.caption.47}\protected@file@percent }
\newlabel{fig:analysis_model_training}{{3.9}{40}{2-layer (with 30 neurons each) fully connected network evaluation on the diamonds dataset depending on the number of epochs and learning rate of the Adam optimizer. The result is plotted with $ log_{10}(\text {Result}) $ for better visualization. \relax }{figure.caption.47}{}}
\newlabel{fig:analysis_sparse_grid_with_machine_learning_085}{{3.10a}{42}{Sparse grid generated with adaptivity parameter $ \gamma = 0.85 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper half for higher values of the learning rate. \relax }{figure.caption.48}{}}
\newlabel{sub@fig:analysis_sparse_grid_with_machine_learning_085}{{a}{42}{Sparse grid generated with adaptivity parameter $ \gamma = 0.85 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper half for higher values of the learning rate. \relax }{figure.caption.48}{}}
\newlabel{fig:analysis_sparse_grid_with_machine_learning_05}{{3.10b}{42}{Sparse grid generated with adaptivity parameter $ \gamma = 0.5 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper right half for higher values of the learning rate and epochs between 25 and 30. \relax }{figure.caption.48}{}}
\newlabel{sub@fig:analysis_sparse_grid_with_machine_learning_05}{{b}{42}{Sparse grid generated with adaptivity parameter $ \gamma = 0.5 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper right half for higher values of the learning rate and epochs between 25 and 30. \relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces  Analysis of sparse grid with machine learning evaluation for different adaptivity parameters ($ 0.85 $ \ref  {fig:analysis_sparse_grid_with_machine_learning_085} and $ 0.5 $ \ref  {fig:analysis_sparse_grid_with_machine_learning_05}), each with 29 and 77 grid points.\relax }}{42}{figure.caption.48}\protected@file@percent }
\newlabel{fig:analysis_sparse_grid_with_machine_learning}{{3.10}{42}{Analysis of sparse grid with machine learning evaluation for different adaptivity parameters ($ 0.85 $ \ref {fig:analysis_sparse_grid_with_machine_learning_085} and $ 0.5 $ \ref {fig:analysis_sparse_grid_with_machine_learning_05}), each with 29 and 77 grid points.\relax }{figure.caption.48}{}}
\BKM@entry{id=30,dest={73756273656374696F6E2E332E332E32},srcline={559}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces  Best hyperparameter configuration found by the sparse grid with different adaptivity parameters and number of grid points. The best result is found with $ \gamma = 0.85 $ and 77 grid points. \relax }}{43}{table.caption.49}\protected@file@percent }
\newlabel{tab:analysis_sparse_grid_with_machine_learning_results}{{3.3}{43}{Best hyperparameter configuration found by the sparse grid with different adaptivity parameters and number of grid points. The best result is found with $ \gamma = 0.85 $ and 77 grid points. \relax }{table.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Optimization on Sparse Grids}{43}{subsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces  Optimization steps of gradient descent algorithm for 5 (top left), 9 (top right), 29 (bottom left) and 49 (bottom right) grid points. In the background of each plot, the contour of the interpolated function is shown. The function evaluated is the same as depicted in Figure \ref  {fig:analysis_model_training}. \relax }}{44}{figure.caption.50}\protected@file@percent }
\newlabel{fig:optimizers_network_visualization}{{3.11}{44}{Optimization steps of gradient descent algorithm for 5 (top left), 9 (top right), 29 (bottom left) and 49 (bottom right) grid points. In the background of each plot, the contour of the interpolated function is shown. The function evaluated is the same as depicted in Figure \ref {fig:analysis_model_training}. \relax }{figure.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces  Exact values for the optima found by the sparse grid points and the gradient descent algorithm. The values for $ x_{min}^{grid} $ are the configurations evaluated by the sparse grid during the generation and the coordinates in column $ x_{min}^{opt} $ are found by the optimizer. In bold, the best function values of the optimum found by the sparse grid and gradient descent algorithm, respectively. \relax }}{45}{table.caption.51}\protected@file@percent }
\newlabel{tab:results_opt_ml}{{3.4}{45}{Exact values for the optima found by the sparse grid points and the gradient descent algorithm. The values for $ x_{min}^{grid} $ are the configurations evaluated by the sparse grid during the generation and the coordinates in column $ x_{min}^{opt} $ are found by the optimizer. In bold, the best function values of the optimum found by the sparse grid and gradient descent algorithm, respectively. \relax }{table.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces  Comparison of optimization algorithms on the sparse grid with increasing number of grid points in two and three dimensions. Epochs, learning rate, and the batch size of a two-layer neural network on the diamonds dataset for regression are optimized. Result is the mean absolute percentage error. \relax }}{46}{figure.caption.52}\protected@file@percent }
\newlabel{fig:Comparison_optimizers}{{3.12}{46}{Comparison of optimization algorithms on the sparse grid with increasing number of grid points in two and three dimensions. Epochs, learning rate, and the batch size of a two-layer neural network on the diamonds dataset for regression are optimized. Result is the mean absolute percentage error. \relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces  Sparse grid optimization with the interpolated function in the background, the white grid points, and the optimizer steps in red. Four different budgets are used (30: top left, 50: top right, 100: bottom left, 200: bottom right. Only three different configurations with values for batch size from $ {100, 400, 2000} $ and values for epochs from $ {1, 5, 13} $. \relax }}{48}{figure.caption.53}\protected@file@percent }
\newlabel{fig:optimizer_categorical_hyperparams}{{3.13}{48}{Sparse grid optimization with the interpolated function in the background, the white grid points, and the optimizer steps in red. Four different budgets are used (30: top left, 50: top right, 100: bottom left, 200: bottom right. Only three different configurations with values for batch size from $ {100, 400, 2000} $ and values for epochs from $ {1, 5, 13} $. \relax }{figure.caption.53}{}}
\BKM@entry{id=31,dest={73656374696F6E2E332E34},srcline={726}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030475C303030725C303030695C303030645C3030302D5C3030302C5C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030795C303030655C303030735C303030695C303030615C3030306E5C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=32,dest={73756273656374696F6E2E332E342E31},srcline={730}}{5C3337365C3337375C303030545C303030775C3030306F5C3030302D5C303030645C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C303030785C303030705C303030655C303030725C303030695C3030306D5C303030655C3030306E5C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C3030306D5C303030615C3030306C5C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B}
\abx@aux@cite{0}{OpenML2013}
\abx@aux@segm{0}{0}{OpenML2013}
\abx@aux@cite{0}{OpenML2013}
\abx@aux@segm{0}{0}{OpenML2013}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Comparison with Grid-, Random Search and Bayesian Optimization}{49}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Two-dimensional Experiment of Regression with Small Neural Network}{49}{subsection.3.4.1}\protected@file@percent }
\abx@aux@cite{0}{scikit-learn}
\abx@aux@segm{0}{0}{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces  Overview over the datasets used for the first comparison of the optimization algorithms. They are all available on OpenML \blx@tocontentsinit {0}\cite {OpenML2013}. \relax }}{50}{table.caption.54}\protected@file@percent }
\newlabel{tab:datasets_first_experiment}{{3.5}{50}{Overview over the datasets used for the first comparison of the optimization algorithms. They are all available on OpenML \cite {OpenML2013}. \relax }{table.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces  Comparison of grid-, random search, bayesian optimization and sparse grid search for the datasets shown in Table \ref  {tab:datasets_first_experiment}. Two hyperparameters, epochs and learning rate of the neural network's optimizer, were optimized. \relax }}{51}{figure.caption.55}\protected@file@percent }
\newlabel{fig:result_first_comparison}{{3.14}{51}{Comparison of grid-, random search, bayesian optimization and sparse grid search for the datasets shown in Table \ref {tab:datasets_first_experiment}. Two hyperparameters, epochs and learning rate of the neural network's optimizer, were optimized. \relax }{figure.caption.55}{}}
\newlabel{fig:first_comparison_grid_search}{{3.15a}{53}{Grid search with 9 (left, $f(x_{opt}) = 0.14393$), 25 (center, $f(x_{opt}) = 0.14080$), and 49 (right, $f(x_{opt}) = 0.14393$) grid points.\relax }{figure.caption.56}{}}
\newlabel{sub@fig:first_comparison_grid_search}{{a}{53}{Grid search with 9 (left, $f(x_{opt}) = 0.14393$), 25 (center, $f(x_{opt}) = 0.14080$), and 49 (right, $f(x_{opt}) = 0.14393$) grid points.\relax }{figure.caption.56}{}}
\newlabel{fig:first_comparison_random_search}{{3.15b}{53}{Random search with 10 (left, $f(x_{opt}) = 0.14141$), 30 (center, $f(x_{opt}) = 0.13526$), and 50 (right, $f(x_{opt}) = 0.13619$) samples. \relax }{figure.caption.56}{}}
\newlabel{sub@fig:first_comparison_random_search}{{b}{53}{Random search with 10 (left, $f(x_{opt}) = 0.14141$), 30 (center, $f(x_{opt}) = 0.13526$), and 50 (right, $f(x_{opt}) = 0.13619$) samples. \relax }{figure.caption.56}{}}
\newlabel{fig:first_comparison_bayesian_optimization}{{3.15c}{53}{Bayesian Optimization with 10 (left, $f(x_{opt}) = 0.13926$), 30 (center, $f(x_{opt}) = 0.13877$), and 50 (right, $f(x_{opt}) = 0.13542$) samples. \relax }{figure.caption.56}{}}
\newlabel{sub@fig:first_comparison_bayesian_optimization}{{c}{53}{Bayesian Optimization with 10 (left, $f(x_{opt}) = 0.13926$), 30 (center, $f(x_{opt}) = 0.13877$), and 50 (right, $f(x_{opt}) = 0.13542$) samples. \relax }{figure.caption.56}{}}
\newlabel{fig:first_comparison_sparse_optimization}{{3.15d}{53}{Sparse grid optimization with 5 (left, $f(x_{opt}) = 0.14393$), 25 (center, $f(x_{opt}) = 0.13739$), and 45 (right, $f(x_{opt}) = 0.13533$) grid points. \relax }{figure.caption.56}{}}
\newlabel{sub@fig:first_comparison_sparse_optimization}{{d}{53}{Sparse grid optimization with 5 (left, $f(x_{opt}) = 0.14393$), 25 (center, $f(x_{opt}) = 0.13739$), and 45 (right, $f(x_{opt}) = 0.13533$) grid points. \relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces  Comparison of the grids generated for the house\_sales dataset. The best configuration found $ x_{opt} $ is marked with a white cross.\relax }}{53}{figure.caption.56}\protected@file@percent }
\newlabel{fig:Comparison_visualization}{{3.15}{53}{Comparison of the grids generated for the house\_sales dataset. The best configuration found $ x_{opt} $ is marked with a white cross.\relax }{figure.caption.56}{}}
\BKM@entry{id=33,dest={73756273656374696F6E2E332E342E32},srcline={923}}{5C3337365C3337375C303030545C303030685C303030725C303030655C303030655C3030302D5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030695C303030765C303030655C3030302D5C303030645C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C303030785C303030705C303030655C303030725C303030695C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C3030306D5C303030615C3030306C5C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Three- and Five-dimensional Experiments of Regression with Small Neural Network}{54}{subsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces  Comparison of grid search, random search, bayesian optimization and sparse grid search. The model optimized consists of two layers with 40 neurons each. The number of epochs, batch size, and learning rate are optimized. \relax }}{55}{figure.caption.57}\protected@file@percent }
\newlabel{fig:result_second_comparison_dim3}{{3.16}{55}{Comparison of grid search, random search, bayesian optimization and sparse grid search. The model optimized consists of two layers with 40 neurons each. The number of epochs, batch size, and learning rate are optimized. \relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces  Comparison of grid search, random search, bayesian optimization and sparse grid search. The number of epochs, batch size, and learning rate are optimized as well as the number of layers and number of neurons per layer. \relax }}{56}{figure.caption.58}\protected@file@percent }
\newlabel{fig:result_second_comparison_dim5}{{3.17}{56}{Comparison of grid search, random search, bayesian optimization and sparse grid search. The number of epochs, batch size, and learning rate are optimized as well as the number of layers and number of neurons per layer. \relax }{figure.caption.58}{}}
\BKM@entry{id=34,dest={73756273656374696F6E2E332E342E33},srcline={1097}}{5C3337365C3337375C3030304E5C303030695C3030306E5C303030655C3030302D5C303030645C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C303030785C303030705C303030655C303030725C303030695C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C3030304E5C303030495C303030535C303030545C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Nine-dimensional Experiment with MNIST Dataset}{57}{subsection.3.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces  Hyperparameters with their type and interval for the nine dimensional space. The values for the Int-Interval are discretized with Python's \textit  {int()} function. The learning rate is sampled logarithmic and the dropout probability can take continuous values between 0 and 1. \relax }}{57}{table.caption.59}\protected@file@percent }
\newlabel{tab:hyperparameter_space_mnist}{{3.6}{57}{Hyperparameters with their type and interval for the nine dimensional space. The values for the Int-Interval are discretized with Python's \textit {int()} function. The learning rate is sampled logarithmic and the dropout probability can take continuous values between 0 and 1. \relax }{table.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces  Hyperparameter optimization of a convolutional neural network on the MNIST dataset with different algorithms. The hyperparameter intervals are presented in Table \ref  {tab:hyperparameter_space_mnist}. \relax }}{58}{figure.caption.60}\protected@file@percent }
\newlabel{fig:MNIST_results}{{3.18}{58}{Hyperparameter optimization of a convolutional neural network on the MNIST dataset with different algorithms. The hyperparameter intervals are presented in Table \ref {tab:hyperparameter_space_mnist}. \relax }{figure.caption.60}{}}
\BKM@entry{id=35,dest={73756273656374696F6E2E332E342E34},srcline={1185}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030306F5C303030745C303030685C303030655C303030725C3030305C3034305C303030415C303030755C303030745C303030685C3030306F5C303030725C30303073}
\abx@aux@cite{0}{WU201926}
\abx@aux@segm{0}{0}{WU201926}
\abx@aux@cite{0}{WU201926}
\abx@aux@segm{0}{0}{WU201926}
\abx@aux@cite{0}{WU201926}
\abx@aux@segm{0}{0}{WU201926}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces  Overview over the best configurations found by the different algorithms grid search (GS), random search (RS), bayesian optimization (BO) and sparse grid optimization (SG). The configuration is given as tuple (epochs, batch size, learning rate, number conv layers, number fully connected layers, kernel size, pool size, neurons per fc layer, dropout probability). \relax }}{59}{table.caption.61}\protected@file@percent }
\newlabel{tab:best_configurations_MNIST}{{3.7}{59}{Overview over the best configurations found by the different algorithms grid search (GS), random search (RS), bayesian optimization (BO) and sparse grid optimization (SG). The configuration is given as tuple (epochs, batch size, learning rate, number conv layers, number fully connected layers, kernel size, pool size, neurons per fc layer, dropout probability). \relax }{table.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Comparison with Implementation of other Authors}{59}{subsection.3.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces  Architecture used in \blx@tocontentsinit {0}\cite {WU201926} for MNIST image classification. \relax }}{59}{table.caption.62}\protected@file@percent }
\newlabel{tab:architecture_MNIST_comp}{{3.8}{59}{Architecture used in \cite {WU201926} for MNIST image classification. \relax }{table.caption.62}{}}
\abx@aux@cite{0}{WU201926}
\abx@aux@segm{0}{0}{WU201926}
\abx@aux@cite{0}{WU201926}
\abx@aux@segm{0}{0}{WU201926}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces  Resulting accuracy with increasing budget for the grid search, random search, bayesian optimization and sparse grid optimization. The MNIST dataset and a small convolutional neural network were used. \relax }}{60}{figure.caption.63}\protected@file@percent }
\newlabel{fig:Comparison_MNIST}{{3.19}{60}{Resulting accuracy with increasing budget for the grid search, random search, bayesian optimization and sparse grid optimization. The MNIST dataset and a small convolutional neural network were used. \relax }{figure.caption.63}{}}
\abx@aux@cite{0}{WU201926}
\abx@aux@segm{0}{0}{WU201926}
\BKM@entry{id=36,dest={73656374696F6E2E332E35},srcline={1264}}{5C3337365C3337375C303030495C303030745C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=37,dest={73756273656374696F6E2E332E352E31},srcline={1268}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lot}{\contentsline {table}{\numberline {3.9}{\ignorespaces  Best configurations and corresponding test accuracy found by the four algorithms for the MNIST dataset. \relax }}{61}{table.caption.64}\protected@file@percent }
\newlabel{tab:configs_MNIST_comp}{{3.9}{61}{Best configurations and corresponding test accuracy found by the four algorithms for the MNIST dataset. \relax }{table.caption.64}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Iterative Adaptive Random Search}{61}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Implementation}{61}{subsection.3.5.1}\protected@file@percent }
\newlabel{eq:refinement_criterion}{{3.7}{61}{Implementation}{equation.3.5.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Iterative adaptive random search for hyperparameter optimization. In each iteration, a random point is sampled following a specific distribution depending on the refinement criterion. \relax }}{62}{algorithm.2}\protected@file@percent }
\newlabel{alg:adaptive_random_search_alg}{{2}{62}{Iterative adaptive random search for hyperparameter optimization. In each iteration, a random point is sampled following a specific distribution depending on the refinement criterion. \relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Interval Based Refinement Strategy}{63}{subsubsection*.66}\protected@file@percent }
\abx@aux@cite{0}{HARMAN20102297}
\abx@aux@segm{0}{0}{HARMAN20102297}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces  Example for the interval based refinement strategy. The point being refined is indicated with the blue cross and the lines show the upper and lower boundary in each direction. The sampling is done uniformly in the blue area. \relax }}{64}{figure.caption.67}\protected@file@percent }
\newlabel{fig:alternative_1}{{3.20}{64}{Example for the interval based refinement strategy. The point being refined is indicated with the blue cross and the lines show the upper and lower boundary in each direction. The sampling is done uniformly in the blue area. \relax }{figure.caption.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Uniform D-Ball Sampling Strategy}{64}{subsubsection*.69}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces  Example for the uniform d-ball sampling strategy. The point being refined is indicated with the blue cross and the circle is the area where the uniform sampling is done. \relax }}{65}{figure.caption.70}\protected@file@percent }
\newlabel{fig:alternative_2}{{3.21}{65}{Example for the uniform d-ball sampling strategy. The point being refined is indicated with the blue cross and the circle is the area where the uniform sampling is done. \relax }{figure.caption.70}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Normal Distribution Sampling Strategy}{66}{subsubsection*.72}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces  Example for the normal distribution sampling strategy. The point being refined is indicated with the blue cross and contour represents the likelihood of samples. \relax }}{66}{figure.caption.73}\protected@file@percent }
\newlabel{fig:alternative_3}{{3.22}{66}{Example for the normal distribution sampling strategy. The point being refined is indicated with the blue cross and contour represents the likelihood of samples. \relax }{figure.caption.73}{}}
\BKM@entry{id=38,dest={73756273656374696F6E2E332E352E32},srcline={1377}}{5C3337365C3337375C303030415C3030306E5C303030615C3030306C5C303030795C303030735C303030695C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Analysis of Parameters with Functions}{67}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Adaptivity Parameter}{67}{subsubsection*.75}\protected@file@percent }
\newlabel{fig:alt0_adapt_param}{{3.23a}{69}{Resulting points for the \textbf {interval based refinement strategy}. Adaptivity parameter $ \gamma $: 1.0 (left), 0.75 (center), 0.0 (right). \relax }{figure.caption.76}{}}
\newlabel{sub@fig:alt0_adapt_param}{{a}{69}{Resulting points for the \textbf {interval based refinement strategy}. Adaptivity parameter $ \gamma $: 1.0 (left), 0.75 (center), 0.0 (right). \relax }{figure.caption.76}{}}
\newlabel{fig:alt1_adapt_param}{{3.23b}{69}{Resulting points for the \textbf {uniform d-ball sampling strategy}. Adaptivity parameter $ \gamma $: 1.0 (left), 0.75 (center), 0.0 (right). \relax }{figure.caption.76}{}}
\newlabel{sub@fig:alt1_adapt_param}{{b}{69}{Resulting points for the \textbf {uniform d-ball sampling strategy}. Adaptivity parameter $ \gamma $: 1.0 (left), 0.75 (center), 0.0 (right). \relax }{figure.caption.76}{}}
\newlabel{fig:alt2_adapt_param}{{3.23c}{69}{Resulting points for the \textbf {normal distribution sampling strategy}. Adaptivity parameter $ \gamma $: 1.0 (left), 0.75 (center), 0.0 (right). \relax }{figure.caption.76}{}}
\newlabel{sub@fig:alt2_adapt_param}{{c}{69}{Resulting points for the \textbf {normal distribution sampling strategy}. Adaptivity parameter $ \gamma $: 1.0 (left), 0.75 (center), 0.0 (right). \relax }{figure.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces  Generated points for optimizing the Rosenbrock function with interval based refinement strategy (\ref  {fig:alt0_adapt_param}), uniform d-ball sampling strategy (\ref  {fig:alt1_adapt_param}), and normal distribution sampling strategy (\ref  {fig:alt2_adapt_param}). In each case, the left one has adaptivity parameter $ \gamma = 1.0 $ which distributes the points most homogeneous. In the center with $ \gamma = 0.75 $, they are concentrated on few areas and in each right case where $ \gamma = 0.0 $, the grid is most adaptive and almost all points are in the same region. \relax }}{69}{figure.caption.76}\protected@file@percent }
\newlabel{fig:Adapt_param_alternatives}{{3.23}{69}{Generated points for optimizing the Rosenbrock function with interval based refinement strategy (\ref {fig:alt0_adapt_param}), uniform d-ball sampling strategy (\ref {fig:alt1_adapt_param}), and normal distribution sampling strategy (\ref {fig:alt2_adapt_param}). In each case, the left one has adaptivity parameter $ \gamma = 1.0 $ which distributes the points most homogeneous. In the center with $ \gamma = 0.75 $, they are concentrated on few areas and in each right case where $ \gamma = 0.0 $, the grid is most adaptive and almost all points are in the same region. \relax }{figure.caption.76}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.10}{\ignorespaces  Resulting optimum found by the three different refinement strategies interval based refinement strategy (1), uniform d-ball sampling strategy (2), and normal distribution sampling strategy (3). The best of each refinement algorithm is marked in bold. \relax }}{70}{table.caption.77}\protected@file@percent }
\newlabel{tab:results_alternatives}{{3.10}{70}{Resulting optimum found by the three different refinement strategies interval based refinement strategy (1), uniform d-ball sampling strategy (2), and normal distribution sampling strategy (3). The best of each refinement algorithm is marked in bold. \relax }{table.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.24}{\ignorespaces  Error of the found optimum by the different refinement strategies for the Rosenbrock (2d) and the Rastrigin (2d and 10d) functions. For the two-dimensional case, the budget is 200 and for the 10-d optimization the budget is set to 5000. \relax }}{71}{figure.caption.78}\protected@file@percent }
\newlabel{fig:alternatives_adaptivities_plot}{{3.24}{71}{Error of the found optimum by the different refinement strategies for the Rosenbrock (2d) and the Rastrigin (2d and 10d) functions. For the two-dimensional case, the budget is 200 and for the 10-d optimization the budget is set to 5000. \relax }{figure.caption.78}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Number of initial points}{72}{subsubsection*.80}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.25}{\ignorespaces  Error of the optimum found by the algorithm depending on the number of initial points. The budget is 200 in all two dimensional cases and 400 in the 4- and 6-dimensional problem. Each point is the average of 10 evaluations. \relax }}{73}{figure.caption.81}\protected@file@percent }
\newlabel{fig:init_points_alternatives}{{3.25}{73}{Error of the optimum found by the algorithm depending on the number of initial points. The budget is 200 in all two dimensional cases and 400 in the 4- and 6-dimensional problem. Each point is the average of 10 evaluations. \relax }{figure.caption.81}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Number of refinements per iteration}{74}{subsubsection*.83}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.26}{\ignorespaces  Error of the optimum found by the algorithm depending on the number of points added per refinement step. The budget is 200 in all two dimensional cases and 400 in the 4- and 6-dimensional problems. Each point is the average of 10 evaluations. \relax }}{75}{figure.caption.84}\protected@file@percent }
\newlabel{fig:ref_per_step}{{3.26}{75}{Error of the optimum found by the algorithm depending on the number of points added per refinement step. The budget is 200 in all two dimensional cases and 400 in the 4- and 6-dimensional problems. Each point is the average of 10 evaluations. \relax }{figure.caption.84}{}}
\BKM@entry{id=39,dest={73756273656374696F6E2E332E352E33},srcline={1854}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Hyperparameter Optimization}{76}{subsection.3.5.3}\protected@file@percent }
\newlabel{fig:alt0_ml}{{3.27a}{77}{Sampled points for refinement strategy 1. The budget is 10 (left), 50 (center), and 100 (right). \relax }{figure.caption.85}{}}
\newlabel{sub@fig:alt0_ml}{{a}{77}{Sampled points for refinement strategy 1. The budget is 10 (left), 50 (center), and 100 (right). \relax }{figure.caption.85}{}}
\newlabel{fig:alt1_ml}{{3.27b}{77}{Sampled points for refinement strategy 1. The budget is 10 (left), 50 (center), and 100 (right). \relax }{figure.caption.85}{}}
\newlabel{sub@fig:alt1_ml}{{b}{77}{Sampled points for refinement strategy 1. The budget is 10 (left), 50 (center), and 100 (right). \relax }{figure.caption.85}{}}
\newlabel{fig:alt2_ml}{{3.27c}{77}{Sampled points for refinement strategy 1. The budget is 10 (left), 50 (center), and 100 (right). \relax }{figure.caption.85}{}}
\newlabel{sub@fig:alt2_ml}{{c}{77}{Sampled points for refinement strategy 1. The budget is 10 (left), 50 (center), and 100 (right). \relax }{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.27}{\ignorespaces  Visualization of the sampled points for the regression problem of the diamonds dataset solved by a two-layer neural network. The influence of the choice of the refinement strategy can be seen in \ref  {fig:alt0_ml}, \ref  {fig:alt1_ml}, and \ref  {fig:alt2_ml}. The adaptivity parameters are $ \gamma _1 = 0.75 $, $ \gamma _2 = 0.35 $, and $ \gamma _3 = 0.6 $ and 20, 16, and 14 initial points are sampled, respectively. The number of refinements per step are 4 in each case. \relax }}{77}{figure.caption.85}\protected@file@percent }
\newlabel{fig:Alternatives_ml_visualization}{{3.27}{77}{Visualization of the sampled points for the regression problem of the diamonds dataset solved by a two-layer neural network. The influence of the choice of the refinement strategy can be seen in \ref {fig:alt0_ml}, \ref {fig:alt1_ml}, and \ref {fig:alt2_ml}. The adaptivity parameters are $ \gamma _1 = 0.75 $, $ \gamma _2 = 0.35 $, and $ \gamma _3 = 0.6 $ and 20, 16, and 14 initial points are sampled, respectively. The number of refinements per step are 4 in each case. \relax }{figure.caption.85}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.11}{\ignorespaces  Best configurations found by the adaptive iterative random search with three different refinement strategies and budgets 10, 50, and 100. \relax }}{78}{table.caption.86}\protected@file@percent }
\newlabel{tab:results_alternatives_ml}{{3.11}{78}{Best configurations found by the adaptive iterative random search with three different refinement strategies and budgets 10, 50, and 100. \relax }{table.caption.86}{}}
\BKM@entry{id=40,dest={73756273656374696F6E2E332E352E34},srcline={1974}}{5C3337365C3337375C303030485C303030695C303030675C303030685C3030302D5C303030645C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {3.28}{\ignorespaces  Mean average percentage error depending on the cost for the different optimization algorithms. The diamonds regression task and a simple two-layer neural network are used. \relax }}{79}{figure.caption.87}\protected@file@percent }
\newlabel{fig:comparison_2d}{{3.28}{79}{Mean average percentage error depending on the cost for the different optimization algorithms. The diamonds regression task and a simple two-layer neural network are used. \relax }{figure.caption.87}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}High-dimensional Optimization}{79}{subsection.3.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.12}{\ignorespaces  Hyperparameters with their type and interval for the 6-dimensional optimization problem solved with the iterative adaptive random search. \relax }}{80}{table.caption.88}\protected@file@percent }
\newlabel{tab:hyperparameter_space_adaptive_random_search}{{3.12}{80}{Hyperparameters with their type and interval for the 6-dimensional optimization problem solved with the iterative adaptive random search. \relax }{table.caption.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.29}{\ignorespaces  Resulting mean average percentage error for the 6-dimensional optimization problem with hyperparameters from Table \ref  {tab:hyperparameter_space_adaptive_random_search}. The iterative adaptive random search with three different refinement strategies depicted in different colors. The Brazilian\_housing dataset was used and each point is the average of 10 runs. \relax }}{80}{figure.caption.89}\protected@file@percent }
\newlabel{fig:result_adaptive_random_high_dim}{{3.29}{80}{Resulting mean average percentage error for the 6-dimensional optimization problem with hyperparameters from Table \ref {tab:hyperparameter_space_adaptive_random_search}. The iterative adaptive random search with three different refinement strategies depicted in different colors. The Brazilian\_housing dataset was used and each point is the average of 10 runs. \relax }{figure.caption.89}{}}
\BKM@entry{id=41,dest={73656374696F6E2E332E36},srcline={2071}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030695C303030735C303030635C303030755C303030735C303030735C303030695C3030306F5C3030306E}
\@writefile{lot}{\contentsline {table}{\numberline {3.13}{\ignorespaces  Comparison of the optimization algorithms in terms of mean average percentage error for the 6-dimensional optimization problem. \relax }}{81}{table.caption.90}\protected@file@percent }
\newlabel{tab:comparison_all_algorithms}{{3.13}{81}{Comparison of the optimization algorithms in terms of mean average percentage error for the 6-dimensional optimization problem. \relax }{table.caption.90}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Comparison and Discussion}{81}{section.3.6}\protected@file@percent }
\BKM@entry{id=42,dest={636861707465722E34},srcline={4}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C3030306C5C3030306F5C3030306F5C3030306B}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusion and Outlook}{83}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion_and_outlook}{{4}{83}{Conclusion and Outlook}{chapter.4}{}}
\BKM@entry{id=43,dest={636861707465722A2E3931},srcline={108}}{5C3337365C3337375C3030304C5C303030695C303030735C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030695C303030675C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\nonumberline List of Figures}{84}{chapter*.91}\protected@file@percent }
\BKM@entry{id=44,dest={636861707465722A2E3932},srcline={109}}{5C3337365C3337375C3030304C5C303030695C303030735C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030545C303030615C303030625C3030306C5C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\nonumberline List of Tables}{89}{chapter*.92}\protected@file@percent }
\BKM@entry{id=45,dest={636861707465722A2E3933},srcline={111}}{5C3337365C3337375C303030425C303030695C303030625C3030306C5C303030695C3030306F5C303030675C303030725C303030615C303030705C303030685C30303079}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Bibliography}{91}{chapter*.93}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\abx@aux@read@bbl@mdfivesum{83B93BC22910F81A204740E6440FC282}
\abx@aux@defaultrefcontext{0}{MALAKOUTI2023200248}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{gorgolis2019hyperparameter}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{wang2016machine}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mahesh2020machine}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ayodele2010types}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{noble2006support}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{granmo2018tsetlin}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rokach2005decision}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop1994neural}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{da2017artificial}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{medsker2001recurrent}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{li2021survey}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{gu2018recent}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{o2015introduction}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{liu2017survey}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{supervised_learning}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{feurer2019hyperparameter}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bischl2021hyperparameter}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2020hyperparameter}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{random_search}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{snoek2012practical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{andonie2019hyperparameter}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{garrido2020dealing}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hutter2011sequential}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{wilson2018maximizing}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{jamieson2016non}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{8030298}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{smithson2016neural}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{loshchilov2016cma}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{pfluger2010spatially}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{garcke2013sparse}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{b_splines}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{zenger1991sparse}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bungartz2004sparse}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{obersteiner2022spatially}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{griebel1990combination}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{obersteiner2021generalized}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hegland2002adaptive}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bungartz1998finite}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hollig2013approximation}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{valentin2018gradient}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{novak1996global}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{duan2016induction}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{saska2007path}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hulsmann2013spagrow}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{chen2013derivative}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{sankaran2009stochastic}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hamzaccebi2006heuristic}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{schilling1999applied}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{MASRI1980353}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{random_search_2}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{schumer1968adaptive}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{tang1994adaptive}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kelahan1978application}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{gall1966practical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{feurer-arxiv19a}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{valentin2016hierarchical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{whitley1996evaluating}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2010engineering}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{OpenML2013}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{scikit-learn}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{WU201926}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{HARMAN20102297}{none/global//global/global}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{1}}
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{9.85492pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{18.0674pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{26.27988pt}
\global\@namedef{scr@dte@table@lastmaxnumwidth}{23.54239pt}
\global\@namedef{scr@dte@figure@lastmaxnumwidth}{23.54239pt}
\@writefile{toc}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\@writefile{lof}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\@writefile{lot}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\gdef \@abspage@last{104}
