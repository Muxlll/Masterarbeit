\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\abx@aux@refcontext{none/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\babel@aux{american}{}
\BKM@entry{id=1,dest={446F632D5374617274},srcline={1}}{5C3337365C3337375C303030415C303030635C3030306B5C3030306E5C3030306F5C303030775C3030306C5C303030655C303030645C303030675C3030306D5C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{iii}{Doc-Start}\protected@file@percent }
\BKM@entry{id=2,dest={636861707465722A2E31},srcline={1}}{5C3337365C3337375C303030415C303030625C303030735C303030745C303030725C303030615C303030635C30303074}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Abstract}{iv}{chapter*.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\BKM@entry{id=3,dest={746F632E30},srcline={91}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=4,dest={636861707465722E31},srcline={4}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\BKM@entry{id=5,dest={636861707465722E32},srcline={4}}{5C3337365C3337375C303030535C303030745C303030615C303030745C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030415C303030725C30303074}
\abx@aux@cite{0}{wang2016machine}
\abx@aux@segm{0}{0}{wang2016machine}
\abx@aux@cite{0}{mahesh2020machine}
\abx@aux@segm{0}{0}{mahesh2020machine}
\abx@aux@cite{0}{ayodele2010types}
\abx@aux@segm{0}{0}{ayodele2010types}
\abx@aux@cite{0}{noble2006support}
\abx@aux@segm{0}{0}{noble2006support}
\abx@aux@cite{0}{granmo2018tsetlin}
\abx@aux@segm{0}{0}{granmo2018tsetlin}
\abx@aux@cite{0}{rokach2005decision}
\abx@aux@segm{0}{0}{rokach2005decision}
\BKM@entry{id=6,dest={73656374696F6E2E322E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{bishop1994neural}
\abx@aux@segm{0}{0}{bishop1994neural}
\abx@aux@cite{0}{da2017artificial}
\abx@aux@segm{0}{0}{da2017artificial}
\abx@aux@cite{0}{da2017artificial}
\abx@aux@segm{0}{0}{da2017artificial}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}State of the Art}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:theoretical_background}{{2}{2}{State of the Art}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to Neural Networks}{2}{section.2.1}\protected@file@percent }
\newlabel{sec:neural_networks}{{2.1}{2}{Introduction to Neural Networks}{section.2.1}{}}
\newlabel{eq:perceptron}{{2.1}{2}{Introduction to Neural Networks}{equation.2.1.1}{}}
\abx@aux@cite{0}{da2017artificial}
\abx@aux@segm{0}{0}{da2017artificial}
\abx@aux@cite{0}{medsker2001recurrent}
\abx@aux@segm{0}{0}{medsker2001recurrent}
\abx@aux@cite{0}{li2021survey}
\abx@aux@segm{0}{0}{li2021survey}
\abx@aux@cite{0}{gu2018recent}
\abx@aux@segm{0}{0}{gu2018recent}
\abx@aux@cite{0}{o2015introduction}
\abx@aux@segm{0}{0}{o2015introduction}
\abx@aux@cite{0}{liu2017survey}
\abx@aux@segm{0}{0}{liu2017survey}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Neural network consisting of only one perceptron. The output is computed according to Equation \ref  {eq:perceptron}.\relax }}{3}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:perceptron}{{2.1}{3}{Neural network consisting of only one perceptron. The output is computed according to Equation \ref {eq:perceptron}.\relax }{figure.caption.4}{}}
\newlabel{eq:weights_update}{{2.2}{3}{Introduction to Neural Networks}{equation.2.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Neural network consisting of two hidden layers. The connections between the perceptrons are bidirectional. In the forward phase, the intermediate results are given to the neurons to the right and in the backward phase from right to left. \relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{fig:network}{{2.2}{4}{Neural network consisting of two hidden layers. The connections between the perceptrons are bidirectional. In the forward phase, the intermediate results are given to the neurons to the right and in the backward phase from right to left. \relax }{figure.caption.5}{}}
\BKM@entry{id=7,dest={73656374696F6E2E322E32},srcline={68}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{supervised_learning}
\abx@aux@segm{0}{0}{supervised_learning}
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\abx@aux@cite{0}{bischl2021hyperparameter}
\abx@aux@segm{0}{0}{bischl2021hyperparameter}
\abx@aux@cite{0}{yang2020hyperparameter}
\abx@aux@segm{0}{0}{yang2020hyperparameter}
\BKM@entry{id=8,dest={73756273656374696F6E2E322E322E31},srcline={87}}{5C3337365C3337375C303030475C303030725C303030695C303030645C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Hyperparameter Optimization}{5}{section.2.2}\protected@file@percent }
\newlabel{sec:hyperparameter_optimization}{{2.2}{5}{Hyperparameter Optimization}{section.2.2}{}}
\newlabel{eq:optimization}{{2.3}{5}{Hyperparameter Optimization}{equation.2.2.3}{}}
\BKM@entry{id=9,dest={73756273656374696F6E2E322E322E32},srcline={95}}{5C3337365C3337375C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\abx@aux@cite{0}{random_search}
\abx@aux@segm{0}{0}{random_search}
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Grid Search}{6}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Random Search}{6}{subsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison of grid search (left) and random search (right) in the two dimensional case. For both techniques, 9 different combinations are evaluated. In the left case, only 3 distinct values for each hyperparameter are set whereas there are 9 different values for each parameter in the random search. Taken from \blx@tocontentsinit {0}\cite {feurer2019hyperparameter}. \relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:comparison_searches}{{2.3}{6}{Comparison of grid search (left) and random search (right) in the two dimensional case. For both techniques, 9 different combinations are evaluated. In the left case, only 3 distinct values for each hyperparameter are set whereas there are 9 different values for each parameter in the random search. Taken from \cite {feurer2019hyperparameter}. \relax }{figure.caption.6}{}}
\BKM@entry{id=10,dest={73756273656374696F6E2E322E322E33},srcline={112}}{5C3337365C3337375C303030425C303030615C303030795C303030655C303030735C303030695C303030615C3030306E5C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{snoek2012practical}
\abx@aux@segm{0}{0}{snoek2012practical}
\abx@aux@cite{0}{bischl2021hyperparameter}
\abx@aux@segm{0}{0}{bischl2021hyperparameter}
\abx@aux@cite{0}{andonie2019hyperparameter}
\abx@aux@segm{0}{0}{andonie2019hyperparameter}
\abx@aux@cite{0}{garrido2020dealing}
\abx@aux@segm{0}{0}{garrido2020dealing}
\abx@aux@cite{0}{hutter2011sequential}
\abx@aux@segm{0}{0}{hutter2011sequential}
\abx@aux@cite{0}{wilson2018maximizing}
\abx@aux@segm{0}{0}{wilson2018maximizing}
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Bayesian Optimization}{7}{subsection.2.2.3}\protected@file@percent }
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\BKM@entry{id=11,dest={73756273656374696F6E2E322E322E34},srcline={162}}{5C3337365C3337375C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\abx@aux@cite{0}{feurer2019hyperparameter}
\abx@aux@segm{0}{0}{feurer2019hyperparameter}
\abx@aux@cite{0}{jamieson2016non}
\abx@aux@segm{0}{0}{jamieson2016non}
\abx@aux@cite{0}{8030298}
\abx@aux@segm{0}{0}{8030298}
\abx@aux@cite{0}{smithson2016neural}
\abx@aux@segm{0}{0}{smithson2016neural}
\abx@aux@cite{0}{loshchilov2016cma}
\abx@aux@segm{0}{0}{loshchilov2016cma}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Bayesian Optimization for a black box function f. In each iteration, the surrogate model is fitted on the current archive and an acquisition function is built. The optimum of this acquisition function is evaluated and added to the archive.\relax }}{8}{algorithm.1}\protected@file@percent }
\newlabel{alg:bayesian_opt}{{1}{8}{Bayesian Optimization for a black box function f. In each iteration, the surrogate model is fitted on the current archive and an acquisition function is built. The optimum of this acquisition function is evaluated and added to the archive.\relax }{algorithm.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  Schematic iteration steps of the bayesian optimization. The maximum of the acquisition function determines the next function evaluation (red dot in the middle). The goal is to find the minimum of the dashed line. The blue band is the uncertainty of the function. Taken from \blx@tocontentsinit {0}\cite {feurer2019hyperparameter}. \relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:bayesian_optimization}{{2.4}{9}{Schematic iteration steps of the bayesian optimization. The maximum of the acquisition function determines the next function evaluation (red dot in the middle). The goal is to find the minimum of the dashed line. The blue band is the uncertainty of the function. Taken from \cite {feurer2019hyperparameter}. \relax }{figure.caption.7}{}}
\BKM@entry{id=12,dest={73656374696F6E2E322E33},srcline={166}}{5C3337365C3337375C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\BKM@entry{id=13,dest={73756273656374696F6E2E322E332E31},srcline={170}}{5C3337365C3337375C3030304E5C303030755C3030306D5C303030655C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Other Techniques}{10}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Sparse Grids}{10}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Numerical Approximation of Functions}{10}{subsection.2.3.1}\protected@file@percent }
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the nodal basis. Level of the grid is 3 and hat functions are used. Taken from \blx@tocontentsinit {0}\cite {pfluger2010spatially}. \relax }}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:interpolant}{{2.5}{11}{Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the nodal basis. Level of the grid is 3 and hat functions are used. Taken from \cite {pfluger2010spatially}. \relax }{figure.caption.8}{}}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Hierarchical subspaces up to level 3 on the left. On the right, nodal spaces up to level 3. The combination of $ W_1 $ up to $ W_3 $ is the same space as $ V_3 $. Taken from \blx@tocontentsinit {0}\cite {pfluger2010spatially}. \relax }}{12}{figure.caption.9}\protected@file@percent }
\newlabel{fig:hierarchical_basis}{{2.6}{12}{Hierarchical subspaces up to level 3 on the left. On the right, nodal spaces up to level 3. The combination of $ W_1 $ up to $ W_3 $ is the same space as $ V_3 $. Taken from \cite {pfluger2010spatially}. \relax }{figure.caption.9}{}}
\abx@aux@cite{0}{garcke2013sparse}
\abx@aux@segm{0}{0}{garcke2013sparse}
\abx@aux@cite{0}{garcke2013sparse}
\abx@aux@segm{0}{0}{garcke2013sparse}
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\BKM@entry{id=14,dest={73756273656374696F6E2E322E332E32},srcline={272}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{zenger1991sparse}
\abx@aux@segm{0}{0}{zenger1991sparse}
\abx@aux@cite{0}{bungartz2004sparse}
\abx@aux@segm{0}{0}{bungartz2004sparse}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the hierarchical basis. Level of the grid is 3 and hat functions are used. Taken from \blx@tocontentsinit {0}\cite {pfluger2010spatially}.\relax }}{13}{figure.caption.10}\protected@file@percent }
\newlabel{fig:weighted_sum_hierarchical}{{2.7}{13}{Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the hierarchical basis. Level of the grid is 3 and hat functions are used. Taken from \cite {pfluger2010spatially}.\relax }{figure.caption.10}{}}
\abx@aux@cite{0}{garcke2013sparse}
\abx@aux@segm{0}{0}{garcke2013sparse}
\abx@aux@cite{0}{garcke2013sparse}
\abx@aux@segm{0}{0}{garcke2013sparse}
\abx@aux@cite{0}{obersteiner2022spatially}
\abx@aux@segm{0}{0}{obersteiner2022spatially}
\abx@aux@cite{0}{griebel1990combination}
\abx@aux@segm{0}{0}{griebel1990combination}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces  Example of a basis function in two dimensions. It is constructed with the tensor product of two 1d hat functions. Taken from \blx@tocontentsinit {0}\cite {garcke2013sparse}. \relax }}{14}{figure.caption.11}\protected@file@percent }
\newlabel{fig:2d_basis}{{2.8}{14}{Example of a basis function in two dimensions. It is constructed with the tensor product of two 1d hat functions. Taken from \cite {garcke2013sparse}. \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Adaptive Sparse Grids}{14}{subsection.2.3.2}\protected@file@percent }
\abx@aux@cite{0}{obersteiner2021generalized}
\abx@aux@segm{0}{0}{obersteiner2021generalized}
\abx@aux@cite{0}{hegland2002adaptive}
\abx@aux@segm{0}{0}{hegland2002adaptive}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{obersteiner2021generalized}
\abx@aux@segm{0}{0}{obersteiner2021generalized}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces  Two dimensional example of a sparse grid with $ n = 3 $. Left, the subspaces $ W_{\vec  {l}} $ can be seen and on the right is the resulting sparse grid. Taken from \blx@tocontentsinit {0}\cite {garcke2013sparse}. \relax }}{15}{figure.caption.12}\protected@file@percent }
\newlabel{fig:sparse_grid}{{2.9}{15}{Two dimensional example of a sparse grid with $ n = 3 $. Left, the subspaces $ W_{\vec {l}} $ can be seen and on the right is the resulting sparse grid. Taken from \cite {garcke2013sparse}. \relax }{figure.caption.12}{}}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{obersteiner2021generalized}
\abx@aux@segm{0}{0}{obersteiner2021generalized}
\abx@aux@cite{0}{obersteiner2021generalized}
\abx@aux@segm{0}{0}{obersteiner2021generalized}
\BKM@entry{id=15,dest={73756273656374696F6E2E322E332E33},srcline={340}}{5C3337365C3337375C303030425C303030615C303030735C303030695C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{pfluger2010spatially}
\abx@aux@segm{0}{0}{pfluger2010spatially}
\abx@aux@cite{0}{bungartz1998finite}
\abx@aux@segm{0}{0}{bungartz1998finite}
\abx@aux@cite{0}{bungartz2004sparse}
\abx@aux@segm{0}{0}{bungartz2004sparse}
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces  Example of the 2-dimensional combination technique. Here the blue regular grids are added and the red ones are subtracted. On the left, the normal combination technique can be seen and on the right is an dimension-adaptive version. Taken from \blx@tocontentsinit {0}\cite {pfluger2010spatially}. \relax }}{16}{figure.caption.13}\protected@file@percent }
\newlabel{fig:combi_technique}{{2.10}{16}{Example of the 2-dimensional combination technique. Here the blue regular grids are added and the red ones are subtracted. On the left, the normal combination technique can be seen and on the right is an dimension-adaptive version. Taken from \cite {pfluger2010spatially}. \relax }{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces  Comparison of sparse grids, full grids, and the combination technique in terms of number of grid points and the accuracy.\relax }}{16}{table.caption.14}\protected@file@percent }
\newlabel{tab:comparison_grids}{{2.1}{16}{Comparison of sparse grids, full grids, and the combination technique in terms of number of grid points and the accuracy.\relax }{table.caption.14}{}}
\abx@aux@cite{0}{hollig2013approximation}
\abx@aux@segm{0}{0}{hollig2013approximation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces  Example of the spatially adaptive combination technique in two dimensions. Taken from \blx@tocontentsinit {0}\cite {obersteiner2021generalized}. \relax }}{17}{figure.caption.15}\protected@file@percent }
\newlabel{fig:spatially_adaptive_combi_technique}{{2.11}{17}{Example of the spatially adaptive combination technique in two dimensions. Taken from \cite {obersteiner2021generalized}. \relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Basis Functions for Sparse Grids}{17}{subsection.2.3.3}\protected@file@percent }
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\BKM@entry{id=16,dest={73756273656374696F6E2E322E332E34},srcline={372}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Optimization with Sparse Grids}{18}{subsection.2.3.4}\protected@file@percent }
\newlabel{Optimization_algorithms}{{2.3.4}{18}{Optimization with Sparse Grids}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gradient-Free Methods}{18}{paragraph*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Nelder Mead Method}{18}{subparagraph*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Differential Evolution}{18}{subparagraph*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline CMA-ES}{19}{subparagraph*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gradient-Based Methods}{19}{paragraph*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Gradient Descent}{19}{subparagraph*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline NLCG}{19}{subparagraph*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Newton}{19}{subparagraph*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline BFGS}{19}{subparagraph*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Rprop}{19}{subparagraph*.35}\protected@file@percent }
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\abx@aux@cite{0}{valentin2018gradient}
\abx@aux@segm{0}{0}{valentin2018gradient}
\abx@aux@cite{0}{novak1996global}
\abx@aux@segm{0}{0}{novak1996global}
\abx@aux@cite{0}{duan2016induction}
\abx@aux@segm{0}{0}{duan2016induction}
\abx@aux@cite{0}{duan2016induction}
\abx@aux@segm{0}{0}{duan2016induction}
\abx@aux@cite{0}{duan2016induction}
\abx@aux@segm{0}{0}{duan2016induction}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces  HPC of level $ k = 5 $ (red stars) and full grid (black dots). A full grid would have $ 33^2 = 1089 $ and this grid has $ 147 $ points. Taken from \blx@tocontentsinit {0}\cite {duan2016induction}.\relax }}{20}{figure.caption.36}\protected@file@percent }
\newlabel{fig:hyperbolic}{{2.12}{20}{HPC of level $ k = 5 $ (red stars) and full grid (black dots). A full grid would have $ 33^2 = 1089 $ and this grid has $ 147 $ points. Taken from \cite {duan2016induction}.\relax }{figure.caption.36}{}}
\abx@aux@cite{0}{saska2007path}
\abx@aux@segm{0}{0}{saska2007path}
\abx@aux@cite{0}{duan2016induction}
\abx@aux@segm{0}{0}{duan2016induction}
\abx@aux@cite{0}{novak1996global}
\abx@aux@segm{0}{0}{novak1996global}
\abx@aux@cite{0}{hulsmann2013spagrow}
\abx@aux@segm{0}{0}{hulsmann2013spagrow}
\abx@aux@cite{0}{chen2013derivative}
\abx@aux@segm{0}{0}{chen2013derivative}
\abx@aux@cite{0}{sankaran2009stochastic}
\abx@aux@segm{0}{0}{sankaran2009stochastic}
\BKM@entry{id=17,dest={636861707465722E33},srcline={4}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\BKM@entry{id=18,dest={73656374696F6E2E332E31},srcline={6}}{5C3337365C3337375C3030304D5C303030655C303030745C303030685C3030306F5C303030645C3030306F5C3030306C5C3030306F5C303030675C30303079}
\BKM@entry{id=19,dest={73756273656374696F6E2E332E312E31},srcline={8}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030475C303030725C303030695C303030645C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\BKM@entry{id=20,dest={73756273656374696F6E2E332E312E32},srcline={15}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=21,dest={73656374696F6E2E332E32},srcline={26}}{5C3337365C3337375C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030665C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{valentin2016hierarchical}
\abx@aux@segm{0}{0}{valentin2016hierarchical}
\abx@aux@cite{0}{whitley1996evaluating}
\abx@aux@segm{0}{0}{whitley1996evaluating}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Hyperparameter Optimization with Sparse Grids}{22}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:main_part}{{3}{22}{Hyperparameter Optimization with Sparse Grids}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Methodology}{22}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Adaptive Grid Search with Sparse Grids}{22}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Implementation}{22}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Sparse Grid Optimization of functions}{22}{section.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  Three test functions and their properties.\relax }}{22}{table.caption.37}\protected@file@percent }
\newlabel{tab:test_functions}{{3.1}{22}{Three test functions and their properties.\relax }{table.caption.37}{}}
\abx@aux@cite{0}{yang2010engineering}
\abx@aux@segm{0}{0}{yang2010engineering}
\abx@aux@cite{0}{yang2010engineering}
\abx@aux@segm{0}{0}{yang2010engineering}
\abx@aux@cite{0}{b_splines}
\abx@aux@segm{0}{0}{b_splines}
\newlabel{fig:Eggholder}{{3.1a}{23}{Eggholder.\relax }{figure.caption.38}{}}
\newlabel{sub@fig:Eggholder}{{a}{23}{Eggholder.\relax }{figure.caption.38}{}}
\newlabel{fig:Rosenbrock}{{3.1b}{23}{$ log_{10}(f) $ of Rosenbrock.\relax }{figure.caption.38}{}}
\newlabel{sub@fig:Rosenbrock}{{b}{23}{$ log_{10}(f) $ of Rosenbrock.\relax }{figure.caption.38}{}}
\newlabel{fig:Rastrigin}{{3.1c}{23}{Rastrigin.\relax }{figure.caption.38}{}}
\newlabel{sub@fig:Rastrigin}{{c}{23}{Rastrigin.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Test functions used for evaluating the sparse grid optimization. Each one is plotted with 200 samples in each dimension. Note that the function values of the Rosenbrock function are transformed with $ log_{10}(f(x)) $ for better visualization.\relax }}{23}{figure.caption.38}\protected@file@percent }
\newlabel{fig:test_functions_plot}{{3.1}{23}{Test functions used for evaluating the sparse grid optimization. Each one is plotted with 200 samples in each dimension. Note that the function values of the Rosenbrock function are transformed with $ log_{10}(f(x)) $ for better visualization.\relax }{figure.caption.38}{}}
\BKM@entry{id=22,dest={73756273656374696F6E2E332E322E31},srcline={96}}{5C3337365C3337375C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030645C303030695C303030665C303030665C303030655C303030725C303030655C3030306E5C303030745C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030695C303030745C303030695C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Sparse Grid Generation with different Adaptivities}{24}{subsection.3.2.1}\protected@file@percent }
\newlabel{fig:Egg_gamma_1}{{3.2a}{25}{$ \gamma = 1.0 $\relax }{figure.caption.39}{}}
\newlabel{sub@fig:Egg_gamma_1}{{a}{25}{$ \gamma = 1.0 $\relax }{figure.caption.39}{}}
\newlabel{fig:Egg_gamma_0_5}{{3.2b}{25}{$ \gamma = 0.5 $\relax }{figure.caption.39}{}}
\newlabel{sub@fig:Egg_gamma_0_5}{{b}{25}{$ \gamma = 0.5 $\relax }{figure.caption.39}{}}
\newlabel{fig:Egg_gamma_0}{{3.2c}{25}{$ \gamma = 0.0 $\relax }{figure.caption.39}{}}
\newlabel{sub@fig:Egg_gamma_0}{{c}{25}{$ \gamma = 0.0 $\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Eggholder function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }}{25}{figure.caption.39}\protected@file@percent }
\newlabel{fig:Eggholder_grid}{{3.2}{25}{Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Eggholder function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }{figure.caption.39}{}}
\newlabel{fig:Ros_gamma_1}{{3.3a}{26}{$ \gamma = 1.0 $\relax }{figure.caption.40}{}}
\newlabel{sub@fig:Ros_gamma_1}{{a}{26}{$ \gamma = 1.0 $\relax }{figure.caption.40}{}}
\newlabel{fig:Ros_gamma_0_5}{{3.3b}{26}{$ \gamma = 0.5 $\relax }{figure.caption.40}{}}
\newlabel{sub@fig:Ros_gamma_0_5}{{b}{26}{$ \gamma = 0.5 $\relax }{figure.caption.40}{}}
\newlabel{fig:Ros_gamma_0}{{3.3c}{26}{$ \gamma = 0.0 $\relax }{figure.caption.40}{}}
\newlabel{sub@fig:Ros_gamma_0}{{c}{26}{$ \gamma = 0.0 $\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rosenbrock function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }}{26}{figure.caption.40}\protected@file@percent }
\newlabel{fig:Rosenbrock_grid}{{3.3}{26}{Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rosenbrock function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }{figure.caption.40}{}}
\newlabel{fig:Ras_gamma_1}{{3.4a}{27}{$ \gamma = 1.0 $\relax }{figure.caption.41}{}}
\newlabel{sub@fig:Ras_gamma_1}{{a}{27}{$ \gamma = 1.0 $\relax }{figure.caption.41}{}}
\newlabel{fig:Ras_gamma_0_5}{{3.4b}{27}{$ \gamma = 0.5 $\relax }{figure.caption.41}{}}
\newlabel{sub@fig:Ras_gamma_0_5}{{b}{27}{$ \gamma = 0.5 $\relax }{figure.caption.41}{}}
\newlabel{fig:Ras_gamma_0}{{3.4c}{27}{$ \gamma = 0.0 $\relax }{figure.caption.41}{}}
\newlabel{sub@fig:Ras_gamma_0}{{c}{27}{$ \gamma = 0.0 $\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rastrigin function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }}{27}{figure.caption.41}\protected@file@percent }
\newlabel{fig:Rastrigin_grid}{{3.4}{27}{Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rastrigin function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces  Influence of the adaptivity parameter on the error (difference to the actual optimal value) of the optimum found by the sparse grid. \relax }}{28}{figure.caption.42}\protected@file@percent }
\newlabel{fig:Functions_results}{{3.5}{28}{Influence of the adaptivity parameter on the error (difference to the actual optimal value) of the optimum found by the sparse grid. \relax }{figure.caption.42}{}}
\BKM@entry{id=23,dest={73756273656374696F6E2E332E322E32},srcline={294}}{5C3337365C3337375C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Local and Global Optimization}{29}{subsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces  Optimization error for different algorithms depending on the number of grid points in two dimensions. The plots show the results with degree 2 (top left), degree 3 (top right) and degree 5 (bottom). The Rosenbrock function was used. \relax }}{30}{figure.caption.43}\protected@file@percent }
\newlabel{fig:Optimizer_results}{{3.6}{30}{Optimization error for different algorithms depending on the number of grid points in two dimensions. The plots show the results with degree 2 (top left), degree 3 (top right) and degree 5 (bottom). The Rosenbrock function was used. \relax }{figure.caption.43}{}}
\BKM@entry{id=24,dest={73656374696F6E2E332E33},srcline={416}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\BKM@entry{id=25,dest={73756273656374696F6E2E332E332E31},srcline={420}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030755C3030306D5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{feurer-arxiv19a}
\abx@aux@segm{0}{0}{feurer-arxiv19a}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces  Error of the optimization depending on the number of grid points used for different dimensions of the Rastrigini function. \relax }}{31}{figure.caption.44}\protected@file@percent }
\newlabel{fig:Dimensions_results}{{3.7}{31}{Error of the optimization depending on the number of grid points used for different dimensions of the Rastrigini function. \relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Hyperparameter Optimization with Sparse Grids}{31}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Optimum Approximation with Sparse Grids}{31}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces  2-layer (with 30 neurons each) fully connected network evaluation on the diamonds dataset depending on the number of epochs and learning rate of the Adam optimizer. \relax }}{32}{figure.caption.45}\protected@file@percent }
\newlabel{fig:analysis_model_training}{{3.8}{32}{2-layer (with 30 neurons each) fully connected network evaluation on the diamonds dataset depending on the number of epochs and learning rate of the Adam optimizer. \relax }{figure.caption.45}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  Best hyperparameter configuration found by the sparse grid with different adaptivity parameters and number of grid points. The best result is found with $ \gamma = 0.85 $ and 77 grid points. \relax }}{33}{table.caption.47}\protected@file@percent }
\newlabel{tab:analysis_sparse_grid_with_machine_learning_results}{{3.2}{33}{Best hyperparameter configuration found by the sparse grid with different adaptivity parameters and number of grid points. The best result is found with $ \gamma = 0.85 $ and 77 grid points. \relax }{table.caption.47}{}}
\newlabel{fig:analysis_sparse_grid_with_machine_learning_085}{{3.9a}{34}{Sparse grid generated with adaptivity parameter $ \gamma = 0.85 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper half for higher values of the learning rate. \relax }{figure.caption.46}{}}
\newlabel{sub@fig:analysis_sparse_grid_with_machine_learning_085}{{a}{34}{Sparse grid generated with adaptivity parameter $ \gamma = 0.85 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper half for higher values of the learning rate. \relax }{figure.caption.46}{}}
\newlabel{fig:analysis_sparse_grid_with_machine_learning_05}{{3.9b}{34}{Sparse grid generated with adaptivity parameter $ \gamma = 0.5 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper right half for higher values of the learning rate and epochs between 25 and 30. \relax }{figure.caption.46}{}}
\newlabel{sub@fig:analysis_sparse_grid_with_machine_learning_05}{{b}{34}{Sparse grid generated with adaptivity parameter $ \gamma = 0.5 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper right half for higher values of the learning rate and epochs between 25 and 30. \relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces  Analysis of sparse grid with machine learning evaluation for different adaptivity parameters ($ 0.85 $ \ref  {fig:analysis_sparse_grid_with_machine_learning_085} and $ 0.5 $ \ref  {fig:analysis_sparse_grid_with_machine_learning_05}), each with 29 and 77 grid points.\relax }}{34}{figure.caption.46}\protected@file@percent }
\newlabel{fig:analysis_sparse_grid_with_machine_learning}{{3.9}{34}{Analysis of sparse grid with machine learning evaluation for different adaptivity parameters ($ 0.85 $ \ref {fig:analysis_sparse_grid_with_machine_learning_085} and $ 0.5 $ \ref {fig:analysis_sparse_grid_with_machine_learning_05}), each with 29 and 77 grid points.\relax }{figure.caption.46}{}}
\BKM@entry{id=26,dest={73756273656374696F6E2E332E332E32},srcline={490}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Optimization on Sparse Grids}{35}{subsection.3.3.2}\protected@file@percent }
\BKM@entry{id=27,dest={636861707465722E34},srcline={4}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C3030306C5C3030306F5C3030306F5C3030306B}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusion and Outlook}{36}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion_and_outlook}{{4}{36}{Conclusion and Outlook}{chapter.4}{}}
\BKM@entry{id=28,dest={636861707465722A2E3438},srcline={106}}{5C3337365C3337375C3030304C5C303030695C303030735C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030695C303030675C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\nonumberline List of Figures}{37}{chapter*.48}\protected@file@percent }
\BKM@entry{id=29,dest={636861707465722A2E3439},srcline={107}}{5C3337365C3337375C3030304C5C303030695C303030735C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030545C303030615C303030625C3030306C5C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\nonumberline List of Tables}{40}{chapter*.49}\protected@file@percent }
\BKM@entry{id=30,dest={636861707465722A2E3530},srcline={109}}{5C3337365C3337375C303030425C303030695C303030625C3030306C5C303030695C3030306F5C303030675C303030725C303030615C303030705C303030685C30303079}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Bibliography}{41}{chapter*.50}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\abx@aux@read@bbl@mdfivesum{01731FB4461BF518EB4A94DF82E8AF08}
\abx@aux@defaultrefcontext{0}{wang2016machine}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mahesh2020machine}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ayodele2010types}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{noble2006support}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{granmo2018tsetlin}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rokach2005decision}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop1994neural}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{da2017artificial}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{medsker2001recurrent}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{li2021survey}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{gu2018recent}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{o2015introduction}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{liu2017survey}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{supervised_learning}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{feurer2019hyperparameter}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bischl2021hyperparameter}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2020hyperparameter}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{random_search}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{snoek2012practical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{andonie2019hyperparameter}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{garrido2020dealing}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hutter2011sequential}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{wilson2018maximizing}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{jamieson2016non}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{8030298}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{smithson2016neural}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{loshchilov2016cma}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{pfluger2010spatially}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{garcke2013sparse}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{b_splines}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{zenger1991sparse}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bungartz2004sparse}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{obersteiner2022spatially}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{griebel1990combination}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{obersteiner2021generalized}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hegland2002adaptive}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bungartz1998finite}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hollig2013approximation}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{valentin2018gradient}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{novak1996global}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{duan2016induction}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{saska2007path}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hulsmann2013spagrow}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{chen2013derivative}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{sankaran2009stochastic}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{valentin2016hierarchical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{whitley1996evaluating}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2010engineering}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{feurer-arxiv19a}{none/global//global/global}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{1}}
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{9.85492pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{18.0674pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{26.27988pt}
\global\@namedef{scr@dte@table@lastmaxnumwidth}{18.0674pt}
\global\@namedef{scr@dte@figure@lastmaxnumwidth}{23.54239pt}
\@writefile{toc}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\@writefile{lof}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\@writefile{lot}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\gdef \@abspage@last{52}
