% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Hyperparameter Optimization with Sparse Grids}\label{chapter:main_part}

\section{Methodology}

\subsection{Adaptive Grid Search with Sparse Grids}

In the scope of this thesis, we implemented and analyzed a new approach for the problem of the hyperparameter optimization of machine learning models. The basic idea is to use the sparse grids and adaptively refine the grid points that seem to be promising. Similar to the grid search, first a few sample points are tried out and similar to bayesian search, more promising configurations are exploited. With the adaptivity parameter of the sparse grid, a trade-off between exploration and exploitation is balanced by generating an either homogeneous or adaptive sparse grid. We analyzed the algorithm for evaluating the neural networks on its behavior to find a general configuration performing well for most datasets. \newline 

The reason for this work is that all well-analyzed hyperparameter optimization algorithms have advantages and shortcomings. We saw the promising possibility of using sparse grids for these kinds of optimization problems and compared the performance with grid-, random search, and bayesian optimization. \newline 

For this comparison, we used several kinds of artificial neural networks such as fully connected or convolutional ones. Additionally, multiple datasets of different sizes and with categorical or numerical features are being used. We tried really general tasks like regression or object detection and classification to validate the findings in a general way. In \ref{subsec:Implementation}, a rough description of the implementation of the algorithms as well as the experiments, is presented.

\subsection{Implementation}
\label{subsec:Implementation}

The implementation was done in Python and two main class are introduced. The first is the \textit{Dataset} encapsulating the input and target data of the dataset used for the machine learning model. One can either instantiate an object with given X and Y vector for the data and target or by giving a task id. This identification number is used to instantiate both arrays with the dataset that can be fetched with the functionality given by OpenML \cite{feurer-arxiv19a}. Each task has a unique id, we first concentrate on Regression tasks with small neural networks. \newline

The second class introduced is the Optimization class. The abstract super class has 4 concrete implementation, one for each of grid-, random search, bayesian optimization and the sparse grid search. In all cases, a dataset object has to be given. Additionally, the machine learning model as a function depending on the hyper parameters is required. For the sparse grid search, the functionality of the \textit{SG++} \cite{valentin2016hierarchical} is used. The function has to inherit the class \textit{ScalarFunction} and the concrete model evaluation is done in the member function \textit{eval(x)}. Also, the hyperparameter space that has to be defined. This is a dictionary with information about the names and the types of the parameters. Possible types are list for categorical ones, interval (int) for continuous (or integer) parameters, and log interval for ones that should be sampled logarithmically. Additionally, the lower and upper bound for each of them has to be provided. The next parameters are the budget defining the upper bound for function evaluations and the verbosity for outputs. \newline

For the sparse grid optimization, the hyperparameters have to be scaled to the standard interval $ [0,1] $ and back to the original one for interpretation. Therefore, the functions \textit{to\_standard}, \textit{from\_standard}, \textit{to\_standard\_log} and \textit{from\_standard\_log} are introduced for the linear and logarithmic scaling. 

\begin{lstlisting}[language=Python]
def to_standard(lower, upper, value):
	return (value - lower) / (upper - lower)
	
def from_standard(lower, upper, value):
	return value * (upper - lower) + lower
	
def to_standard_log(lower, upper, value):
	a = math.log10(upper / lower)
	return math.log10(value / lower) / a	
	
def from_standard_log(lower, upper, value):
	a = math.log10(upper / lower)
	return lower * 10 ** (a * value)
\end{lstlisting}

The parameters lower and upper are the bounds of the original hyperparameter interval. The value argument is then scaled to or from the standard domain $ [0,1] $, respectively.


\section{Sparse Grid Optimization of Functions}

Before optimizing the configurations of machine learning models, simple functions are used. This has the advantage that the optimal point is already known in advance and a function call is much faster than evaluating a neural network. Therefore, three different test functions are given with the following properties \cite{valentin2016hierarchical}:

\begin{table}[htbp!]
	\caption{ Three test functions and their properties.}
	\label{tab:test_functions}
	\centering
	\begin{tabular}{|c c c c|} 
		\hline
		Function & Domain & $x_{opt}$ & $ f(x_{opt}) $\\
		\hline
		Eggholder & $[-512, 512]^2 $ & $(512, 404.2319)$ & $ -959.6407 $ \\
		Rosenbrock & $[-5, 10]^2 $ & $(1,1)$ & $ 0 $ \\
		Rastrigin & $[-2, 8]^d $ & $\vec{0}$ & $ 0 $ \\
		\hline
	\end{tabular}
\end{table}

The Eggholder function is defined with \cite{whitley1996evaluating} 
\begin{equation}
	f(x_0, x_1) = -x_0 * \text{sin}(\sqrt{ | x_0 - (x_1 + 47) | }) - (x_1 + 47) \text{sin}(\sqrt{ | x_1 +47 + \frac{x_0}{2} | }).
\end{equation}

The second function (Rosenbrock) \cite{yang2010engineering} is calculated with 
\begin{equation}
	f(x_0, x_1) = (1-x_0)^2 + 100 (x_1 - x_0^2)^2.
\end{equation} 
and the third one (Rastrigin) \cite{yang2010engineering} is defined with
\begin{equation}
	f(\vec{x}) = 10 d + \sum_{ i = 1 }^{d} (x_i^2 - 10 \text{cos}(2 \pi x_i))
\end{equation} 
where $ d $ is the dimensionality of the input vector $ \vec{x} $. Figure \ref{fig:test_functions_plot} shows the functions in two dimensions.


\begin{figure}[htbp!]
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Eggholder_normal}
		\includegraphics[width=\textwidth]{Eggholder_above}
		\caption{Eggholder.}
		\label{fig:Eggholder}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Rosenbrock_normal}
		\includegraphics[width=\textwidth]{Rosenbrock_above}
		\caption{$ log_{10}(f) $ of Rosenbrock.}
		\label{fig:Rosenbrock}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Rastrigin_normal}
		\includegraphics[width=\textwidth]{Rastrigin_above}
		\caption{Rastrigin.}
		\label{fig:Rastrigin}
	\end{subfigure}
	
	\caption{Test functions used for evaluating the sparse grid optimization. Each one is plotted with 200 samples in each dimension. Note that the function values of the Rosenbrock function are transformed with $ log_{10}(f(x)) $ for better visualization.}
	\label{fig:test_functions_plot}
\end{figure}

The Eggholder function (see Figure \ref{fig:Eggholder}) is oscillatory and the optimal point $ x_{opt} $ lays at the border of the domain. Additionally, it is multi modal. The second one (see Figure \ref{fig:Rosenbrock}) is plotted with the function values additionally transformed with $ log_{10}(x) $ for better visualization. The last one is also multi modal (see Figure \ref{fig:Rastrigin}). 

As a first step, the sparse grid generation which is done with the Ritter Novak refinement criterion \cite{b_splines} is evaluated. In each iteration, the grid point $ x_{l,i} $ that minimizes the product 
\begin{equation}
	(r_{l,i} +1)^{1 - \gamma} \cdot (||l||_1 + d_{l,i} + 1)^{\gamma}
\end{equation}
is refined. In this case, $ r_{l,i} = |\{ (l', i') \in K | f(x_{l',i'}) \le f(x_{l,i}) \}| \in \{1, ... , |K|\} $ is the rank of the grid point with $ K $ being the current set of level-index pairs of the grid. This rank denotes the place of the function value in the ascending order of all values of the current grid. On the other hand, the degree of the point $ d_{l,i} \in \mathbb{N}_0 $ is the number of previous refinements at this point. One important choice has to be made for the adaptivity parameter $ \gamma $. This value has to be between 0 and 1 and the smaller this value is, the more adaptive is the sparse grid. With this value, a trade-off between exploration and exploitation can be balanced. The optimal value generally depends on the function that has to be optimized. \newline

\subsection{Sparse Grid Generation with different Adaptivities}

In the following, the behavior of the sparse grid generation is analyzed with the help of the three test functions. In each case, 3 different values for the adaptivity parameter $ \gamma \in \{0.0, 0.5, 1.0\} $ are used and the resulting sparse grid with the triangulated function values is plotted. The first test case is the Eggholder function and the result is depicted in Figure \ref{fig:Eggholder_grid}.

\begin{figure}[htbp!]
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Eggholder/tex_files/Eggholder_1}
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Eggholder/tex_files/Eggholder_above_1}
		\caption{$ \gamma = 1.0 $}
		\label{fig:Egg_gamma_1}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Eggholder/tex_files/Eggholder_0_5}
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Eggholder/tex_files/Eggholder_above_0_5}
		\caption{$ \gamma = 0.5 $}
		\label{fig:Egg_gamma_0_5}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Eggholder/tex_files/Eggholder_0}
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Eggholder/tex_files/Eggholder_above_0}
		\caption{$ \gamma = 0.0 $}
		\label{fig:Egg_gamma_0}
	\end{subfigure}
	
	\caption{Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Eggholder function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.}
	\label{fig:Eggholder_grid}
\end{figure}

In the first case with $ \gamma = 1.0 $, the grid is homogeneous and not adaptive at all. The grid points are distributed over the whole domain and the interpolated function looks very similar to the real plot from Figure \ref{fig:Eggholder}. 

The other extreme case is depicted in Figure \ref{fig:Egg_gamma_0}. There, the grid is maximally adaptive and really concentrated at the top left corner around $ (-260, 280) $. As it is known from Table \ref{tab:test_functions}, this is not where the optimal point is located. This behavior can be explained by the high exploitation throughout the iterations. With such a low adaptivity parameter, the grid points with low function values in the first iterations are mostly refined in the other iteration steps. 

The middle case with $ \gamma = 0.5 $ depicts a case where exploitation and exploration are more balanced. The grid points are more distributed than in the case of $ \gamma = 0.0 $ but there are also some regions where they are more refined, e.g. in the top left corner of the domain.

Note that in all three cases, the exact same number of grid points are evaluated. In this case it is very hard for the sparse grid to find the real optimum because it is located at the border of the domain and it does not use grid points at the border. Also, the oscillatory nature of the function makes it hard to not concentrate on a local optimum which can happen in case of too high adaptivity. \newline

The next function used is the Rosenbrock function (see Figure \ref{fig:Rosenbrock}). Again, three different values for the adaptivity parameter are used. The results are depicted in Figure \ref{fig:Rosenbrock_grid}.

\begin{figure}[htbp!]
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rosenbrock/tex_files/Rosenbrock_1}
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rosenbrock/tex_files/Rosenbrock_above_1}
		\caption{$ \gamma = 1.0 $}
		\label{fig:Ros_gamma_1}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rosenbrock/tex_files/Rosenbrock_0_5}
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rosenbrock/tex_files/Rosenbrock_above_0_5}
		\caption{$ \gamma = 0.5 $}
		\label{fig:Ros_gamma_0_5}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rosenbrock/tex_files/Rosenbrock_0}
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rosenbrock/tex_files/Rosenbrock_above_0}
		\caption{$ \gamma = 0.0 $}
		\label{fig:Ros_gamma_0}
	\end{subfigure}
	
	\caption{Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rosenbrock function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.}
	\label{fig:Rosenbrock_grid}
\end{figure}


The first fact that can be observed is that the sparse grid in the first case (Figure \ref{fig:Ros_gamma_1}) is exactly the same as the one for the Eggholder function (Figure \ref{fig:Egg_gamma_1}). This is due to the fact that this value of $ \gamma $ leads to a homogeneous sparse grid which is not dependent on the function used but rather the number of iterations for the grid generation. In this case, the interpolated function looks similar to the real function (Figure \ref{fig:Rosenbrock}). 

Now with decreasing value of $ \gamma $, the grid gets more and more inhomogeneous, while concentrating to refine smaller function values. The extreme case can be seen in Figure \ref{fig:Ros_gamma_0}, where the most grid points are next to the point $ (2.5, 6.25) $. After refining the first point in the middle, the one on top of it is getting refined all the time for the case $ \gamma = 0.0 $. Again, the sparse grid with $ \gamma = 0.5 $ is more balanced with more grid points on the left where the real optimum is located. \newline 


The last function used is the Rastrigin function (see Figure \ref{fig:Rastrigin} for the plot of the function and \ref{fig:Rastrigin_grid} for the resulting sparse grids). 

\begin{figure}[htbp!]
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rastrigin/tex_files/Rastrigin_1}
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rastrigin/tex_files/Rastrigin_above_1}
		\caption{$ \gamma = 1.0 $}
		\label{fig:Ras_gamma_1}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rastrigin/tex_files/Rastrigin_0_5}
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rastrigin/tex_files/Rastrigin_above_0_5}
		\caption{$ \gamma = 0.5 $}
		\label{fig:Ras_gamma_0_5}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rastrigin/tex_files/Rastrigin_0}
		\includegraphics[width=\textwidth]{figures/Results/Sparse_grid_generation_results/Rastrigin/tex_files/Rastrigin_above_0}
		\caption{$ \gamma = 0.0 $}
		\label{fig:Ras_gamma_0}
	\end{subfigure}
	
	\caption{Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rastrigin function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.}
	\label{fig:Rastrigin_grid}
\end{figure}

As in the previous two cases, the sparse grid is exactly the same for $ \gamma = 1.0 $. We can also see the same behavior for a decreasing adaptivity parameter. For $ \gamma = 0.0 $, only the grid point in the center is refined in all steps. In the middle case, the grid looks more balanced with tendency to the bottom left corner.  \newline 


In conclusion, these experiments show that the value for the adaptivity parameter strongly influences the grid generation and the resulting optimal value found by the algorithm. In general, this trade-off between exploitation with trying to find a solution fast and exploration with reconstructing the function in the whole domain can be balanced with this adaptivity parameter. \newline

In the following, the goal is to further analyze this adaptivity parameter. For each of the three test functions, experiments with $ \gamma \in \{0.0, 0.25, 0.5, 0.75, 1.0\} $ are made. The error of the optimum found by the grid is calculated depending on the number of grid points used. It is evaluated with 
\begin{equation}
 Error = f(x_{opt}^*) - f(x_{opt}) 
\end{equation}
where $ x_{opt}^* $ is the optimal point found by the sparse grid and $ x_{opt} $ is the real optimum. Figure \ref{fig:test_functions_plot} shows the resulting Errors depending on the number of grid points used. The top left plot is for the Eggholder function, the one on the top right belongs to the Rosenbrock function and the bottom one corresponds to the Rastrigin function. \newline

\begin{figure}[htbp!]
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Number of grid points,
			ylabel = Error (Eggholder),
			scale=0.7,
			cycle list name=exotic,
			]
			
			\addplot+[mark=*, style=thick] coordinates {(1,934.1803628147137)(53,416.6753127436343)(105,416.67517650515606)(157,416.67517650515606)(209,416.6751765051557)(261,416.6751765051555)(313,416.67517650515515)(365,416.67517650515583)(417,416.67517650515606)(469,416.67517650515515)(521,416.67517650515595)(573,416.6751765051555)(625,416.6751765051557)(677,416.6751765051554)(729,416.6751765051555)(781,416.67517650515595)(833,416.6751765051555)(885,416.6751765051557)(937,416.6751765051557)};
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(1,934.1803628147137)(53,416.6753127436343)(105,416.67517650515606)(157,402.86392358909313)(209,401.165066632121)(261,401.16506654901593)(313,401.16506654901764)(365,401.1650665490138)(417,401.1650665490125)(469,401.1650665490105)(521,401.16506654901195)(573,401.1650665490175)(625,401.16506654902184)(677,401.16506654901843)(729,401.16506654901593)(781,401.1650665490224)(833,401.16506654902105)(885,401.1650665490139)(937,401.1650665490138)};
			
			\addplot+[mark=*, style=thick]   coordinates 
			{(1,934.1803628147137)(53,416.6753127436343)(105,416.67517650515595)(157,416.67517650515595)(209,416.6751765051556)(261,401.16506923718293)(313,401.1650665490133)(365,401.1650665490122)(417,401.16506654901445)(469,401.1650665490125)(521,401.16506654901264)(573,401.16506654901264)(625,401.1650665490216)(677,394.1301115312152)(729,393.6496427441624)(781,393.6444455890784)(833,393.6437130885647)(885,393.6437130885688)(937,393.6437130885528)};
			
			\addplot+[mark=*, style=thick]  coordinates
				{(1,934.1803628147137)(53,416.680649645461)(105,416.67519174547897)(157,416.6751765051557)(209,416.6751765051554)(261,385.37674780492864)(313,385.35652906792404)(365,385.3565290679238)(417,243.68003942485666)(469,243.67992321536838)(521,243.67992321536633)(573,243.01108728703605)(625,242.99463717547087)(677,242.99463717547076)(729,242.99463717547087)(781,242.99463717546962)(833,242.99463717546973)(885,213.83490792055488)(937,213.834907133721)};
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(1,934.1803628147137)(53,518.1399491004825)(105,518.1399491004823)(157,487.54622876272015)(209,487.54622876272003)(261,487.54622876272)(313,410.9897867087419)(365,410.9897867087424)(417,410.9897867087427)(469,410.9897867087419)(521,410.9897867087424)(573,410.98978670874214)(625,410.9897867087425)(677,410.9897867087424)(729,410.9897867087428)(781,102.75674102278083)(833,102.7567410227864)(885,102.75674102278651)(937,102.75674102279345)};
			
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Number of grid points,
			ylabel = Error (Rosenbrock),
			scale = 0.7,
			ymode=log,
			cycle list name=exotic,
			]
			
			\addplot+[mark=*, style=thick] coordinates {(1,1408.5)(53,2.25)(105,2.2491001457701714)(157,2.2491001457701714)(209,2.2491001457722626)(261,2.2491001457672675)(313,2.249100145771435)(365,2.249100145769731)(417,2.2491001457715205)(469,2.249100145772637)(521,2.2491001457718287)(573,2.249100145770883)(625,2.2491001457697997)(677,2.24910014577051)(729,2.2491001457706186)(781,2.249100145769841)(833,2.2491001457692894)(885,2.249100145769003)(937,2.2491001457706337)};
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(1,1408.5)(53,2.25)(105,2.2491001457701714)(157,2.2491001457701714)(209,2.2491001457721036)(261,2.2491001457695767)(313,2.249100145770952)(365,2.2491001457690514)(417,2.2491001457697983)(469,2.2491001457687094)(521,2.2491001457707274)(573,2.249100145770689)(625,2.2491001457705138)(677,2.249100145770712)(729,0.13999960097248135)(781,0.13973409018662308)(833,0.1397340901880652)(885,0.1397340901910032)(937,0.13973409019308297)};
			
			\addplot+[mark=*, style=thick]   coordinates 
			{(1,1408.5)(53,2.25)(105,2.2491001457701714)(157,0.65972900390625)(209,0.33738539328720274)(261,0.337384855010467)(313,0.3373848550084554)(365,0.3164062500005624)(417,0.3160824744022442)(469,0.316082474402264)(521,0.3143689789723247)(573,0.31436571579881106)(625,0.3143657157996295)(677,0.3143657157842261)(729,0.3143657158217291)(781,0.31436571578093114)(833,0.3143657158159762)(885,0.31436571582376066)(937,0.04369918730841191)};
			
			\addplot+[mark=*, style=thick]  coordinates
			{(1,1408.5)(53,0.65972900390625)(105,0.33738539328578554)(157,0.3373848550099865)(209,0.33738485500900234)(261,0.06609634402749087)(313,0.04368661484965771)(365,0.043686614913836186)(417,0.043686614861208506)(469,0.04368661485251346)(521,0.04368661485326686)(573,0.04368661485538938)(625,0.04368661485308678)(677,0.043686614838481574)(729,0.04368661486120118)(781,0.04368661488831904)(833,0.043686614839174354)(885,0.04368661486845582)(937,0.04368661486256009)};
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(1,1408.5)(53,0.65972900390625)(105,0.65972900390625)(157,0.65972900390625)(209,0.6597290039064774)(261,0.6597290039068184)(313,0.65972900390625)(365,0.65972900390625)(417,0.65972900390625)(469,0.6597290039065911)(521,0.6597290039067047)(573,0.6597290039064774)(625,0.6597290039064774)(677,0.6597290039067047)(729,0.6597290039067047)(781,0.6597290039064774)(833,0.65972900390625)(885,0.6597290039065911)(937,0.6597290039065911)};
			
		\end{axis}
	\end{tikzpicture}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Number of grid points,
			ylabel = Error (Rastrigin),
			scale = 0.7,
			cycle list name=exotic,
			legend pos=outer north east,
			]
			
			\addplot+[mark=*, style=thick] coordinates{(1,18.0)(53,17.954603830243304)(105,17.954601241487484)(157,17.954601241487484)(209,17.95460124148829)(261,17.954601241487573)(313,17.95460124148752)(365,17.954601241487435)(417,17.95460124148751)(469,17.954601241487524)(521,17.954601241487467)(573,17.954601241487463)(625,17.954601241487506)(677,17.954601241487463)(729,17.95460124148793)(781,17.954601241488138)(833,17.954601241487886)(885,17.954601241487513)(937,17.954601241488046)};
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(1,18.0)(53,9.995024358227056)(105,9.994959057644616)(157,9.994959057644616)(209,9.994959057646103)(261,9.99495905764463)(313,9.994959057644524)(365,9.99495905764453)(417,9.99495905764462)(469,9.994959057644579)(521,9.994959057644468)(573,9.994959057644502)(625,9.994959057644628)(677,9.994959057644607)(729,9.99495905764461)(781,9.994959057644595)(833,9.99495905764462)(885,9.994959057644616)(937,9.994959057644667)};
			
			\addplot+[mark=*, style=thick]   coordinates 
			{(1,18.0)(53,9.995024358227056)(105,9.994959057644618)(157,9.000000184767032)(209,9.000000000721773)(261,9.0000000007217)(313,9.000000000721338)(365,9.000000000721721)(417,9.00000000072155)(469,9.00000000072114)(521,9.000000000721972)(573,9.000000000721784)(625,9.00000000072177)(677,9.000000000721762)(729,9.00000000072182)(781,9.000000000721915)(833,9.000000000721828)(885,9.000000000721684)(937,9.000000000721721)};
			
			\addplot+[mark=*, style=thick]  coordinates
			{(1,18.0)(53,9.995597577515051)(105,9.994959320973711)(157,9.193123758467696)(209,9.000000000721291)(261,9.000000000720725)(313,9.00000000073414)(365,9.000000000715445)(417,9.000000000715788)(469,9.00000000072136)(521,5.889114376269022)(573,0.001513592628570368)(625,1.9994962252604298e-07)(677,1.2455895805700494e-08)(729,1.2778394708011167e-08)(781,1.2291804361994243e-08)(833,1.179890717001382e-08)(885,1.251678228282502e-08)(937,1.231146666513552e-08)};
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(1,18.0)(53,11.944557188134524)(105,11.944557188134521)(157,10.130623758467696)(209,10.130623758467692)(261,9.193123758467797)(313,9.193123758467703)(365,9.193123758467696)(417,9.1931237584677)(469,9.1931237584677)(521,9.193123758467692)(573,5.8891143762690925)(625,5.889114376269034)(677,5.88911437626902)(729,5.889114376269047)(781,5.889114376269048)(833,5.889114376269062)(885,5.889114376269007)(937,5.889114376269115)};
			
			\legend{$ \gamma = 0.0 $, $ \gamma = 0.25 $, $ \gamma = 0.5 $, $ \gamma = 0.75 $, $ \gamma = 1.0 $,}
			
		\end{axis}
	\end{tikzpicture}
	\caption{ Influence of the adaptivity parameter on the error (difference to the actual optimal value) of the optimum found by the sparse grid. }	
	\label{fig:Functions_results}
\end{figure}

For all three test functions, a value $ 0.75 \le \gamma \le 1.0 $ leads to the smallest error with higher number of grid points. This means that a fast exploitation is not good for finding the global optimum. For the Rastrigin function, the most adaptive sparse grid does not even lead to a smaller error with up to 1000 grid points. This is the same example as in Figure \ref{fig:Ras_gamma_0}, where only the center point is refined. Note that by now, no optimization algorithm is applied on top of the sparse grid. For further experiments, the adaptivity parameter will be set to $ \gamma = 0.85 $. \newline 

\subsection{Local and Global Optimization}

The next improvement is to add optimizers after the grid generation phase. There are two different types of algorithms. The first one is local optimization, for example based on the gradient like gradient descent. The second one is global optimization. An example therefore is to use a multi start approach by trying out multiple different starting points for the algorithm. These various initial values can be uniformly distributed in the domain. The following experiments show how the error (difference between optimum found by algorithm and actual optimal function value) behaves with higher number of grid points of the underlying sparse grid. As a local optimization algorithm, the gradient descent is used. The starting point for this algorithm is set to the point of the sparse grid where the smallest function value was found. For the global optimization, a multi start approach with 20 equally distributed starting points is used. The concrete algorithm is the nelder mead optimization as described in \ref{Optimization_algorithms}. The number of function evaluations used in this method, as well as the number of steps for gradient descent is set to 1000. One important parameter for the optimization is the degree of the B-splines used on the sparse grid. The resulting errors depending on the number of grid points for the degrees 2, 3, and 5 can be seen in Figure \ref{fig:Optimizer_results}. The Rosenbrock function is used in all cases.

\begin{figure}[htbp!]
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Number of grid points,
			ylabel = Error (degree 2),
			scale=0.85,
			cycle list name=exotic,
			legend pos=north east,
			]
			
			\addplot+[mark=*, style=thick] coordinates
			{(1,52.5) (9,23.124999999999993) (29,20.221948342338713) (49,12.642146873903034) (97,12.037968800252116) (197,5.324940782205941) (297,5.070573664357864) (397,5.059483529507994) (497,5.059483529509992) (749,4.981978981445401) (997,4.975134049661977) };
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(1,52.5) (9,36.56247783900481) (29,20.221948342338713) (49,12.642146873903027) (97,12.037968800252123) (197,5.324940782205946) (297,5.070573664357985) (397,5.059483529510453) (497,5.059483529510453) (749,4.981978981455729) (997,4.975134049663653) };
			
			\addplot+[mark=*, style=thick]   coordinates 
			{(1,84.58773867867924) (9,37.34881542254573) (29,35.353118558881526) (49,26.048828056455157) (97,5.180279637187814) (197,1.4141172901666579) (297,1.3031251937216908) (397,1.0903873189756652) (497,0.08680369103412033) (749,0.10422400147368194) (997,0.020853342869873615) };
			
			\legend{Before Optimization, Local Optimization, Global Optimization}
			
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Number of grid points,
			ylabel = Error (degree 3),
			scale=0.85,
			cycle list name=exotic,
			legend pos=north east,
			]
			
			\addplot+[mark=*, style=thick] coordinates
			{(1,52.5) (9,23.124999999999986) (29,20.221948342338727) (49,12.642146873903025) (97,12.037968800252077) (197,5.324940782206068) (297,5.070573664356438) (397,5.059483529515382) (497,5.059483529505752) (749,4.9819789814573205) (997,4.975134049660271) };
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(1,52.5) (9,39.26658373479667) (29,20.847171449176678) (49,49.99973745586844) (97,6.259687480962173) (197,5.002154846904871) (297,4.975751718068931) (397,4.975746034104377) (497,4.975697508130967) (749,3.980892241733912) (997,3.979835850885795) };
			
			\addplot+[mark=*, style=thick]   coordinates 
			{(1,48.864612088186476) (9,31.16918203994022) (29,20.48853217431511) (49,50.53387608332728) (97,3.0805866327728566) (197,0.14445847530268452) (297,0.06980177032781754) (397,0.06988725538127483) (497,0.02077395004058502) (749,0.003850124352695161) (997,0.4352002384999345) };
			
			\legend{Before Optimization, Local Optimization, Global Optimization}
			
		\end{axis}
	\end{tikzpicture}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Number of grid points,
			ylabel = Error (degree 5),
			scale=0.85,
			cycle list name=exotic,
			legend pos=north east,
			]
			
			\addplot+[mark=*, style=thick] coordinates
			{(1,52.5) (9,23.124999999999996) (29,20.221948342338706) (49,12.642146873903044) (97,12.037968800252427) (197,5.324940782206359) (297,5.070573664363549) (397,5.059483529512068) (497,5.059483529510554) (749,4.981978981456664) (997,4.975134049668072) };
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(1,52.5) (9,34.964270829168846) (29,22.048613242940117) (49,49.999815285630675) (97,8.771776029715767) (197,4.977579661397227) (297,4.9749420077761) (397,4.974930572100956) (497,4.974940210636003) (749,3.9798360656238323) (997,4.974790247773939) };
			
			\addplot+[mark=*, style=thick]   coordinates 
			{(1,73.32956404214876) (9,37.355007206361236) (29,22.57998988395788) (49,50.570154840256905) (97,1.7957114981045024) (197,0.059327067234512754) (297,0.0044097596433303465) (397,0.11308692978262513) (497,0.008066444905090009) (749,0.02197720207793452) (997,0.002142203898742423) };
			
			\legend{Before Optimization, Local Optimization, Global Optimization}
			
		\end{axis}
	\end{tikzpicture}
	\caption{ Optimization error for different algorithms depending on the number of grid points in two dimensions. The plots show the results with degree 2 (top left), degree 3 (top right) and degree 5 (bottom). The Rastrigin function was used. }	
	\label{fig:Optimizer_results}
\end{figure}

In all three cases, the errors of the local and global optimizers first increase with increasing number of grid points until about 100 to 200 function evaluations are done. After that, both local and global optimization algorithms are at least as good as the result found by the sparse grid. With increasing number of grid points, the global optimization finds the best solution in all cases with degrees 2, 3, and 5. \newline 

The visualization in Figure \ref{fig:optimizers_visualized} indicates why the global optimizer achieves the best results for a high number of sparse grid points. 
\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.31\textwidth]{figures/Results/Testfunctions/Optimizer_visualization/Optimizers_visualization5}
	\includegraphics[width=0.31\textwidth]{figures/Results/Testfunctions/Optimizer_visualization/Optimizers_visualization80}
	\includegraphics[width=0.31\textwidth]{figures/Results/Testfunctions/Optimizer_visualization/Optimizers_visualization1000}
	
	\caption{Resulting optimal points from local (red points) and global (black points) optimization algorithm. In the background, the contour of the interpolated function is depicted with the corresponding sparse grid points in white. The left one has 5 grid points, the one in the center 77, and the right one 997. The degree of the B-splines on the sparse grid is 2. }
	\label{fig:optimizers_visualized}
\end{figure}

In the left example of Figure \ref{fig:optimizers_visualized} with 5 sparse grid points, the result of the local optimizer is at the left border of the domain as indicated by the red line which connects the different steps of the algorithm. All resulting optimal points found by the multi start approach are located in the lower left corner. Note that the interpolated function is not very similar to the original one (see Figure \ref{fig:Rastrigin}) because of the low number of grid points. 

With increasing number of these basis points, both algorithms find a better solution to the problem. In the center plot of Figure \ref{fig:optimizers_visualized}, 77 grid points were used and the interpolated contour looks more oscillatory and more similar to the original function. The local and global algorithms find different optimal points in this case. 

In the right example of Figure \ref{fig:optimizers_visualized}, 997 grid points were used and the contour already looks very similar to the function plot. There are many candidates of the global optimizer and so	me of them are really near to the actual optimal point which is located at $ (0, 0) $. 

The exact solutions of each algorithm can be seen in Table \ref{tab:optimizer_vis_function}.
\begin{table}[h!]
	\caption{ Overview over the exact solutions found by the local and global optimizer for the same problem as in Figure \ref{fig:optimizers_visualized}. The actual optimum is at (0, 0), with function value 0 (see Table \ref{tab:test_functions}).}
	\label{tab:optimizer_vis_function}
	\centering
	\begin{tabular}{|c c c c c|} 
		\hline
		Number grid points & Optimizer & Coordinates & Interpolated value & Actual value \\
		\hline
		5 & Local &  (-5,2.5)  &  23.125  &  51.25  \\
		5 & Global &  (-4.998,-4.971)  &  -6.191  &  49.862  \\
		77 & Local &  (-1.25,-1.016)  &  12.642  &  12.642  \\
		77 & Global &  (0.157,-1.006)  &  5.756  &  5.514  \\
		997 & Local &  (-1.990,-0.994)  &  4.975  &  4.975  \\
		997 & Global & \textbf{(-0.014,-0.002)} & 0.151 & \textbf{0.042} \\
		\hline
	\end{tabular}
\end{table}

The big advantage of the global optimizer is that multiple starting points are used so that the chance of reaching the global minimum is very high. Especially for a high number of grid points used, this is the case. While the local optimizer only finds a local minimum (see Figure \ref{fig:optimizers_visualized}, red dot at (-1.99, -0.994)), the global optimizer finds multiple candidates at more than one local minimum. One of them is very near to the global optimal point in this case (see Figure \ref{fig:optimizers_visualized}, black dot at (0.002, 0.013)). \newline 


One approach is to combine all three resulting points with their corresponding function value. The overall optimal point is then just set to the one with the smallest function value. \newline 

The input of the previous experiments were all just in two dimensions. The following ones analyze the impact of the dimension on the performance of the algorithm. Therefore, the Rastrigin function is optimized, again with increasing number of grid points. The degree of the B-splines used on the sparse grid is set to 5 and for the adaptivity parameter, we set $ \gamma = 0.85 $. The results are depicted in Figure \ref{fig:Dimensions_results}.


\begin{figure}[htbp!]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Number of grid points,
			ylabel = Error,
			scale=1,
			cycle list name=exotic,
			legend pos=outer north east,
			]
			
			\addplot+[mark=*, style=thick] coordinates
			{(49,9.101448500006125) (97,0.6579925771421742) (197,0.24877350577453328) (297,0.004037953576528253) (397,0.00025859422627760864) (497,1.7408297026122455e-13) (749,7.460698725481052e-14) (997,3.481659405224491e-13) (1497,5.044853423896711e-13) (1997,2.1316282072803006e-14) (2997,3.652389892749852e-06) (3997,2.8704901623655132e-05) (4997,5.787840197764794e-07) };
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(49,28.11190602660223) (97,28.027731473104676) (193,27.012106473104676) (297,15.113252690760966) (393,9.600766347261924e-06) (497,1.6991705820146308e-09) (745,2.1316282072803006e-14) (993,8.952838470577262e-13) (1497,5.101696842757519e-12) (1993,7.959662418317201) (2993,7.959662532446124) (3993,7.959662444938445) (4993,1.989918226969806) };
			
			\addplot+[mark=*, style=thick]   coordinates 
			{(49,48.423119589928916) (97,46.02773147310465) (193,45.19017740517299) (289,45.000002956271466) (397,45.00000295627026) (493,44.97832419159962) (745,44.83586659446601) (997,2.9518635934690423) (1489,1.2755663192365319e-10) (1993,3.339550858072471e-13) (2989,3.609557097661309e-12) (3997,4.974795290848007) (4993,2.3494095557907713e-10) };
			
			\addplot+[mark=*, style=thick]  coordinates 
			{(49,72.00000000000047) (97,64.13062375846819) (193,63.995042024453646) (289,63.68474722559662) (385,62.68963603135439) (497,62.68978857181299) (737,62.688613023877224) (993,63.000000000740776) (1489,57.870967998853985) (1985,17.661726128773374) (2993,6.96530803395143) (3985,1.0274447959091049e-11) (4993,9.632833695150111e-07) };
			
			\addplot+[mark=*, style=thick]   coordinates 
			{(41,90.00000000000006) (81,85.19455718813529) (181,82.0037952623156) (281,81.99495932097409) (381,81.99495905766113) (481,81.59599881413277) (741,81.61443670251717) (981,81.61429574478895) (1481,81.99495913158995) (1981,81.99495935231735) (2981,81.99495919483614) (3981,66.96083466127668) (4981,81.74484728227361) };
			
			\legend{Dimension 2, Dimension 4, Dimension 6, Dimension 8, Dimension 10}
			
		\end{axis}
	\end{tikzpicture}
	\caption{ Error of the optimization depending on the number of grid points used for different dimensions of the Rastrigini function. }	
	\label{fig:Dimensions_results}
\end{figure}

The results show that with increasing dimensionality of the problem, more grid points are needed in order to reduce the error. The metric was again defined as the optimum found by the algorithm subtracted by the actual optimal value. \newline 

In conclusion, the adaptivity parameter, the degree of the B-splines on the sparse grid, and the dimensionality of the problem that is optimized, strongly influence the behavior of the sparse grid optimization. For the adaptivity parameter, a value with about $ \gamma = 0.85 $ is in general a good choice for a function that is not known in advance. With a higher degree, we can find the optimum a bit faster and for higher dimensionality, more refinement iterations of the sparse grid are needed. 


\section{Hyperparameter Optimization with Sparse Grids}

Now we replace the function, where we know the analytical optimum with the evaluation of a machine learning model. The biggest change is the much higher duration of one function call and the complexity of finding the analytical solution which is impossible due to the high number of network weights in neural networks. 

\subsection{Optimum Approximation with Sparse Grids}

The following plots show an example of a small neural network being evaluated on the diamonds dataset which is available on OpenML \cite{feurer-arxiv19a}. We used a model consisting of two fully connected layers, each made up of 30 neurons. In one batch, 100 data samples are processed. The two dimensional plot (see Figure \ref{fig:analysis_model_training}) depicts the network evaluation depending on the number of epochs used and the value for the learning rate of the Adam optimizer of the model. The loss of the network is calculated with the mean squared error. As pre-processing of the data, the numeric features of the input data is scaled with a standard scaler and the categorical features are one hot encoded. The target values are also scaled separately. For the evaluation metric, we chose the average result of 2-fold-cross validation with the mean absolute percentage error (MAPE). This metric is defined as 
\begin{equation}
 	\text{MAPE} = \frac{1}{n} \sum_{i=1}^{n}\left|\frac{y^i - y^i_{pred}}{y^i}\right|
\end{equation}
where $ y^i $ is the target value and $ y^i_{pred} $ is the predicted value of data point $ x^i $.

The interval of the epochs is $ [1, 40] $ and the learning rate is sampled equidistant and logarithmic between $ [10^{-10}, 10^{-1}] $. The resulting plot with the result depending on the epochs and the $ log_{10}(\text{learning rate}) $ can be seen in Figure \ref{fig:analysis_model_training}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/1000_evaluations/Network_above}
	\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/1000_evaluations/Network_normal}
	
	\caption{ 2-layer (with 30 neurons each) fully connected network evaluation on the diamonds dataset depending on the number of epochs and learning rate of the Adam optimizer. The result is plotted with $ log_{10}(\text{Result}) $ for better visualization. }
	\label{fig:analysis_model_training}
\end{figure}

In the plot in Figure \ref{fig:analysis_model_training}, for each of epochs and learning rate, 100 different, equidistant values are sampled from the corresponding interval (linear sampling for epochs and log sampling for learning rate). All different combinations are evaluated leading to $ 100^2 = 10000 $ function evaluations. This took about 45 hours on a normal machine. Regarding the goal of hyperparameter optimization, this technique of trying all different combinations of distinct values is comparable to the grid search approach which is not very efficient. Although, the rough behavior of the machine learning model can be analyzed. The function is nearly constant for the learning rate between $ 10^{-6} $ and $ 10^{-10} $. This is caused by the Adam optimizer adjusting the weights of the network too slow in order to minimize the error. In the other half of the domain (learning rate between $ 10^{-1} $ and $ 10^{-5} $), a blue region where the smallest errors are achieved can be observed. This is where the combination between epochs and learning rate is very good for achieving good results. With this plot, a general observation can be made. However, it takes far too much time to analyze each problem with a plot like this. Additionally to the high effort, only the region of the minimum can be determined, not the optimal point itself. \newline

With this knowledge about the underlying function that has to be minimized, the behavior of the sparse grid can be analyzed. Figure \ref{fig:analysis_sparse_grid_with_machine_learning} depicts the generated grid colored corresponding to the function value for different adaptivity values and number of grid points. Note that the scale of the learning rate is the one interpreted by the sparse grid. The actual scale is logarithmic between $ 10^{-10} $ for $ 0.0 $ and $ 10 ^{-1} $ for $ 1.0 $. 

\begin{figure}[htbp!]
	\begin{subfigure}{\textwidth}
		\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/first_analysis/above_30_85}
		\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/first_analysis/above_80_85}
		\caption{ Sparse grid generated with adaptivity parameter $ \gamma = 0.85 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper half for higher values of the learning rate. }
		\label{fig:analysis_sparse_grid_with_machine_learning_085}
	\end{subfigure}
	
	\begin{subfigure}{\textwidth}
		\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/first_analysis/Above_30_5}
		\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/first_analysis/Above_80_5}
		\caption{Sparse grid generated with adaptivity parameter $ \gamma = 0.5 $ and number of grid points of 29 (left) and 77 (right). Most points are in the upper right half for higher values of the learning rate and epochs between 25 and 30. }
		\label{fig:analysis_sparse_grid_with_machine_learning_05}
	\end{subfigure}

	\caption{ Analysis of sparse grid with machine learning evaluation for different adaptivity parameters ($ 0.85 $ \ref{fig:analysis_sparse_grid_with_machine_learning_085} and $ 0.5 $ \ref{fig:analysis_sparse_grid_with_machine_learning_05}), each with 29 and 77 grid points.}
	\label{fig:analysis_sparse_grid_with_machine_learning}
\end{figure}

In the first case with $ \gamma = 0.85 $, more grid points are generated in the upper half of the domain. A similar behavior can be observed for a higher number of grid points (\ref{fig:analysis_sparse_grid_with_machine_learning_085}, right). Especially in regions where the learning rate is about $ 0.0002 $ to $ 0.0003 $ (scale marked at $ 0.7 $), many grid points are refined. From the analysis in Figure \ref{fig:analysis_model_training}, there might be the optimal value. 

For a smaller adaptivity value of $ \gamma = 0.5 $, the sparse grid is much more adaptive as depicted in Figure \ref{fig:analysis_sparse_grid_with_machine_learning_05}. For both number of grid points $ 50 $ and $ 100 $, the most refined part of the sparse grid is at the same region. One important thing to observe here is that with higher adaptivity, a lower number of grid points is necessary to find one candidate of the optimum value. On the other hand, with lower adaptivity, more grid points are needed to find a good minimal value. 

However, as shown in Table \ref{tab:analysis_sparse_grid_with_machine_learning_results}, the best value is found with the configuration $ \gamma = 0.85 $ and number of grid points of 100. This means that lower adaptivity leads to better results.

\begin{table}[htbp!]
	\centering
	\caption{ Best hyperparameter configuration found by the sparse grid with different adaptivity parameters and number of grid points. The best result is found with $ \gamma = 0.85 $ and 77 grid points. }
	\label{tab:analysis_sparse_grid_with_machine_learning_results}
	\begin{tabular}{| c c | c c | c |} 
		\hline
		$ \gamma $ & N &  Epochs & Learning rate & Result \\
		\hline
%		0.85 & 29 & 27 & 0.0005623413251903491 & 0.3259957134723667 \\
%		0.85 & 77 & 35 & 0.00029427271762092817 & \textbf{0.26339022070169354} \\
%		0.5 & 29 & 26 & 0.00015399265260594922 & 0.26837945729494134 \\
%		0.5 & 77 & 26 & 0.00015399265260594922 & 0.2683794572949415 \\
		0.85 & 29 & 27 & 0.00056 & 0.32599 \\
		0.85 & 77 & 35 & 0.00029 & \textbf{0.26339} \\
		0.5 & 29 & 26 & 0.00015 & 0.26837 \\
		0.5 & 77 & 26 & 0.00015 & 0.26837 \\
		\hline
	\end{tabular}
\end{table}

The results shown in this table prove that too high adaptivity is not good for finding the smallest function value. However, when using higher adaptivity, the number of grid points has no big impact on finding the best solution in this case. This is because of the function shown in Figure \ref{fig:analysis_model_training}. In general, a higher adaptivity is good for finding a configuration faster. This can be useful in cases of hyperparameter optimization where not much time is available and a relatively good solution is enough. 

\subsection{Optimization on Sparse Grids}

By adding optimization methods like gradient descent or evolutionary algorithms, an even better approximation of the optimum might be found if enough grid points are available. If this number is too small then the interpolant is not precise enough and the solution of the optimizer is not the real optimum. One example is depicted in Figure \ref{fig:optimizers_network_visualization}. The steps of the normal gradient descent optimization algorithm are depicted and connected with arrows. In the background, the interpolated function is shown. The problem is the same as in Figure \ref{fig:analysis_model_training}. The degree of the B-splines on the grid is 2 and the sparse grid points are shown in white. 

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.49\textwidth]{figures/Results/Machine_learning/optimizer/Optimizers_visualization_ml_5}
	\includegraphics[width=0.49\textwidth]{figures/Results/Machine_learning/optimizer/Optimizers_visualization_ml_10}
	
	\includegraphics[width=0.49\textwidth]{figures/Results/Machine_learning/optimizer/Optimizers_visualization_ml_30}
	\includegraphics[width=0.49\textwidth]{figures/Results/Machine_learning/optimizer/Optimizers_visualization_ml_50}
	
	\caption{ Optimization steps of gradient descent algorithm for 5 (top left), 9 (top right), 29 (bottom left) and 49 (bottom right) grid points. In the background of each plot, the contour of the interpolated function is shown. The function evaluated is the same as depicted in Figure \ref{fig:analysis_model_training}. }
	\label{fig:optimizers_network_visualization}
\end{figure}

In Figure \ref{fig:optimizers_network_visualization}, the solution of the gradient descent optimization algorithm is analyzed with 5, 9, 29 and 49 grid points on the top left, top right, bottom left and bottom right, respectively. The minimal interpolated function values and the actual function evaluations can be seen in Table \ref{tab:results_opt_ml}.


\begin{table}[htbp!]
	\caption{ Exact values for the optima found by the sparse grid points and the gradient descent algorithm. The values for $ x_{min}^{grid} $ are the configurations evaluated by the sparse grid during the generation and the coordinates in column $ x_{min}^{opt} $ are found by the optimizer. In bold, the best function values of the optimum found by the sparse grid and gradient descent algorithm, respectively. }
	\label{tab:results_opt_ml}
	\centering
	\begin{tabular}{| c c c c c c |} 
		\hline
		N & $ x_{min}^{grid} $ & $ f(x_{min}^{grid}) $ & $ x_{min}^{opt} $ & $ f(x_{min}^{opt}) $ & $ f_{interpolated}(x_{min}^{opt}) $ \\ 
		\hline
		5 & $ (30.25, 3.162*10^{-6}) $ & $ 0.729 $ & $ (40, 3.162*10^{-6}) $ & $ 0.668 $ & $ 0.670 $ \\ 
		9 & $ (30.25, 5.623*10^{-4}) $ & $ 0.353 $ & $ (40, 1*10^{-1}) $ & $ 1.257 $ & $ -0.950 $ \\ 
		29 & $ (27.81, 5.623*10^{-4}) $ & $ 0.326 $ & $ (27.81,5.623*10^{-4}) $ & $ \mathbf{0.326} $ & $ 0.326 $ \\ 
		49 & $ (20.5, 2.129*10^{-4}) $ & $ \mathbf{0.267} $ & $ (21.72,2.094*10^{-2}) $ & $ 0.524 $ & $ -0.247 $ \\ 
		\hline
	\end{tabular}
\end{table}

In the first case, the optimizer starts at the right grid points and makes one step towards the right boundary of the domain. In the top right case with 9 grid points, the solution of the optimizer is at the right top corner of the domain. With 29 basis points in the left lower case, the solution of the optimizer is the same as the grid point with the smallest evaluated function value and in the right lower plot, the optimizer first takes a step to the right and then a second step up to a higher learning rate. 

As illustrated in Table \ref{tab:results_opt_ml}, the optimizer does not always find an actual better solution. While the interpolated function value actually decreases, the evaluation of the network with the given configuration results in a higher value which is worse. This behavior is the same as in Figure \ref{fig:Optimizer_results} where both optimizers lead to a higher error than the smallest value found by the sparse grid. This is due to the small number of function evaluations made so far and the interpolant of the function becing too inaccurate. Also the visual comparison of the interpolant plots in Figure \ref{fig:optimizers_network_visualization} and Figure \ref{fig:analysis_model_training} shows that there are still big differences. Nevertheless in the scope of this thesis, further increasing the number of grid points is infeasible because of the high costs of one single function evaluation which is the training and evaluation of a machine learning model. \newline

Note that the additional optimization is very cheap compared to generating the adaptive sparse grid because the algorithm uses the interpolated function and does not have to train and evaluate whole machine learning models. In Figure \ref{fig:Comparison_optimizers}, the different algorithms for optimization based on the sparse grid are compared. The same neural network with the same dataset is used for the comparison.


\begin{figure}[htbp!]
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Number of grid points,
			ylabel = Result (2d),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=1
			]
			
			\addplot coordinates
			{(5,0.7287221550941467)(9,0.3527909889817238)(29,0.32599571347236633)(49,0.2671877220273018)(69,0.2633902207016945)};
			
			\addplot coordinates 
			{(5,0.6684863269329071)(9,1.3836875557899475)(29,0.32599571347236633)(49,0.29520951211452484)(69,0.2633902207016945)};
			
			\addplot coordinates
			{(5,0.6684863269329071)(9,1.290485441684723)(29,0.32599571347236633)(49,0.2671877220273018)(69,0.2633902207016945)};
			
			\addplot coordinates 
			{(5,2.1821350157260895)(9,8.017291724681854)(29,0.32599571347236633)(49,0.3126538023352623)(69,0.2633902207016945)};
			
			\addplot coordinates 
			{(5,0.7181140184402466)(9,8.104115903377533)(29,1.1781185865402222)(49,1.290873944759369)(69,0.7903150916099548)};
			
			\addplot coordinates
			{(5,0.6684863269329071)(9,1.2569370865821838)(29,0.32599571347236633)(49,0.5238737538456917)(69,0.2633902207016945)};
			
			\addplot coordinates 
			{(5,0.6905131638050079)(9,0.6032524108886719)(29,1.3066118955612183)(49,1.2903412878513336)(69,0.6954652667045593)};
			
			\addplot coordinates
			{(5,0.6684863269329071)(9,1.2569370865821838)(29,0.32599571347236633)(49,1.3082255125045776)(69,0.2633902207016945)};
			
			\addplot coordinates 
			{(5,0.6684863269329071)(9,0.6032524108886719)(29,0.32599571347236633)(49,0.4634876996278763)(69,0.27645721286535263)};
			
			\addplot[green,mark=star] coordinates
			{(5,0.6684863269329071)(9,1.2569370865821838)(29,0.32599571347236633)(49,0.2671877220273018)(69,0.2633902207016945)};
			
			\addplot[black,mark=star] coordinates 
			{(5,0.872105598449707)(9,0.8723746538162231)(29,0.32599571347236633)(49,0.32110922783613205)(69,0.2806633412837982)};
			
			
			\legend{without opt, adaptive gradient descent, adaptive newton, bfgs, differential evolution, gradient descent, multi start, nlcg, nelder mead, newton, rprop}
			
		\end{axis}
	\end{tikzpicture}
\centering
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Number of grid points,
			ylabel = Result (3d),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=1
			]
			
			\addplot coordinates
			{(1,0.8082304298877716)(7,0.3521949201822281)(25,0.2879640683531761)(49,0.2879640683531761)(67,0.26214249432086945)};
			
			\addplot coordinates 
			{(1,0.8082304298877716)(7,0.6366344541311264)(25,0.2879640683531761)(49,0.2879640683531761)(67,0.26214249432086945)};
			
			\addplot coordinates
			{(1,0.8082304298877716)(7,0.6366344541311264)(25,0.2879640683531761)(49,0.2879640683531761)(67,0.26214249432086945)};
			
			\addplot coordinates 
			{(1,0.8082304298877716)(7,0.6366344541311264)(25,0.2879640683531761)(49,0.2879640683531761)(67,0.26214249432086945)};
			
			\addplot coordinates
			{(1,0.8723749816417694)(7,1.0919488668441772)(25,0.44320355355739594)(49,1.269576072692871)(67,0.5115671902894974)};
			
			\addplot coordinates 
			{(1,0.8082304298877716)(7,0.6366344541311264)(25,0.2879640683531761)(49,0.2879640683531761)(67,0.26214249432086945)};
			
			\addplot coordinates
			{(1,0.2994298115372658)(7,0.6812499314546585)(25,0.503110408782959)(49,0.6323543787002563)(67,0.401133269071579)};
			
			\addplot coordinates 
			{(1,0.8082304298877716)(7,0.6366344541311264)(25,0.2879640683531761)(49,0.2879640683531761)(67,0.26214249432086945)};
			
			\addplot coordinates
			{(1,0.8082304298877716)(7,1.2571589648723602)(25,0.31235726177692413)(49,0.2879640683531761)(67,1.3579680025577545)};
			
			\addplot[green,mark=star] coordinates 
			{(1,0.8082304298877716)(7,0.6366344541311264)(25,0.2879640683531761)(49,0.2879640683531761)(67,0.26214249432086945)};
			
			\addplot[black,mark=star] coordinates
			{(1,0.8082304298877716)(7,0.8723750114440918)(25,0.31212611496448517)(49,0.2879640683531761)(67,0.2691311687231064)};
			
			\legend{without opt, adaptive gradient descent, adaptive newton, bfgs, differential evolution, gradient descent, multi start, nlcg, nelder mead, newton, rprop}
			
		\end{axis}
	\end{tikzpicture}
	\caption{ Comparison of optimization algorithms on the sparse grid with increasing number of grid points in two and three dimensions. Epochs, learning rate, and the batch size of a two-layer neural network on the diamonds dataset for regression are optimized. Result is the mean absolute percentage error.  }	
	\label{fig:Comparison_optimizers}
\end{figure}

For both cases, the solution found by the sparse grid without optimization step decreases with increasing number of basis points. For the optimizers, this is not always the case. The resulting machine learning performance with the configuration found by the respective algorithm is even much worse than the one found by the sparse grid in some cases. In comparison to Figure \ref{fig:Optimizer_results}, we are only using a small number of grid points and get the same result in this situation. But further increasing the number of grid points is not feasible because of the high evaluation cost of the neural network. \newline

One thing that has to be taken into consideration is that e.g. the epochs hyperparameter is not continuous. The values for the number of epochs are always integers converted with \textit{int()}. This means that all values in the interval $ [x, x+1] $ for any $ x \in \mathbb{N} $ are interpreted as the smaller integer $ x $. This means that there are regions in the interval that have the exact same function value because they are evaluated with the same configuration. This fact might impact the behavior of a gradient based optimizer. If the function values in an interval are exactly the same then the gradient is zero in this interval. This would make it impossible to find the real optimum if the optimizer gets stuck. In the experiment shown in Figure \ref{fig:optimizer_categorical_hyperparams}, an extreme example is tried out. We use two hyperparameters with three distinct values each. The first one is the batch size with values from $ {100, 400, 2000} $ and the second one is epochs taking values from $ {1, 5, 13} $. In two dimensions, nine different configurations are then possible. We used B-splines of degree 0 and a regular sparse grid with smallest adaptivity possible. As the machine learning model being evaluated, we took a 2-layer network with 30 neurons each and a learning rate of $ 10^{-5} $. As dataset, the diamonds regression set is used.

\begin{figure}[htbp!]
	\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/optimizer/categorical_hyperpar/Categorical_hyppar_28}
	\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/optimizer/categorical_hyperpar/Categorical_hyppar_48}
	
	
	\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/optimizer/categorical_hyperpar/Categorical_hyppar_98}
	\includegraphics[width=0.48\textwidth]{figures/Results/Machine_learning/optimizer/categorical_hyperpar/Categorical_hyppar_198}
	
	\caption{ Sparse grid optimization with the interpolated function in the background, the white grid points, and the optimizer steps in red. Four different budgets are used (30: top left, 50: top right, 100: bottom left, 200: bottom right. Only three different configurations with values for batch size from $ {100, 400, 2000} $ and values for epochs from $ {1, 5, 13} $. }
	\label{fig:optimizer_categorical_hyperparams}
\end{figure}

In Figure \ref{fig:optimizer_categorical_hyperparams}, four different budgets (30, 50, 100, 200) which are shown top left, top right, bottom left and bottom right, respectively, are depicted. In each case, the interpolated function is shown in the background with the grid points from the sparse grid. In red, the steps of the gradient descent optimizer are shown. \newline 

We can observe one tendency. The more grid points are used, the more clear it gets that there are 9 different configurations and corresponding function values. For the first two cases in the top, the gradient descent still makes steps towards the bottom right corner of the domain. The interpolated function also still looks smooth. In the other two cases with a budget of 100 and 200, the optimizer does not take any steps because the grid points located next to the one with the smallest value also have the same smallest value. \newline 

Now in this case with only 9 distinct values, the sparse grid with its optimizer is finding the global optimum. But in other cases where there are more distinct configurations, this might lead to the optimizer not finding the global optimum. \newline


We can conclude that the use of any optimizer is not very promising for finding an improved solution in our case. However, we will still use a local and global optimizer because it is very cheap compared to the sparse grid generation and in some cases, the solution is improved. The resulting configuration returned by the algorithm will just be the best of three alternatives. \newline


\section{Comparison with Grid-, Random Search and Bayesian Optimization}

Now with the sparse grid search being analyzed with defined functions and a small neural network, this algorithm will be compared to the already existing optimization methods presented in Chapter \ref{chapter:theoretical_background}. We will always compare the algorithms depending on some given budget. This is the upper bound for the number of function evaluations the algorithm is allowed to make. For the grid search, this budget is always $ n^d $ where $ d $ is the number of hyperparameters or dimension of the problem. Therefore, $ n $ different values are taken for each hyperparameter. On the other hand, for the sparse grid search, the highest number of grid points is $ budget - 2 $ because we have to evaluate the point of the local and global optimization for comparison. \newline 

\subsection{Two-dimensional Experiment of Regression with Small Neural Network}

The first experiment is using a two layer neural network with 40 neurons in each layer. A batch size of 100 is used and as datasets, 4 different tasks from OpenML are taken. Table \ref{tab:datasets_first_experiment} gives an overview over the datasets. 

\begin{table}[htbp!]
	\centering
	\caption{ Overview over the datasets used for the first comparison of the optimization algorithms. They are all available on OpenML \cite{OpenML2013}. }
	\label{tab:datasets_first_experiment}
	\begin{tabular}{| c | c | c | c |} 
		\hline
		 & \multicolumn{2}{c|}{Number of features} &   \\
		 Dataset& numeric & categorical & Number of instances  \\
		 \hline
		 diamonds & 7 & 3 & 53940  \\
		 house\_16H & 17 & 0 & 22784  \\
		 sensory & 11 & 1 & 576  \\
		 house\_sales & 16 & 2 & 21613  \\
		 Brazilian\_houses & 9 & 3 & 10692  \\
		\hline
	\end{tabular}
\end{table}

All the categorical features are transformed with one hot encoding and the numeric as well as the target values are scaled using the \textit{StandardScaler} from the \textit{scikit-learn} library \cite{scikit-learn}. Both transformers are trained on the training set and used for both training and test set. We used 2-fold cross validation with the mean absolute percentage error as metric. Before each run, the seeds for getting the random values are reset. This is necessary because the evaluation of the neural network with a specific configuration should always return the same value. For the B-splines on the sparse grid, we use a degree of 2 and the grid is generated with adaptivity value $ 0.85 $. Two hyperparameters, the number of epochs and the learning rate for the neural network's optimizer, are optimized. The first one is linear from 1 to 40 and the second hyperparameter logarithmic from $ 10^{-9} $ and $ 10^{-1} $. The resulting performance can be seen in Figure \ref{fig:result_first_comparison}.

\begin{figure}[htbp!]
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (diamonds),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.85
			]
			
			\addplot coordinates
			{(9,0.44587014615535736)(25,0.2773677632212639)(49,0.25869983434677124)(64,0.2679326832294464)};
			
			\addplot coordinates 
			{(10,0.5277889221906662)(30,0.34567804634571075)(50,0.2493194043636322)(70,0.2493194043636322)};
			
			\addplot coordinates
			{(10,0.3841075301170349)(30,0.27727870643138885)(50,0.23435243219137192)(70,0.24744291603565216)};
			
			\addplot coordinates 
			{(7,0.44587014615535736)(27,0.25871288776397705)(47,0.25869983434677124)(67,0.2585172727704048)};
			
			
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (house\_16H),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.85
			]
			
			\addplot coordinates
			{(9,0.6142911016941071)(25,0.44890759885311127)(49,0.4118947982788086)(64,0.3798752874135971)};
			
			\addplot coordinates 
			{(10,0.6987867951393127)(30,0.39865100383758545)(50,0.39865100383758545)(70,0.39865100383758545)};
			
			\addplot coordinates
			{(10,0.6207705140113831)(30,0.41639772057533264)(50,0.36166810989379883)(70,0.3422168046236038)};
			
			\addplot coordinates 
			{(7,0.6142911016941071)(27,0.4056653678417206)(47,0.39544127881526947)(67,0.39544127881526947)};
		
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (sensory),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.85
			]
			
			\addplot coordinates
			{(9,0.04462818056344986)(25,0.04462808556854725)(49,0.04462808184325695)(64,0.04462808184325695)};
			
			\addplot coordinates 
			{(10,0.04297909699380398)(30,0.04257974028587341)(50,0.042243996635079384)(70,0.042243996635079384)};
			
			\addplot coordinates
			{(10,0.04346185550093651)(30,0.04339478723704815)(50,0.043271640315651894)(70,0.04313840717077255)};
			
			\addplot coordinates 
			{(7,0.04462807811796665)(27,0.04462807811796665)(47,0.04462807811796665)(67,0.04462807811796665)};
			
			
			
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (house\_sales),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.85
			]
			
			\addplot coordinates
			{(9,0.14392731338739395)(25,0.14080144464969635)(49,0.14392731338739395)(64,0.13686956465244293)};
			
			\addplot coordinates 
			{(10,0.15643399953842163)(30,0.14065127074718475)(50,0.13791099935770035)(70,0.13791099935770035)};
			
			\addplot coordinates
			{(10,0.1414126306772232)(30,0.13740301132202148)(50,0.13768181949853897)(70,0.13644825667142868)};
			
			\addplot coordinates 
			{(7,0.14392731338739395)(27,0.13739430904388428)(47,0.13533496856689453)(67,0.13533496856689453)};
			
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (Brazilian\_houses),
			cycle list name=exotic,
			legend style={at={(2,0.7)}},
			scale=0.85
			]
			
			\addplot coordinates
			{(9,0.10614052042365074)(25,0.05877404101192951)(49,0.031237424351274967)(64,0.045658400282263756)};
			
			\addplot coordinates 
			{(10,0.14960936456918716)(30,0.0883832648396492)(50,0.0883832648396492)(70,0.055092256516218185)};
			
			\addplot coordinates
			{(10,0.052035532891750336)(30,0.04132956638932228)(50,0.04881274700164795)(70,0.04101819172501564)};
			
			\addplot coordinates 
			{(7,0.14743999391794205)(27,0.029803195036947727)(47,0.029803195036947727)(67,0.029803195036947727)};
			
			\legend{Grid search, Random search, Bayesian optimization, Sparse Grid Search}
			
		\end{axis}
	\end{tikzpicture}
	\caption{ Comparison of grid-, random search, bayesian optimization and sparse grid search for the datasets shown in Table \ref{tab:datasets_first_experiment}. Two hyperparameters, epochs and learning rate of the neural network's optimizer, were optimized. }	
	\label{fig:result_first_comparison}
\end{figure}

It can be seen that in almost all cases, the result is decreasing with increasing cost. For the house\_sales and Brazilian\_houses datasets, the sparse grid optimization is performing best for a cost which is higher than 20. For the sensory dataset, all four optimization algorithms already get very good results with small costs. This is due to the small number of data entries. This is not representative for a general behavior. When comparing the normal grid search and the sparse grid optimization, the second one performs better in most cases. Figure \ref{fig:Comparison_visualization} shows the behavior for the house\_sales dataset and a upper bound of 10, 30, and 50 for the cost.

\begin{figure}[htbp!]
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Grid10}
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Grid30}
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Grid50}
		\caption{ Grid search with 9 (left, $f(x_{opt}) = 0.14393$), 25 (center, $f(x_{opt}) = 0.14080$), and 49 (right, $f(x_{opt}) = 0.14393$) grid points.}
		\label{fig:first_comparison_grid_search}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Random10}
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Random30}
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Random50}
		\caption{ Random search with 10 (left, $f(x_{opt}) = 0.14141$), 30 (center, $f(x_{opt}) = 0.13526$), and 50 (right, $f(x_{opt}) = 0.13619$) samples. }
		\label{fig:first_comparison_random_search}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Bayesian10}
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Bayesian30}
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Bayesian50}
		\caption{ Bayesian Optimization with 10 (left, $f(x_{opt}) = 0.13926$), 30 (center, $f(x_{opt}) = 0.13877$), and 50 (right, $f(x_{opt}) = 0.13542$) samples. }
		%\caption{ Sparse grid optimization... performance of \textbf{0.14393} (left), \textbf{0.13739} (center), and \textbf{0.13533} (right)  }
		\label{fig:first_comparison_bayesian_optimization}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Sparse8}
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Sparse28}
		\includegraphics[width=0.31\textwidth]{figures/Results/First_comparison/Sparse48}
		\caption{ Sparse grid optimization with 5 (left, $f(x_{opt}) = 0.14393$), 25 (center, $f(x_{opt}) = 0.13739$), and 45 (right, $f(x_{opt}) = 0.13533$) grid points. }
		\label{fig:first_comparison_sparse_optimization}
	\end{subfigure}
	
	\caption{ Comparison of the grids generated for the house\_sales dataset. The best configuration found $ x_{opt} $ is marked with a white cross.}
	\label{fig:Comparison_visualization}
\end{figure}

The big difference between grid search and sparse grid optimization is the adaptivity and the location of grid points. In Figure \ref{fig:Comparison_visualization}, this can be seen very clearly. While the grid generated for the grid search (see Figure \ref{fig:first_comparison_grid_search}) is very homogeneous and spread over the whole domain, the grid points are more gathered for the sparse grid optimization (see Figure \ref{fig:first_comparison_sparse_optimization}). For these upper bounds for the cost and for this dataset, the sparse grid optimization finds configurations with a bit better performance.


\subsection{Three- and Five-dimensional Experiments of Regression with Small Neural Network}

While the two-dimensional case is good for the visualization of the approaches, one is often interested in the optimization of more hyperparameters. In the following, we will focus again on regression problems, but this time we will include the batch size, the number of neural layers, and the number of neurons per layer in the hyperparameter space. \newline 

For the three dimensional case, we first use the same network with fixed architecture again, consisting of 2 layers with 40 neurons each. The hyperparameterspace consists of the epochs, the batch size and the learning rate of the neural network. The first two are linear integer from 1 to 40 for the epochs and the batch size has 100 and 2050 as bounds. The learning rate is again logarithmic from $ 10^{-9} $ to $ 10^{-1} $. We used the three datasets house\_16H, house\_sales, and Brazilian\_houses (see Table \ref{tab:datasets_first_experiment}). The resulting performance can be seen in Figure \ref{fig:result_second_comparison_dim3}. 


\begin{figure}[htbp!]
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (house\_16H),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.8
			]
			
			\addplot coordinates
			{(27,0.5013840645551682)(27,0.5013840645551682)(64,0.4510077089071274)(64,0.4510077089071274)(64,0.4510077089071274)};
			
			\addplot coordinates 
			{(30,0.4240635335445404)(50,0.4060138463973999)(70,0.4060138463973999)(90,0.40074291825294495)(110,0.3524976521730423)};
			
			\addplot coordinates
			{(30,0.33826425671577454)(50,0.3445577770471573)(70,0.44258445501327515)(90,0.36445462703704834)(110,0.3701321929693222)};
			
			\addplot coordinates 
			{(27,0.7578987777233124)(45,0.7578987777233124)(69,0.36318539083004)(87,0.3436579704284668)(105,0.3411036729812622)};
		
			
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (house\_sales),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.8
			]
			
			\addplot coordinates
			{(27,0.1389964520931244)(27,0.1389964520931244)(64,0.1349487155675888)(64,0.1349487155675888)(64,0.1349487155675888)};
			
			\addplot coordinates 
			{(30,0.13356372714042664)(50,0.13356372714042664)(70,0.1317945420742035)(90,0.1317945420742035)(110,0.12730281800031662)};
			
			\addplot coordinates
			{(30,0.13418538868427277)(50,0.13357464969158173)(70,0.1307784542441368)(90,0.13196838647127151)(110,0.1292630285024643)};
			
			\addplot coordinates 
			{(27,0.1285368874669075)(45,0.1285368874669075)(69,0.12759799510240555)(87,0.12759799510240555)(105,0.12759799510240555)};
			
			
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
	\begin{axis}[
		xlabel = Cost,
		ylabel = Result (Brazilian\_houses),
		cycle list name=exotic,
		legend style={at={(2,0.7)}},
		scale=0.8
		]
		
		\addplot coordinates
		{(27,0.21274123340845108)(27,0.21274123340845108)(64,0.11224503070116043)(64,0.11224503070116043)(64,0.11224503070116043)};
		
		\addplot coordinates 
		{(30,0.0599429477006197)(50,0.055251454934477806)(70,0.055251454934477806)(90,0.055251454934477806)(110,0.055251454934477806)};
		
		\addplot coordinates
		{(30,0.09721800312399864)(50,0.08995457738637924)(70,0.12300247699022293)(90,0.06836232542991638)(110,0.068906981498003)};
		
		\addplot coordinates 
		{(27,0.08419231697916985)(45,0.07201046124100685)(69,0.07080245390534401)(87,0.07080245390534401)(105,0.06940628960728645)};
		
		\legend{Grid search, Random search, Bayesian optimization, Sparse grid optimization}
		
	\end{axis}
	\end{tikzpicture}
	\caption{ Comparison of grid search, random search, bayesian optimization and sparse grid search. The model optimized consists of two layers with 40 neurons each. The number of epochs, batch size, and learning rate are optimized.  }	
	\label{fig:result_second_comparison_dim3}
\end{figure}


The advantage of adaptive sparse grid optimization compared to a homogeneous grid is the adaptivity meaning that more evaluations are done in a region of already small values. Especially with the house\_sales dataset, the sparse grid optimization achieves good results already with small cost. Although in general, this method needs at least some cost to generate a grid that is precise enough to find the optimal point. This can be seen in the case of the house\_16H dataset. At first, the resulting mean average percentage error is very high compared to the other three techniques but after a cost of 69, the sparse grid optimization achieves the best results with the smallest error. \newline 

To also analyze the behavior in settings of higher dimensionality, we now also add the number of layers and the number of neurons per layer as fourth and fifth variable to the hyperparameter space. The other variables and their intervals stay the same. The number of layers can now range from 2 to 21 and each layer can have 1 to 20 neurons. The resulting errors depending on the cost for the same three datasets as in Figure \ref{fig:result_second_comparison_dim3} can be seen in Figure \ref{fig:result_second_comparison_dim5}.


\begin{figure}[htbp!]
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (house\_16H),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.8
			]
			
			\addplot coordinates
			{(32,0.40729059278964996)(32,0.40729059278964996)(32,0.40729059278964996)};
			
			\addplot coordinates 
			{(50,0.36966221034526825)(90,0.3504626899957657)(130,0.3504626899957657)};
			
			\addplot coordinates
			{(50,0.35314927995204926)(90,0.3563224822282791)(130,0.3386409282684326)};
			
			\addplot coordinates 
			{(43,0.3417779803276062)(83,0.3410039693117142)(123,0.3410039693117142)};
			
			
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (house\_sales),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.8
			]
			
			\addplot coordinates
			{(32,0.1750636175274849)(32,0.1750636175274849)(32,0.1750636175274849)};
			
			\addplot coordinates 
			{(50,0.13386639952659607)(90,0.13123302906751633)(130,0.13123302906751633)};
			
			\addplot coordinates
			{(50,0.1305573582649231)(90,0.13035045564174652)(130,0.12902139127254486)};
			
			\addplot coordinates 
			{(43,0.1314886212348938)(83,0.12908799946308136)(123,0.12908799946308136)};
			
			
		\end{axis}
	\end{tikzpicture}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Result (Brazilian\_houses),
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.8
			]
			
			\addplot coordinates
			{(32,0.20395687967538834)(32,0.20395687967538834)(32,0.20395687967538834)};
			
			\addplot coordinates 
			{(50,0.11145678907632828)(90,0.10634149610996246)(130,0.10634149610996246)};
			
			\addplot coordinates
			{(50,0.1136467456817627)(90,0.07838094979524612)(130,0.07470651902258396)};
			
			\addplot coordinates 
			{(43,0.08488190919160843)(83,0.03610859997570515)(123,0.028350481763482094)};
			
			\legend{Grid search, Random search, Bayesian, Sparse grid optimization}
			
		\end{axis}
	\end{tikzpicture}
	\caption{  Comparison of grid search, random search, bayesian optimization and sparse grid search. The number of epochs, batch size, and learning rate are optimized as well as the number of layers and number of neurons per layer. }	
	\label{fig:result_second_comparison_dim5}
\end{figure}


The big advantage of sparse grid is the efficiency in higher dimensions. This can be seen in Figure \ref{fig:result_second_comparison_dim5}. Here with 5 dimensions and a maximum limit for cost of 130, the normal grid search can only use 2 different values for each hyperparameter leading to 32 function evaluations. That is the reason why we can only see one point in each of the three plots for the datasets. The big advantage of sparse grid optimization is that less grid points are needed and those basis points generated are also adaptive to the problem leading to a fast decrease of error. For almost all three datasets, the sparse grid optimization finds the configuration leading to the smallest mean absolute percentage error. 


\subsection{Nine-dimensional Experiment with MNIST Dataset}

To also compare the different approaches with a convolutional neural network for object detection, we use a small model with the MNIST dataset. Nine different hyperparameters are optimized. An overview over the type and the ranges can be seen in Table \ref{tab:hyperparameter_space_mnist}.

\begin{table}[htbp!]
	\caption{ Hyperparameters with their type and interval for the nine dimensional space. The values for the Int-Interval are discretized with Python's \textit{int()} function. The learning rate is sampled logarithmic and the dropout probability can take continuous values between 0 and 1. }
	\label{tab:hyperparameter_space_mnist}
	\centering
	\begin{tabular}{| c c c |} 
		\hline
		Hyperparameter & Type & Interval \\ 
		\hline
	 	Epochs & Int-Interval & $ [1,10] $ \\ 
	 	Batch size & Int-Interval & $ [200,1000] $ \\ 
	 	Learning rate & Log-Interval & $ [10^{-10},10^{-1}] $ \\ 
	 	Number conv layers & Int-Interval & $ [1,3] $ \\ 
	 	Number fully connected layers & Int-Interval & $ [1,3] $ \\ 
	 	Kernel size & Int-Interval & $ [1,4] $ \\ 
	 	Pool size & Int-Interval & $ [1,3] $ \\ 
	 	Neurons in fully connected & Int-Interval & $ [1,7] $ \\ 
	 	Dropout probability & Interval & $ [0,1] $ \\ 
		\hline
	\end{tabular}
\end{table}

The input shape is always $ (28, 28, 1) $ which defines the first layer of the network. After that, blocks consisting of convolutional and pooling layers are added. The number of blocks are defined by the hyperparameter number of convolutional layers. The kernel and pool size of the 2D convolution and the following pooling layer is defined by the kernel size and pool size and they are optimized. After that, fully connected layers are added. The number of layers as well as the number of neurons per layer are optimized with the two hyperparameters. Between the convolutional part and the fully connected layers, a dropout layer is added. The probability is optimized as well. The other hyperparameters like epochs, batch size, and learning rate are optimized like in the previous experiments. As optimizer, we used the Adam optimizer and the loss function is the categorical crossentropy. \newline 

The evaluation of the network is done as follows. The dataset is fetched with the predefined training and test set from keras. The model is fit on the training set with validation split of $ 10\% $. It is not shuffled as we want to have reproducability because the same hyperparameter configuration should produce the same result. This is also the reason why we reset the seeds of Python's random functions to always have the same weight initialization of the network. After the training step, we evaluate the model using the test set. As optimization metric we use the negative accuracy of the prediction as we still minimize. The resulting accuracy depending on the increasing cost is depicted in Figure \ref{fig:MNIST_results}.



\begin{figure}[htbp!]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Accuracy,
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.8
			]
			
			\addplot coordinates
			{(1,0.1006999984383583)};
			
			\addplot coordinates 
			{(5,0.7871000170707703)(30,0.863099992275238)(50,0.9151999950408936)(80,0.974399983882904)(110,0.974399983882904)};
			
			\addplot coordinates
			{(5,0.19499999284744263)(30,0.9649999737739563)(50,0.9501000046730042)(80,0.9753999710083008)(110,0.9714999794960022)};
			
			\addplot coordinates 
			{(3,0.3700999915599823)(21,0.8806999921798706)(39,0.9319000244140625)(75,0.9732999801635742)(93,0.9753999710083008)};
			
			\legend{Grid search, Random search, Bayesian, Sparse grid optimization}
			
		\end{axis}
	\end{tikzpicture}
	\caption{ Hyperparameter optimization of a convolutional neural network on the MNIST dataset with different algorithms. The hyperparameter intervals are presented in Table \ref{tab:hyperparameter_space_mnist}. }	
	\label{fig:MNIST_results}
\end{figure}

Again, with increasing cost, the algorithms find better configurations leading to a higher accuracy. In this example, the grid search can only evaluate the model with one single configuration. This is due to the reason that the next step would be $ 2^9 = 512 $ different configurations where it would try out 2 different values for each hyperparameter. \newline 

The first points for sparse grid optimization and bayesian search are also very low, but with a bit higher cost, the performance increases very much. For the random search, even the first point with a budget of 5, the best accuracy is already at about $ 79\% $. \newline 

For the sparse grid search, the advantage compared to the normal grid search is that there is no \textit{curse of the dimensionality}. The performance is comparable with the other two algorithms, random search and bayesian optimization. \newline 

The best configurations found by each algorithm for the problem are depicted in Table \ref{tab:best_configurations_MNIST}.

\begin{table}[htbp!]
	\caption{ Overview over the best configurations found by the different algorithms grid search (GS), random search (RS), bayesian optimization (BO) and sparse grid optimization (SG). The configuration is given as tuple (epochs, batch size, learning rate, number conv layers, number fully connected layers, kernel size, pool size, neurons per fc layer, dropout probability). }
	\label{tab:best_configurations_MNIST}
	\centering
	\begin{tabular}{| c c c c |} 
		\hline
		Algorithm & Configuration & Accuracy & Cost \\ 
		\hline
		GS & $ (5, 600, 10^{-6}, 2, 2, 2, 2, 4, 0.5) $ & $ 10.1\% $ & 1 \\ 
		RS & $ (9, 975, 0.0173, 2, 1, 3, 1, 6, 0.619) $ & $ 97.4\% $ & 80 \\ 
		BO & $ (6, 584, 10^{-2.17}, 2, 1, 3, 1, 5, 0.281) $ & $ 97.5\% $ & 80 \\ 
		SG & $ (7, 400, 10^{-2}, 2, 2, 2, 2, 5, 0.5) $ & $ 97.5\% $ & 93 \\ 
		\hline
	\end{tabular}
\end{table}

We can observe that the algorithms find different best configurations for the convolutional model. However, the resulting best accuracy is very similar. This is normal for many machine learning problems. There are many different possibilities for very good model performances. In this case, we already achieve a high accuracy of about $ 97\% $. However, it is still possible to further increase this performance by e.g. increasing the number of epochs or using a more complex neural network. But this is not what we want to analyze in this thesis. 

\subsection{Comparison with Implementation of other Authors}

A similar experiment with the same dataset is presented in \cite{WU201926}. The authors concentrate on bayesian optimization and optimize two hyperparameters. The first one is the batch size with an interval $ [20, 2000] $. The second one is the learning rate which can take values from $ [0,1] $. The network that is used for preicting the class of the images is a small convolutional neural network. The architecture can be seen in Table \ref{tab:architecture_MNIST_comp}.


\begin{table}[htbp!]
	\caption{ Architecture used in \cite{WU201926} for MNIST image classification. }
	\label{tab:architecture_MNIST_comp}
	\centering
	\begin{tabular}{| c c c c |} 
		\hline
		Type & Filters  & Kernel size & Activation function \\ 
		\hline
		Convolutional 2D & 32 & $ (5,5) $ & ReLU \\ 
		Max Pooling 2D &  & $ (2,2) $ &  \\ 
		Convolutional 2D & 32 & $ (5,5) $ & ReLU \\ 
		Max Pooling 2D &  & $ (2,2) $ &  \\ 
		Dense &  &  & Softmax \\ 
		\hline
	\end{tabular}
\end{table}

As optimizer, they used the gradient descent and the loss function is the categorical crossentropy. The metric to be optimized is the accuracy. \newline 

In \cite{WU201926}, it is not described for how many epochs they train the neural network which makes it hard to directly compare the algorithms. Additionally, we set the interval of the batch size to $ [20, 1020] $ because of hardware limitations. The learning rate is in our case again a logarithmic hyperparameter with lower bound of $ 10^{-16} $ and an upper bound of $ 1 $. For the neural network evaluation, we train the model on the training set with a validation split of $ 10\% $. The resulting score is then defined as the accuracy of the predictions of the test set. The result of the experiment with our four different algorithms are presented in Figure \ref{fig:Comparison_MNIST}.


\begin{figure}[htbp!]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Accuracy,
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.8
			]
			
			\addplot coordinates
			{(1,0.0860000029206276)(4,0.0892999991774559)(9,0.22550000250339508)(16,0.8327000141143799)(25,0.9394999742507935)(36,0.9652000069618225)(49,0.9764000177383423)};
			
			\addplot coordinates 
			{(3,0.15240000188350677)(7,0.9340000152587891)(11,0.9715999960899353)(17,0.9789999723434448)(25,0.9789999723434448)(40,0.9851999878883362)(55,0.9851999878883362)};
			
			\addplot coordinates
			{(3,0.9896000027656555)(7,0.9896000027656555)(11,0.9896000027656555)(17,0.9896000027656555)(25,0.9896000027656555)(40,0.9896000027656555)(55,0.9896000027656555)};
			
			\addplot coordinates 
			{(3,0.0860000029206276)(7,0.12290000170469284)(11,0.9416999816894531)(15,0.9824000000953674)(23,0.9861000180244446)(39,0.9883000254631042)(55,0.9883000254631042)};
			
			\legend{Grid search, Random search, Bayesian, Sparse grid optimization}
			
		\end{axis}
	\end{tikzpicture}
	\caption{ Resulting accuracy with increasing budget for the grid search, random search, bayesian optimization and sparse grid optimization. The MNIST dataset and a small convolutional neural network were used. }	
	\label{fig:Comparison_MNIST}
\end{figure}

Again, with increasing cost, the performance of the optimization techniques improve. The absolute highest accuracy is achieved by the bayesian optimization with $ 98.96\% $ followed by the sparse grid optimization with $ 98.83\% $, and random search with $ 98.52\% $. The lowest accuracy is achieved by grid search with a value of $ 97.64\% $. The authors of \cite{WU201926} reach an accuracy of $ 99.14\% $ with a budget of 50, however we do not know for how many epochs they trained the network. For the evaluation of our four algorithms, we always used exactly ten epochs. The best configurations found by the techniques are depicted in Table \ref{tab:configs_MNIST_comp}. \newline 

\begin{table}[htbp!]
	\caption{ Best configurations and corresponding test accuracy found by the four algorithms for the MNIST dataset. }
	\label{tab:configs_MNIST_comp}
	\centering
	\begin{tabular}{| c c c c |} 
		\hline
		Algorithm & Batch size & Learning rate & Accuracy \\ 
		\hline
		Grid search & 91 & 0.01000 & 0.9764 \\ 
		Random search & 42 & 0.03448 & 0.9852 \\ 
		Bayesian optimization & 57 & 0.1979 & 0.9896 \\ 
		Sparse grid optimization & 270 & 0.3162 & 0.9883 \\ 
		\hline
	\end{tabular}
\end{table}

The results shown in Table \ref{tab:configs_MNIST_comp} indicate that in this case, a rather small batch size leads to better testing results. This shows that the reduced interval is not restricting the performance of our algorithms. For this experiment, the authors in \cite{WU201926} do not provide the configuration found by their algorithm which makes it hard to compare the performances. 


\section{Iterative Adaptive Random Search}

In the previous analysis of the sparse grid optimization for finding suitable hyperparameter configurations, we have seen that using sparse grids has advantages compared to the other algorithms in some situations. However, the pure random search is very powerful in most cases. One possible adaption of this method is to add iterative refinement like it is done in the grid generation of sparse grids. This is a promising way of combining the advantages of iterative methods and random search which allows to evaluate many different values for each hyperparameter. 

\subsection{Presentation and Implementation details}

The algorithm starts with initial points that are distributed randomly in the whole search space. The number of these first points is variable and can be given by the user. A suitable value is dependent on the problem and the number of hyperparameters to be optimized. In each following iteration, one point is selected that is refined. The refinement criterion is similar to the Ritter-Novak criterion which is used for the sparse grid generation. The point i that minimizes 
\begin{equation}
\label{eq:refinement_criterion}
	(rank_i + 1)^{1-\gamma} \cdot (level_i + refinements_i + 1)^\gamma
\end{equation} 
is refined. Here, the rank is the position in the array which is sorted with ascending function value. The adaptivity parameter $ \gamma $ can be used to balance exploration and exploitation. Each point additionally has an attribute level which is 0 for the initial points. In each iteration, the level of the added points is one higher than the refined point. The refinements attribute is increased each time the point is refined. \newline

For the refinement strategy, meaning where the points are added, random points are sampled following a distribution on a specific domain. There are different possible techniques for the refinement. In general, the algorithm takes the steps as depicted in Algorithm \ref{alg:adaptive_random_search_alg}. The iterations take place as long as the number of points where the function is evaluated is smaller or equal than the budget which has to be given.

\begin{algorithm}[htbp!]
	\caption{  }
	\label{alg:adaptive_random_search_alg}
	\textbf{Input} \textbf{m}: number of initial points, \textbf{n}: number of refinements per iteration \textbf{budget}: upper bound for evaluations, \boldmath{ $ \gamma $}: adaptivity parameter\\
	\textbf{Output} \textbf{points}: Array of all evaluated points
	\begin{algorithmic}
		\For{\texttt{min(m, budget)}}
			\State \texttt{sample point uniformly in search space}
			\State \texttt{add point with level 0 to array}
		\EndFor
		\State \texttt{sort points with ascending function value}
		\While{Number of points + n <= Budget}
			\State \texttt{select point p\_ref according to refinement criterion}
			\For{\texttt{n}}
				\State \texttt{sample point according to refinement strategy}
				\State \texttt{add point with level p\_ref.level + 1}
				\State \texttt{increase p\_ref.refinements}
			\EndFor
			\State \texttt{increase number of points by n}
			\State \texttt{sort points with ascending function value}
		\EndWhile
		\State return array of points
	\end{algorithmic}
\end{algorithm}

The general idea of the refinement strategy is to sample points which are near to the point that is being refined because this point is either promising due to a small function value and thus rank, or it has not been refined that often. Concretely, we provide three different strategies.

\subsubsection{Interval Based Strategy}

For this first refinement strategy, intervals in each dimension are used. The bounds are defined by neighboring points with and the sampling is done uniformly. An example for such a refinement is presented in Figure \ref{fig:alternative_1}. \newline 

\begin{figure}[htbp!]
	\centering
	\includegraphics[scale=0.8]{figures/Adaptive_random/Visualizations/Alternative_1}
	\caption{ Example for the interval based refinement strategy. The point being refined is indicated with the blue cross and the lines show the upper and lower boundary in each direction. The sampling is done uniformly in the blue area. }
	\label{fig:alternative_1}
\end{figure}

In the example in Figure \ref{fig:alternative_1}, the number of initial points is 5. The point with the blue cross is selected by the refinement criterion defined with \ref{eq:refinement_criterion}. In each dimension, an interval is defined with the bounds from the next points in each dimension. In this case, the interval in horizontal direction is $ [-4.151, 5.045] $ and in the vertical dimension $ [-0.3779, 6.039] $. Note that only points with same or smaller level are considered when building the interval.

\subsubsection{Uniform D-Ball Sampling Strategy}

This strategy is based on the uniform sampling inside a multi dimensional ball. The point that is refined is the center of this ball and we define a certain radius of the ball. For the uniform sampling, multiple possible algorithms exist \cite{HARMAN20102297}. Some of them suffer from the curse of the dimensionality. One example is the rejection method where a value from $ [0,1]^d $ is sampled and rejected if it is not inside the ball. The probability for a sample being rejected increases with higher dimensionality. Another method is using the normal distribution. For a random variable $ Y \sim N_d(0_d, 1_d) $, $ S_n = \frac{Y}{|| Y ||} $ is uniform distributed on the d-sphere which is the surface of the ball. The last step is to multiply $ S_n $ by $ U^{\frac{1}{d}} $ where $ U $ has the uniform distribution on the interval between 0 and 1. Now the sampling is done inside the standard d-ball with radius 1. The final value just has to be multiplied by the radius to change the sampling to a d-ball with arbitrary size. One two-dimensional example can is presented in Figure \ref{fig:alternative_2}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[scale=0.8]{figures/Adaptive_random/Visualizations/Alternative_2}
	\caption{ Example for the uniform d-ball sampling strategy. The point being refined is indicated with the blue cross and the circle is the area where the uniform sampling is done. }
	\label{fig:alternative_2}
\end{figure}

In the example depicted in Figure \ref{fig:alternative_2}, five initial points (colored in red) are sampled. The one marked with the blue cross is chosen to be refined in this iteration. The radius is depending on the level of the point and the smallest and maximum distance to other points. It is computed with
\begin{equation}
	r = \frac{dist_{max} + dist_{min}}{(level+2)\cdot 2}
\end{equation}

where $ dist_{max} $ and $ dist_{min} $ are the highest and lowest distance to other points, respectively. The $ level $ is the level of the current point that is refined. With this, the radius of the ball at points with higher levels is smaller leading to samples near to the point being refined.

\subsubsection{Normal Distribution Sampling Strategy}

The last alternative is not sampling uniformly but based on the normal distribution. The idea behind this strategy is that we might be interested in sampling new points near the one that has to be refined in this iteration. This might lead to better results because the point that is refined might already be very promising. Therefor, in each dimension, the coordinate is sampled. A two-dimensional example is presented in Figure \ref{fig:alternative_3}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[scale=0.8]{figures/Adaptive_random/Visualizations/Alternative_3}
	\caption{ Example for the normal distribution sampling strategy. The point being refined is indicated with the blue cross and contour represents the likelihood of samples. }
	\label{fig:alternative_3}
\end{figure}

The example in Figure \ref{fig:alternative_3} shows five initial points in red. The point marked with the blue cross is refined in this iteration step. The contour plot in the background indicates the probability distribution for new points. In each dimension, a normal distribution is constructed with mean and standard deviation with 
\begin{equation}
	\begin{split}
		dist =& \frac{dist_{max} + dist_{min}}{(level+2)\cdot 2} \\
		lower =& \text{ min}(\text{ max}(coord - dist, bound_{lower}), bound_{upper}) \\
		upper =& \text{ max}(\text{ min}(coord + dist, bound_{upper}), bound_{lower}).
	\end{split}
\end{equation}

Here, $ coord $ is the coordinate of the point refined in the corresponding dimension, $ bound_{lower} $ and $ bound_{upper} $ represent the bounds of the interval of the hyperparameter in this dimension and the distance $ dist $ is calculated the same way as the radius of the second refinement strategy. \newline 

For the implementation, we added a new subclass inheriting \texttt{Optimization}. It has ad additional parameter for the choice of refinement strategy. 

\subsection{Analysis of Parameters with Functions}

In this case, we also have the problem that the optimal configuration of hyperparameters for the model evaluation is not known in advance. This is why we first conduct an analysis of the algorithm with functions where we know the optimum. We will especially focus on the following parameters of the adaptive random search:

\begin{itemize}
	\item Adaptivity parameter
	\item Number of initial points
	\item Number of refinements per iteration
	\item Refinement criterion
	\item Budget
\end{itemize}

We will optimize the Rosenbrock function (see Table \ref{tab:test_functions} and Figure \ref{fig:test_functions_plot}) in two dimensions for additional visualization. The optimal point is at $ (1, 1) $ with function value 0. 

\subsubsection{Adaptivity Parameter}

The first parameter we want to analyze is the adaptivity parameter $ \gamma $ to check, if the algorithm behaves as expected. Especially the refinement criterion and strategy are influencing the behavior depending on this parameter. Therefor, we want to visualize how the points are distributed depending on the concrete value. We use 


\begin{figure}[htbp!]
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.31\textwidth]{figures/Adaptive_random/Adaptivity_parameter/Alternative_0_adapt_1.0}
		\includegraphics[width=0.31\textwidth]{figures/Adaptive_random/Adaptivity_parameter/Alternative_0_adapt_0.75}
		\includegraphics[width=0.31\textwidth]{figures/Adaptive_random/Adaptivity_parameter/Alternative_0_adapt_0.0}
		\caption{ }
		\label{fig:alt0_adapt_param}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.31\textwidth]{figures/Adaptive_random/Adaptivity_parameter/Alternative_1_adapt_1.0_1_}
		\includegraphics[width=0.31\textwidth]{figures/Adaptive_random/Adaptivity_parameter/Alternative_1_adapt_0.75}
		\includegraphics[width=0.31\textwidth]{figures/Adaptive_random/Adaptivity_parameter/Alternative_1_adapt_0.0_1_}
		\caption{ }
		\label{fig:alt1_adapt_param}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.31\textwidth]{figures/Adaptive_random/Adaptivity_parameter/Alternative_2_adapt_1.0_2_}
		\includegraphics[width=0.31\textwidth]{figures/Adaptive_random/Adaptivity_parameter/Alternative_2_adapt_0.75}
		\includegraphics[width=0.31\textwidth]{figures/Adaptive_random/Adaptivity_parameter/Alternative_2_adapt_0.0_2_}
		\caption{ }
		\label{fig:alt2_adapt_param}
	\end{subfigure}
	
	\caption{  }
	\label{fig:Adapt_param_alternatives}
\end{figure}


\begin{table}[htbp!]
	\caption{  }
	\label{tab:results_alternatives}
	\centering
	\begin{tabular}{| c |  c |c c |} 
		\hline
		Alternative & Adaptivity parameter & Coordinates & Error \\ 
		\hline
		%1 & 1.0 & [0.4058562472117627, -0.04054645111546973] & 4.566409386159776 \\ 
		%1 & 0.75 & [1.038927050174919, 1.079563922856002] & 0.001519098543161749 \\ 
		%1 & 0.0 & [2.158598821574482, 4.617743382926759] & 1.517121125222536 \\ 
		
		%2 & 1.0 & [-0.11352135098080107, -0.7217782890446469] & 55.21325276318493 \\ 
		%2 & 0.75 & [1.6268172463461463, 2.6253824302103714] & 0.43764024412597213 \\ 
		%2 & 0.0 & [0.9541974556509123, 0.9027712932595593]  & 0.008060015566753589 \\ 
		
		%3 & 1.0 & [2.2818252596693043, 4.712738873843234] & 26.04545502357817 \\ 
		%3 & 0.75 & [0.9605735775731732, 0.838958015201549] & 0.7028532076339621 \\ 
		%3 & 0.0 & [1.097943243714717, 1.1936061770810482] & 0.02369014149435738 \\ 
		
		 & 1.0 & [0.4058, -0.04054] & 4.566 \\ 
		1 & 0.75 & \textbf{[1.038, 1.079]} & \textbf{0.001519} \\ 
		 & 0.0 & [2.158, 4.617] & 1.517 \\ 
		\hline
		 & 1.0 & [-0.1135, -0.7217] & 55.21 \\ 
		2 & 0.75 & [1.626, 2.625] & 0.4376 \\ 
		 & 0.0 & \textbf{[0.9541, 0.9027]}  & \textbf{0.008060} \\ 
		\hline
		 & 1.0 & [2.281, 4.712] & 26.04 \\ 
		3 & 0.75 & [0.9605, 0.8389] & 0.7028 \\ 
		 & 0.0 & \textbf{[1.097, 1.193]} & \textbf{0.02369} \\ 
		
		\hline
	\end{tabular}
\end{table}


\subsubsection{Number of initial points}

The first parameter we want to analyze is the number of initial points. Intuitively, a high value for this parameter makes the algorithm behave more similar like the normal random search. 



\begin{figure}[htbp!]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			xlabel = Cost,
			ylabel = Accuracy,
			cycle list name=exotic,
			legend pos=outer north east,
			scale=0.8,
			ymode=log,
			]
			
			\addplot coordinates
			{(1,1.0824542513569115)(2,0.022824707884739743)(3,5.585921915937778)(4,200.52004701446975)(5,1.986900892952598)(6,177.21789729703605)(7,0.6698486643704894)(8,3.3099362146828004)(9,3.433219063082395)(10,5.435078584997052)(11,4.003896464967107)(12,3.9434639671630585)(13,0.03724102900210536)(14,6.267498175504956)(15,0.3381284815468573)(16,0.046123385116976036)(17,0.0985549775462101)(18,0.19549670018723903)(19,0.07668040762064277)(20,0.3752236841353867)(21,0.009328082056961012)(22,0.07643868119616351)(23,0.1498686638808814)(24,0.03709701752919507)(25,0.008826856012237289)(26,0.09758779535476758)(27,0.03876440771759021)(28,0.008648299392310318)(29,0.01246134600284824)(30,0.3361016864127241)(31,0.5248598102907517)(32,3.8470435690614915)(33,0.48727762623627385)(34,3.889730397430627)(35,5.1847029860959015)(36,4.30659313364929)(37,1.9619040607343758)(38,0.08836918603390391)(39,0.08778600377594262)(40,0.052192211378525685)(41,0.5220480928988157)(42,0.07629943793273662)(43,0.624831540050998)(44,0.6044148116200726)(45,0.6697639710700126)(46,0.620283063233602)(47,0.5705940448145075)(48,0.05581015754261292)(49,0.17886935456146463)(50,0.664278908339185)(51,0.7358162014760116)(52,1.2534156620884254)(53,0.6057154346979778)(54,0.6175583181016042)(55,1.1011715386204202)(56,0.5138006709199092)(57,0.5101271808638543)(58,0.6902825766579348)(59,4.449059622057639)(60,3.8300017468029486)(61,0.566907833147002)(62,0.8838380034902121)(63,1.1365284578298318)(64,0.8002766610845463)(65,4.602829937321019)(66,4.469267054543329)(67,0.6184859582248011)(68,1.275830487834945)(69,0.497326174512877)(70,0.48599733289061353)(71,0.4871240942695079)(72,0.4908350374841834)(73,0.8358283868543894)(74,0.5354876233705403)(75,0.5399863375848265)(76,0.5399863375848265)(77,0.6036431419688656)(78,0.6473420869354031)(79,0.6473420869354031)(80,0.6473420869354031)(81,0.6151397669864926)(82,0.7220912201972044)(83,0.5537753774072882)(84,0.7065085444537679)(85,1.0034769819139193)(86,0.517652249762286)(87,0.5042561506198654)(88,0.7717948762634341)(89,0.9562857474478765)(90,2.669133044655288)(91,0.577540190819865)(92,1.0484318444927037)(93,1.0484318444927037)(94,1.0484318444927037)(95,0.7011948607030232)(96,0.5563405132173597)(97,0.6353040981400784)(98,0.6772299163700155)(99,0.5045799575696759)(100,0.5045799575696759)}
			;
			
			\addplot coordinates 
			{(1,91192.7352857547)(2,0.43210260522932)(3,1.1368838384606967)(4,2.3445964442569083)(5,1.6814943200784942)(6,2.00470256416415)(7,1.5752658254271203)(8,0.17980149496389383)(9,0.01968412710478548)(10,0.22116247750574144)(11,0.22026252848311473)(12,1.6059269001324084)(13,0.4088552460941704)(14,0.4649563991119929)(15,0.39993298989927584)(16,0.27184879283224994)(17,0.07600202590975233)(18,0.22688226912289206)(19,0.4422724673538001)(20,0.12015283696970903)(21,1.4140464411477467)(22,0.4536913220208135)(23,3.469665407501982)(24,0.07570066635312893)(25,0.09182148661293572)(26,0.2411926201685719)(27,0.13953529045176374)(28,0.60634175808139)(29,0.09598105343505758)(30,0.6544095718133428)(31,0.19425878444237885)(32,0.33773565198268096)(33,0.09109833588591429)(34,0.15278577328143375)(35,0.5303341269829903)(36,0.6935058625298565)(37,0.7104628617166381)(38,0.4623932667229571)(39,0.16960453248827906)(40,0.7090664627637555)(41,0.5946572798443049)(42,0.17503248632661675)(43,0.3370215360664308)(44,0.09880005953743214)(45,0.28141094691829055)(46,0.6522163490443755)(47,0.17547202680808982)(48,0.04636136984051147)(49,0.6098497196885282)(50,0.23094047845203358)(51,0.6714930907492961)(52,0.06482770554383079)(53,0.5885784194994911)(54,0.13472957808506333)(55,0.563608456450431)(56,0.6482412547173924)(57,2.317250931130875)(58,1.8197414760491424)(59,0.034545153138116944)(60,0.7093260752053131)(61,0.3504617260186304)(62,0.7002397809185289)(63,0.47645493631966374)(64,0.3027516872513246)(65,0.9496195955002291)(66,3.1117612726430908)(67,0.5191657586228722)(68,0.6926831471600906)(69,1.2059178993618986)(70,2.195049301832584)(71,1.328075287767527)(72,1.1185638510565967)(73,0.37930484072474985)(74,3.5031845768951313)(75,1.3057496943795555)(76,0.06614063241805569)(77,0.6130549564384976)(78,0.2626385616164666)(79,1.4943387201566776)(80,1.2861912755264524)(81,2.192420066505074)(82,1.476135008410604)(83,4.744242883189514)(84,0.39282053520199256)(85,2.19180125828675)(86,0.454904066990766)(87,2.2299372366465713)(88,6.3726454668946015)(89,4.959425948334679)(90,1.365975065623728)(91,1.9010560707399842)(92,0.04774819952882293)(93,1.9687070216325782)(94,1.6742934402276075)(95,4.487276519103571)(96,2.7077292764612206)(97,4.94109354538378)(98,0.11790626548177811)(99,0.08549996324339412)(100,2.6199247597976307)}
			;
			
			\addplot coordinates
			{(1,7.60886224544455)(2,1.5013844205787559)(3,5.038685005084763)(4,0.21152089786246958)(5,1.3029940917164315)(6,2.670403147475678)(7,1.546612736959585)(8,2.7565025007109845)(9,3.527362835617622)(10,2.785613393697746)(11,1.0443579686358238)(12,1.348815097011264)(13,0.09349857773981352)(14,1.3354447767046833)(15,2.1349131825210934)(16,5.045573728659054)(17,1.2927529006046998)(18,1.6071012172523884)(19,0.10936627458734662)(20,1.6838303163314052)(21,0.5944673233359798)(22,0.5257215743427377)(23,0.09527195663108633)(24,1.251465640426598)(25,0.29892354714633884)(26,0.3711208293064955)(27,0.7400141779791747)(28,1.7042598425218025)(29,1.7042598425218025)(30,2.121753751896072)(31,2.121753751896072)(32,4.574849141914204)(33,4.574849141914204)(34,4.574849141914204)(35,2.61381782284558)(36,4.574849141914204)(37,2.5822836275757535)(38,2.5822836275757535)(39,1.0607046801287117)(40,2.5822836275757535)(41,2.5822836275757535)(42,1.0011651253371112)(43,1.0011651253371112)(44,1.0011651253371112)(45,1.3744023409438184)(46,2.5822836275757535)(47,2.5822836275757535)(48,2.5822836275757535)(49,2.5822836275757535)(50,2.0970853979455626)(51,2.0970853979455626)(52,2.5822836275757535)(53,2.5822836275757535)(54,2.5822836275757535)(55,2.5822836275757535)(56,2.5822836275757535)(57,2.522677403782818)(58,2.522677403782818)(59,2.522677403782818)(60,2.522677403782818)(61,2.522677403782818)(62,2.522677403782818)(63,2.522677403782818)(64,2.522677403782818)(65,2.522677403782818)(66,2.522677403782818)(67,2.522677403782818)(68,2.522677403782818)(69,2.522677403782818)(70,2.522677403782818)(71,2.522677403782818)(72,2.522677403782818)(73,2.522677403782818)(74,2.522677403782818)(75,2.522677403782818)(76,2.522677403782818)(77,2.522677403782818)(78,2.522677403782818)(79,11.64581980879618)(80,11.64581980879618)(81,14.581454715878754)(82,1.2859031051630794)(83,1.2859031051630794)(84,1.2859031051630794)(85,1.7974951531253844)(86,1.7974951531253844)(87,1.7974951531253844)(88,1.7974951531253844)(89,1.7974951531253844)(90,1.8470346200866017)(91,1.8470346200866017)(92,1.8470346200866017)(93,1.8470346200866017)(94,1.8470346200866017)(95,1.5369585057441855)(96,1.5369585057441855)(97,2.04510629928819)(98,2.04510629928819)(99,2.04510629928819)(100,2.357496339355764)}
			;
			
			
			\legend{ALT 1, ALT 2, ALT 3}
			
		\end{axis}
	\end{tikzpicture}
	\caption{  }	
	\label{fig:init_points_alternatives}
\end{figure}


\subsection{Hyperparameter Optimization}

\begin{itemize}
	\item small first experiments (2d with visualizations)
\end{itemize}

\subsection{Comparison with other Techniques}

\begin{itemize}
	\item compare all 5 techniques
\end{itemize}








