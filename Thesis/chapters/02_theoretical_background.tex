% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in somexcv editors.

\chapter{Related Work}\label{chapter:theoretical_background}

\section{Hyperparameter Optimization}

Most machine learning models have parameters that have to be defined before the learning phase. They are called hyperparameters and strongly influence the behavior of the model. One example is the number of epochs of the learning phase of a neural network. There are different techniques for the optimization of hyperparameters and they all define the machine learning model as a black box function $ f $ with the hyperparameters as input and the resulting performance as output. The overall goal is to find a configuration $ \lambda_{min} $ from $ \Lambda = \Lambda_1 \times \Lambda_2 \times ... \times \Lambda_N $ that minimizes the function $ f $ with $ N $ hyperparameters with 
\begin{equation}
	\label{eq:optimization}
	\lambda_{min} = \text{arg} \min_{\lambda \in \Lambda} f(\lambda) .
\end{equation}

In our case, the function f is a machine learning algorithm that is trained on a training set and evaluated on a testing set. With this, the minimization of e.g. the loss of the model optimizes the decisions it is making which leads to better prediction results. Note that one function evaluation of f is usually very expensive as the training of a machine learning model with many parameters and weights takes much time. The data set consists of $ \{ (x_i, y_i) | x_i \in X, y_i \in Y, 0 \le i \le m \} $ with m being the number of data samples. The $ x_i $ is the input data to the model and the goal is that 
\begin{equation}
	\forall i: M(x_i) = y_i.
\end{equation}
where M is the model. In the context of supervised learning, the whole data set is split into a training set which is used to optimize the model and a testing set to evaluate the performance on new, unseen data \cite{supervised_learning}.

All in all, the goal is get evaluation scores on the testing data set which can be achieved with Equation \ref{eq:optimization}. 
\cite{feurer2019hyperparameter,bischl2021hyperparameter,yang2020hyperparameter}

In the following, different techniques for the optimization are presented and discussed with their advantages and disadvantages.

\subsection{Grid Search}
The idea of the first approach for the optimization is to discretize the domains of each hyperparameter and evaluate each combination. This suffers from the curse of the dimensionality as it scales exponentially with the number of hyperparameters. For $ d $ parameters and $ n $ values per hyperparameter, $ n^d $ different configurations are possible which all have to be evaluated. 

One advantage of this method is that it is easy to implement and very simple. Also, the whole search space is explored evenly.

On the other hand, the curse of the dimensionality makes it very slow if the function evaluations are very expensive. This is the case for most machine learning algorithms because the training phase of the model takes much time. Another drawback is that each hyperparameter only takes $ n $ different values.


\subsection{Random Search}
The next technique \cite{random_search} is similar to the grid search because the idea is also to evaluate different hyperparameter configurations. In contrast to the previous one, random search generates for each run and for each parameter exactly one random value from an interval which has to be specified. For this approach, a budget $ b $ has to be given. This parameter determines the number of different combinations that are evaluated. A direct comparison of grid search and random search can be seen in figure \ref{fig:comparison_searches}.

\begin{figure}[hbtp!]
	\centering
	\includegraphics[scale=0.2]{figures/comparison_searches.png}
	\caption{Comparison of grid search (left) and random search (right) in the two dimensional case. For both techniques, 9 different combinations are evaluated. In the left case, only 3 distinct values for each hyperparameter are set whereas there are 9 different values for each parameter in the random search. Taken from \cite{feurer2019hyperparameter}. }
	\label{fig:comparison_searches}
\end{figure}

In this figure, a two dimensional setting is depicted. For both techniques, 9 different combinations are evaluated. In the case of grid search, only 3 distinct values are taken for each hyperparameter while there are 9 different ones in the random search. In this example, the better result is found in random search as more distinct values are taken for the important parameter. Note that this is not always the case. 

Compared to the normal grid search, this is one advantage. For each hyperparameter, $ b $ (budget) different values are taken into consideration which is much more compared to the grid search with the same overall number of combinations. Additionally, this technique is also easy to implement and relatively simple. 

One disadvantage is that it is also very expensive if the budget is high because of the long training times of machine learning models. 


\subsection{Bayesian Optimization}

Another possible technique for finding the best hyperparameters of machine learning models is called bayesian optimization (BO) \cite{snoek2012practical}. This is an iterative approach for optimizing the expensive black box function by modeling it based on observations. A so-called \textit{surrogate model} $ \hat{f} $ is made with the help of the \textit{archive} $ A $ which contains observed function evaluations. This surrogate model is created by regression and the technique which is most often used is the Gaussian process \cite{bischl2021hyperparameter} which is only suitable if the number of hyperparameters is not too high \cite{andonie2019hyperparameter}. The problem of this technique arises when some hyperparameters are categorical or integer-valued which is the reason why extra approximations can lead to worse results and special treatment is needed \cite{garrido2020dealing}. Another possible technique for the surrogate model is using random forests \cite{hutter2011sequential}. All in all, this function estimates the machine learning model depending on the hyperparameter configuration and also the prediction uncertainty $ \sigma(\lambda) $. A second function called \textit{acquisition function} $ u(\lambda)$ is built based on the prediction distribution. This $ u $ is responsible for the trade-off between exploitation and exploration. This means that configurations that lead to better model performances are exploited and values where no much information is gathered are explored. There are many numerous different possibilities for this function  \cite{wilson2018maximizing} but the most used one is the \textit{expected improvement} (EI) which is calculated with

\begin{equation}
	E[I(\lambda)] = E[max(f_{min}-y), 0].
\end{equation}

If the model prediction y with configuration $ \lambda $ follows a normal distribution \cite{feurer2019hyperparameter}, it leads to 

\begin{equation}
	E[max(f_{min}-y), 0] = (f_{min}-\mu(\lambda)) \Phi(\frac{f_{min}-\mu(\lambda)}{\sigma}) + \sigma \phi (\frac{f_{min}-\mu(\lambda)}{\sigma})
\end{equation}

with $ \phi $ and $ \Phi $ being the standard normal density and standard normal distribution and $ f_{min} $ the best result so far. 


In each iteration, a new candidate configuration $ \lambda^+ $  is generated by optimizing the acquisition function $ u $. This $ u $ is much cheaper to evaluate than the $ f $ which includes learning of an expensive neural network which makes the optimization much easier.

The exact steps are presented in Algorithm \ref{alg:bayesian_opt} and Figure \ref{fig:bayesian_optimization} shows schematic iteration steps.

\begin{algorithm}[htbp!]
	\caption{Bayesian Optimization}\label{alg:bayesian_opt}
	\begin{algorithmic}
		\State Generate initial $\lambda^{(1)}, ..., \lambda^{(k)} $
		\State Initialize archive $A^{[0]} \gets ((\lambda^{(1)}, f(\lambda^{(1)})), ..., (\lambda^{(k)}, f(\lambda^{(k)})))$
		\State $ t \gets 1 $ 
		\While{Stopping criterion not met}
			\State Fit surrogate model $ (f(\lambda), \sigma(\lambda)) $ on $ A^{[t-1]} $
			\State Build acquisition function $ u(\lambda) $ from $ (\hat{f}(\lambda), \sigma(\lambda)) $
			\State Obtain proposal $ \lambda^{+} $ by optimizing $ u: \lambda^+ \in arg\max_{\lambda \in \Lambda} u(\lambda) $
			\State Evaluate $ f(\lambda^+)$
			\State Obtain $A^[t]$ by augmenting $ A^{[t-1]} $ with $ (\lambda^{(+)}, f(\lambda^{(+)})) $
			\State $ t \gets t+1 $
		\EndWhile
		\State return $ \lambda_{best} $: Best-performing $\lambda$ from archive or according to surrogates prediction
	\end{algorithmic}
\end{algorithm}

\begin{figure}[htbp!]
	\centering
	\includegraphics[scale=0.35]{figures/bayesian_optimization.png}
	\caption{ Schematic iteration steps of the bayesian optimization. The maximum of the acquisition function determines the next function evaluation (red dot in the middle). The goal is to find the minimum of the dashed line. The blue band is the uncertainty of the function. Taken from \cite{feurer2019hyperparameter}. }
	\label{fig:bayesian_optimization}
\end{figure}


First, $ k $ initial hyperparameter configurations are sampled and evaluated. This set is the starting archive $ A^{[0]} $. After that, the loop is executed as long as the stopping criterion is not met. This can be for example a budget, meaning a maximum number of function evaluations. The first step of the loop is to fit the surrogate model on the current archive. Then the acquisition function is made and optimized to get the next configuration $ \lambda^+ $. This point is evaluated and added to the archive. The overall result of the algorithm is the $ \lambda $ which is the hyperparameter configuration for the machine learning model with the overall best result.

\subsection{Other Techniques}
There are also other techniques for finding the best hyperparameters. Multi-fidelity optimization \cite{feurer2019hyperparameter} aims to probe the learning of model on a task with reduced complexity such as a subset of the data or less epochs for training the model for discovering the best configurations. For example, the learning curve can be predicted so that early stopping can be done if the prediction is not as good as the best model so far. There are also bandit-based selection methods that do not predict the learning curve but compare the different combinations on a small number of epochs and only performs the best ones. This can be done iteratively like it is done in \textit{successive halving} for hyperparameter optimization \cite{jamieson2016non}. The algorithm is very simple. It starts to evaluate all different combinations with very small budget. The best half of the candidates are then evaluated in the next iteration with double budget and so on until only one combination is left. In \cite{8030298}, a similar algorithm is presented. The authors use a model of the objective function (neural network depending on configurations) to find candidate hyperparameters. Those are then trained on a smaller number of epochs and the best ones then evaluated with higher budget. Also neural networks can be used for the optimization which was done by the authors in \cite{smithson2016neural}. Also, covariance matrix adaptation evolution strategy was implemented as an alternative to bayesian optimization in \cite{loshchilov2016cma}. 


\section{Sparse Grids}

Sparse grids are a useful tool to mitigate the \textit{curse of the dimensionality} by reducing the number of grid points. In the following, this technique is presented after the general numerical approximation of functions.

\subsection{Numerical Approximation of Functions}

\cite{b_splines}
Let $ f: \Omega \rightarrow R $ be a function defined on the unit interval $ \Omega = [0,1]^d $ in $ d $ dimensions. For simplicity, we first set $ d=1$. Now this function can be represented on a grid of level $ l \in \mathbb{N}_0 $ with $ 2^l + 1 $ grid points which are 
\begin{equation}
	x_{l,i} = i*h_l, \text{   } i = 0,...,2^l,
\end{equation}

with i being the index and $ h_l = 2^{-l} $ being the distance between the grid points. Each of them gets a basis function defined by 
\begin{equation}
	\varphi_{l,i}: [0,1] \rightarrow \mathbb{R}.
\end{equation}

There are different possibilities for the basis functions which will be presented later. For the simplicity, we present a simple example being the hat function defined by
\begin{equation}
	\varphi_{l,i}(x) = \max(1- |\frac{x}{h_l}-i|, 0).
\end{equation}
All in all, the space of functions that can be presented exactly by a linear combination is called the \textit{nodal space} $V_l$ with the assumption that the basis functions form a basis:
\begin{equation}
	V_l = \text{span}\{ \varphi_{l,i} | i = 0,...,2^l\}. 
\end{equation}

Every function $f: [0,1] \rightarrow \mathbb{R}$ can be interpolated by a the interpolant $ u $ defined by
\begin{equation}
	f_l = \sum_{i=0}^{2^l}\alpha_{l,i} \varphi_{l,i}, \forall i = 0,...,2^l: f_l(x_{l,i}) = f(x_{l,i})
\end{equation}

for constants $ \alpha_{l,i} \in \mathbb{R} $. An example can be seen in Figure \ref{fig:interpolant}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[scale=0.5]{figures/weighted_sum.png}
	\caption{ Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the nodal basis. Level of the grid is 3 and hat functions are used. Taken from \cite{pfluger2010spatially}. }
	\label{fig:interpolant}
\end{figure}

On the left side, the function f (black line) can be seen with a grid of level 3. On the right side, the interpolant u as a linear combination of the basis functions (hat functions centered on the grid points) can be seen. This approach is the nodal basis. The second possibility is called hierarchical basis and the index set is $I_l^h = \{i \in \mathbb{N} | 1 \le i \le i \le s^l-1, i \text{odd}\}$. The hierarchical subspaces are then 
\begin{equation}
	W_l = \text{span}\{ \varphi_{l,i}(x) | i \in I_l^h\}.
\end{equation}

The same nodal space $ V_l $ can be obtained with the hierarchical subspaces with 
\begin{equation}
	V_l = \bigoplus_{i \le l} W_i.
\end{equation}

An example can be seen in Figure \ref{fig:hierarchical_basis}.
\begin{figure}[htbp!]
	\centering
	\includegraphics[scale=0.5]{figures/hierarchical_basis.png}
	\caption{ Hierarchical subspaces up to level 3 on the left. On the right, nodal spaces up to level 3. The combination of $ W_1 $ up to $ W_3 $ is the same space as $ V_3 $. Taken from \cite{pfluger2010spatially}. }
	\label{fig:hierarchical_basis}
\end{figure}

On the left, you can see the hierarchical subspaces up to level 3. All in all, combined they span the same space as $ V_3 $. In the hierarchical case, a function $ f $ can also be interpolated by its interpolant $ u $ by 
\begin{equation}
	f_l = \sum_{i \in I_l^h}\alpha_{l,i} \varphi_{l,i}, \forall i = 0,...,2^l: f_l(x_{l,i}) = f(x_{l,i}).
\end{equation}

An example can be seen in Figure \ref{fig:weighted_sum_hierarchical}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[scale=0.5]{figures/weighted_sum_hierarchical.png}
	\caption{ Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the hierarchical basis. Level of the grid is 3 and hat functions are used. Taken from \cite{pfluger2010spatially}.}
	\label{fig:weighted_sum_hierarchical}
\end{figure}

To get into higher dimensions $ d > 1 $, we use the tensor product. The domain is now $ \Omega = [0,1]^d $ and the level is defined by the level per dimension meaning $ \vec{l} = (l_1, ..., l_d) \in \mathbb{N}_0^d $. The index set is then
\begin{equation}
	I_{\vec{l}} = \{ \vec{i} | 1 \le i_j \le 2^{l_j} -1 , i_j \text{odd}, 1 \le j \le d \}
\end{equation}

and the subspaces 

\begin{equation}
	W_{\vec{l}} = \text{span}\{ \varphi_{\vec{l},\vec{i}}( \vec{x} ) | \vec{i} \in I_{\vec{l}}\}
\end{equation}

with the basis functions $ \varphi_{\vec{l},\vec{i}} = \prod_{j=1}^{d} \varphi_{l_j,i_j}(x_j) $ which are constructed with the tensor product. The function space $ V_n $ is constructed by
\begin{equation}
	V_n = \bigoplus_{|\vec{l}|_\infty \le n} W_l
\end{equation}
with $ |\vec{l}| = \max_{1 \le i \le d} |d_i| $. 



\subsection{Adaptive Sparse Grids}
