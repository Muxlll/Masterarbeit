% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Conclusion and Outlook}\label{chapter:conclusion_and_outlook}

In this work, two new approaches for hyperparameter optimization are introduced. They are compared to grid search, random search, and Bayesian optimization. In the following, the results are summarized and ideas for further improvements are presented. \newline

The first approach uses sparse grids to approximate the function depending on the hyperparameters. The idea is to adaptively generate the grid and finally find a hyperparameter configurations that leads to a good model performance. The application of an additional optimization algorithm on the interpolated function turned out to be inaccurate in most cases. This is due to the small number of grid points we are using because of the high cost for one single function evaluation. However, the adaptivity of the grid generation phase already makes use of previously evaluated configurations leading to more points in regions where promising results are found. This makes it very efficient especially in higher dimensions of the hyperparameter space. \newline 

The second approach presented is based on the conventional random search with the addition of an iterative adaptive phase. After sampling initial random points, we added iterations where promising regions are preferred for new points. We present three different refinement strategies for the adaptive sampling and promising results are achieved in smaller dimensions. However, with a higher number of hyperparameters, we encounter the problem of too small exploration which is due to the behavior of the refinement strategies in higher dimensions. \newline 

We compared the two approaches to grid search, conventional random search, and Bayesian optimization for different types of neural networks and machine learning tasks. The results show that sparse grid hyperparameter optimization performs well for most problem settings compared to the other approaches. Especially in high dimensions for our problems chosen, the new approach performs better than grid search, random search, and Bayesian optimization. 

For the iterative adaptive random search, promising results are achieved for small dimensions. In some cases, the conventional random search is outperformed. However, in higher dimensions, the refinement strategies encounter problems of too small exploration. \newline

In future work, especially the iterative adaptive random search has to be further analyzed and investigations are necessary for the refinement strategies. Possible improvements are for example to use a different, new refinement strategy or to adapt it by changing the radius. Also the refinement criterion which selects the point to be refined can be changed. The function value itself could for e.g. be included instead of only the rank of a point. 

In general, the focus of this thesis was on integer, logarithmic, and continuous hyperparameters. Only a few thoughts on categorical ones were presented for the sparse grid optimization. Further investigations are still necessary as categorical hyperparameters appear often for machine learning models.