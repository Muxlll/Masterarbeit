% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}

Machine learning has gained enormous importance in many fields of application. In many areas of daily life, artificial intelligence can increase productivity by automatically making decisions. There are many types of algorithms and architectures to reach this goal. They all have one thing in common, which is that they have to be defined and trained on data. In this process, the developer has to find a suitable configuration for the machine learning algorithm. Some of them depend on each other and for most of them, expert knowledge can help find the right values. Automating this task of hyperparameter optimization or tuning can be done without expert knowledge or an approach called babysitting which is just trying out different configurations until a reasonably good performance is achieved. This is also called trial and error and is often applied as can be seen in \cite{MALAKOUTI2023200248, gorgolis2019hyperparameter}. This technique can take some time to find a good configuration which is why there are already many algorithms to automate this. \newline 

All existing techniques like grid-, random search, and bayesian optimization have advantages and shortcomings depending on the concrete scenario. In the scope of this thesis, a new approach is introduced which uses sparse grids. They have the general advantage that there is no \textit{curse of the dimensionality} in higher dimensions compared to a general homogeneous grid because the number of grid points does not scale exponentially. We present the new technique and compare it using different types of machine learning models and various datasets and tasks like regression and classification. 

We additionally introduce an iterative adaptive random search approach which combines the advantages of random search and iterative methods like the bayesian optimization or the adaptive sparse grid search. 

The implementation as well as all experiments conducted can be found on Github \footnote{https://github.com/Muxlll/Masterarbeit}. \newline 

Therefore, in Chapter \ref{chapter:theoretical_background}, we give a brief introduction to machine learning with neural networks, present other well-analyzed hyperparameter optimization techniques like grid-, random search, and bayesian optimization, and explain the concept of adaptive sparse grid which is the base for the new approach. In the following in Chapter \ref{chapter:main_part}, we explain the Methodology and how the sparse grid optimization method works. First, normal functions are optimized because we know the optimal points. Later on, we transfer the findings to the machine learning model evaluation and further analyze the behavior in different settings. After finding suitable configurations for the sparse grid, we compare the implemented algorithms with different datasets and tasks followed by an introduction of the iterative adaptive random search. This method is also analyzed and compared to the existing approaches. In the end, we give a conclusion and outlook in Chapter \ref{chapter:conclusion_and_outlook}.


