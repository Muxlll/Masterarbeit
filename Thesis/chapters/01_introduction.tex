% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}

Machine learning has gained enormous importance in many fields of application. In various areas of daily life, artificial intelligence can enhance productivity by autonomously making decisions. There are numerous types of algorithms and architectures designed to achieve this goal. They all have one thing in common, which is that they have to be defined and trained on data. During this process, developers need to identify a suitable configuration for the machine learning algorithm.Some configurations are interdependent, and for many of them, expert knowledge can facilitate the identification of appropriate values. Automating this task of hyperparameter optimization or tuning can be done without expert knowledge or an approach called "babysitting" which is just trying out different configurations until a reasonably good performance is achieved. This is also called trial and error and is often applied as can be seen in \cite{MALAKOUTI2023200248, gorgolis2019hyperparameter}. This method might consume a significant amount of time to discover an optimal configuration, prompting the development of multiple algorithms designed to automate this process. \newline 

All existing techniques, such as grid search, random search, and Bayesian optimization, exhibit advantages and limitations contingent on the specific scenario. In the scope of this thesis, a new approach is introduced which uses sparse grids. Theyoffer a fundamental advantage over general homogeneous grids in higher dimensions, as they do not succumb to the \textit{curse of the dimensionality}. Unlike homogeneous grids, the number of grid points does not grow exponentially. We present this new technique and compare it across various machine learning models, datasets, and tasks, including regression and classification.

We additionally introduce an iterative adaptive random search approach which combines the advantages of random search and iterative methods like the Bayesian optimization or the adaptive sparse grid search. 

The implementation as well as all experiments conducted can be found on Github \footnote{https://github.com/Muxlll/Masterarbeit}. \newline 

Therefore, in Chapter \ref{chapter:theoretical_background}, we give a brief introduction to machine learning with neural networks, present other well-analyzed hyperparameter optimization techniques like grid-, random search, and Bayesian optimization, and explain the concept of adaptive sparse grid which is the base for the new approach. In the following in Chapter \ref{chapter:main_part}, we explain the Methodology and how the sparse grid optimization method works. First, normal functions are optimized because we know the optimal points. Later on, we transfer the findings to the machine learning model evaluation and further analyze the behavior in different settings. After finding suitable configurations for the sparse grid, we compare the implemented algorithms with different datasets and tasks followed by an introduction of the iterative adaptive random search. This method is also analyzed and compared to the existing approaches. In the end, we give a conclusion and outlook in Chapter \ref{chapter:conclusion_and_outlook}.


