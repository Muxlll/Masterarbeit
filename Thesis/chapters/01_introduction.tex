% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}

Machine learning has gained enormous importance in many fields of application. In many areas of daily life, artificial intelligence can increase productivity by automatically making decisions. There are many types of algorithms and architectures to reach this goal. They all have one thing in common, which is that they have to be defined and trained on data. In this process, the developer has to find a suitable configuration for the machine learning algorithm. Some of them depend on each other and for most of them, expert knowledge can help finding the right values. By automating this task of hyperparameter optimization or tuning, this can be done without expert knowledge or an approach called babysitting or trial and error which is just trying out different configurations until a reasonably good performance is achieved which is often applied as can be seen in \cite{MALAKOUTI2023200248, gorgolis2019hyperparameter}. This technique can take some time to find a good configuration which is why there are already many algorithms to automate this. \newline 

All existing techniques like grid-, random search, and bayesian optimization have advantages and shortcomings depending on the concrete scenario. In the scope of this thesis, a new approach is introduced which uses sparse grids. They have the general advantage that there is no \textit{curse of the dimensionality} in higher dimensions compared to a general homogeneous grid because the number of grid points does not scale exponentially. We present the new technique and compare it using different kind of machine learning models and various datasets and tasks like regression and classification. \newline 

Therefor, in Chapter \ref{chapter:theoretical_background}, we give a brief introduction to machine learning with neural networks, present other well-analyzed hyperparameter optimization techniques like grid-, random search, and bayesian optimization, and explain the concept of adaptive sparse grid which is the base for the new approach. In the following in Chapter \ref{chapter:main_part}, we explain the Methodology and how the sparse grid optimization method works. First, normal functions are optimized because we know the optimal points. Later on we transfer the findings to the machine learning model evaluation and further analyze the behavior in different settings. After finding suitable configurations for the sparse grid, we compare the implemented algorithms with different datasets and tasks. In the end, we give a conclusion and outlook in Chapter \ref{chapter:conclusion_and_outlook}.	



