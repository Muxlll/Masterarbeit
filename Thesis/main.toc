\babel@toc {american}{}\relax 
\contentsline {chapter}{Acknowledgments}{iii}{Doc-Start}%
\contentsline {chapter}{\nonumberline Abstract}{iv}{chapter*.1}%
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {chapter}{\numberline {2}State of the Art}{2}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduction to Neural Networks}{2}{section.2.1}%
\contentsline {section}{\numberline {2.2}Hyperparameter Optimization}{5}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Grid Search}{6}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Random Search}{6}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Bayesian Optimization}{7}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Other Techniques}{10}{subsection.2.2.4}%
\contentsline {section}{\numberline {2.3}Sparse Grids}{10}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Numerical Approximation of Functions}{10}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Adaptive Sparse Grids}{14}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Basis Functions for Sparse Grids}{17}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Optimization with Sparse Grids}{18}{subsection.2.3.4}%
\contentsline {paragraph}{\nonumberline Gradient-Free Methods}{18}{paragraph*.17}%
\contentsline {subparagraph}{\nonumberline Nelder Mead Method}{18}{subparagraph*.19}%
\contentsline {subparagraph}{\nonumberline Differential Evolution}{18}{subparagraph*.21}%
\contentsline {subparagraph}{\nonumberline CMA-ES}{19}{subparagraph*.23}%
\contentsline {paragraph}{\nonumberline Gradient-Based Methods}{19}{paragraph*.25}%
\contentsline {subparagraph}{\nonumberline Gradient Descent}{19}{subparagraph*.27}%
\contentsline {subparagraph}{\nonumberline NLCG}{19}{subparagraph*.29}%
\contentsline {subparagraph}{\nonumberline Newton}{19}{subparagraph*.31}%
\contentsline {subparagraph}{\nonumberline BFGS}{19}{subparagraph*.33}%
\contentsline {subparagraph}{\nonumberline Rprop}{19}{subparagraph*.35}%
\contentsline {chapter}{\numberline {3}Hyperparameter Optimization with Sparse Grids}{22}{chapter.3}%
\contentsline {section}{\numberline {3.1}Methodology}{22}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Adaptive Grid Search with Sparse Grids}{22}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Implementation}{22}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}Sparse Grid Optimization of functions}{23}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Sparse Grid Generation with different Adaptivities}{25}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Local and Global Optimization}{29}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Hyperparameter Optimization with Sparse Grids}{34}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Optimum Approximation with Sparse Grids}{34}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Optimization on Sparse Grids}{37}{subsection.3.3.2}%
\contentsline {section}{\numberline {3.4}Comparison with Grid-, Random Search and Bayesian Optimization}{43}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Two-dimensional Experiment of Regression with Small Neural Network}{43}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Three- and Five-dimensional Experiments of Regression with Small Neural Network}{44}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Nine-dimensional Optimization for Image Classification}{49}{subsection.3.4.3}%
\contentsline {chapter}{\numberline {4}Conclusion and Outlook}{50}{chapter.4}%
\contentsline {chapter}{\nonumberline List of Figures}{51}{chapter*.60}%
\contentsline {chapter}{\nonumberline List of Tables}{55}{chapter*.61}%
\contentsline {chapter}{\nonumberline Bibliography}{56}{chapter*.62}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
