\babel@toc {american}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Neural network consisting of only one perceptron. The output is computed according to Equation \ref {eq:perceptron}.\relax }}{3}{figure.caption.4}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Neural network consisting of two hidden layers. The connections between the perceptrons are bidirectional. In the forward phase, the intermediate results are given to the neurons to the right and in the backward phase from right to left. \relax }}{4}{figure.caption.5}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison of grid search (left) and random search (right) in the two dimensional case. For both techniques, 9 different combinations are evaluated. In the left case, only 3 distinct values for each hyperparameter are set whereas there are 9 different values for each parameter in the random search. Taken from \blx@tocontentsinit {0}\cite {feurer2019hyperparameter}. \relax }}{6}{figure.caption.6}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Schematic iteration steps of the bayesian optimization. The maximum of the acquisition function determines the next function evaluation (red dot in the middle). The goal is to find the minimum of the dashed line. The blue band is the uncertainty of the function. Taken from \blx@tocontentsinit {0}\cite {feurer2019hyperparameter}. \relax }}{9}{figure.caption.7}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the nodal basis. Level of the grid is 3 and hat functions are used. Taken from \blx@tocontentsinit {0}\cite {pfluger2010spatially}. \relax }}{11}{figure.caption.8}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Hierarchical subspaces up to level 3 on the left. On the right, nodal spaces up to level 3. The combination of $ W_1 $ up to $ W_3 $ is the same space as $ V_3 $. Taken from \blx@tocontentsinit {0}\cite {pfluger2010spatially}. \relax }}{12}{figure.caption.9}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Interpolation of the function $ f $ (black line) by its interpolant $ u $ (red, dashed) in the hierarchical basis. Level of the grid is 3 and hat functions are used. Taken from \blx@tocontentsinit {0}\cite {pfluger2010spatially}.\relax }}{13}{figure.caption.10}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Example of a basis function in two dimensions. It is constructed with the tensor product of two 1d hat functions. Taken from \blx@tocontentsinit {0}\cite {garcke2013sparse}. \relax }}{14}{figure.caption.11}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Two dimensional example of a sparse grid with $ n = 3 $. Left, the subspaces $ W_{\vec {l}} $ can be seen and on the right is the resulting sparse grid. Taken from \blx@tocontentsinit {0}\cite {garcke2013sparse}. \relax }}{15}{figure.caption.12}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Example of the 2-dimensional combination technique. Here the blue regular grids are added and the red ones are subtracted. On the left, the normal combination technique can be seen and on the right is an dimension-adaptive version. Taken from \blx@tocontentsinit {0}\cite {pfluger2010spatially}. \relax }}{16}{figure.caption.13}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces Example of the spatially adaptive combination technique in two dimensions. Taken from \blx@tocontentsinit {0}\cite {obersteiner2021generalized}. \relax }}{17}{figure.caption.15}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces HPC of level $ k = 5 $ (red stars) and full grid (black dots). A full grid would have $ 33^2 = 1089 $ and this grid has $ 147 $ points. Taken from \blx@tocontentsinit {0}\cite {duan2016induction}.\relax }}{20}{figure.caption.36}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Test functions used for evaluating the sparse grid optimization. Each one is plotted with 200 samples in each dimension. Note that the function values of the Rosenbrock function are transformed with $ log_{10}(f(x)) $ for better visualization.\relax }}{24}{figure.caption.38}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Eggholder function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }}{26}{figure.caption.39}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rosenbrock function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }}{27}{figure.caption.40}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Sparse grid generation depending on the adaptivity parameter $ \gamma $ for the Rastrigin function. In all cases, the same number of grid points is used. Here, in each of the 249 iterations, 4 new grid points are added resulting in a overall number of 997 function evaluations.\relax }}{28}{figure.caption.41}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Influence of the adaptivity parameter on the error (difference to the actual optimal value) of the optimum found by the sparse grid. \relax }}{30}{figure.caption.42}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Optimization error for different algorithms depending on the number of grid points in two dimensions. The plots show the results with degree 2 (top left), degree 3 (top right) and degree 5 (bottom). The Rastrigin function was used. \relax }}{32}{figure.caption.43}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Resulting optimal points from local (red points) and global (black points) optimization algorithm. In the background, the contour of the interpolated function is depicted with the corresponding sparse grid points in white. The left one has 5 grid points, the one in the center 77, and the right one 997. The degree of the B-splines on the sparse grid is 2. \relax }}{33}{figure.caption.44}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Error of the optimization depending on the number of grid points used for different dimensions of the Rastrigini function. \relax }}{34}{figure.caption.46}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces 2-layer (with 30 neurons each) fully connected network evaluation on the diamonds dataset depending on the number of epochs and learning rate of the Adam optimizer. \relax }}{35}{figure.caption.47}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Analysis of sparse grid with machine learning evaluation for different adaptivity parameters ($ 0.85 $ \ref {fig:analysis_sparse_grid_with_machine_learning_085} and $ 0.5 $ \ref {fig:analysis_sparse_grid_with_machine_learning_05}), each with 29 and 77 grid points.\relax }}{37}{figure.caption.48}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces Optimization steps of gradient descent algorithm for 5 (top left), 9 (top right), 29 (bottom left) and 49 (bottom right) grid points. In the background of each plot, the contour of the interpolated function is shown. The function evaluated is the same as depicted in Figure \ref {fig:analysis_model_training}. \relax }}{39}{figure.caption.50}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces Comparison of optimization algorithms on the sparse grid with increasing number of grid points in two and three dimensions. Epochs, learning rate, and the batch size of a two-layer neural network on the diamonds dataset for regression are optimized. Result is the mean absolute percentage error. \relax }}{41}{figure.caption.52}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
