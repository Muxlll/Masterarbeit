\babel@toc {american}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {2.1}{\ignorespaces Comparison of sparse grids, full grids, and the combination technique in terms of number of grid points and the accuracy. \relax }}{19}{table.caption.15}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Three test functions and their properties.\relax }}{28}{table.caption.38}%
\contentsline {table}{\numberline {3.2}{\ignorespaces Overview over the exact solutions found by the local and global optimizer for the same problem as in Figure \ref {fig:optimizers_visualized}. The actual optimum is at (0, 0), with function value 0 (see Table \ref {tab:test_functions}).\relax }}{38}{table.caption.46}%
\contentsline {table}{\numberline {3.3}{\ignorespaces Best hyperparameter configuration found by the sparse grid with different adaptivity parameters and number of grid points. The best result is found with $ \gamma = 0.85 $ and 77 grid points. \relax }}{43}{table.caption.50}%
\contentsline {table}{\numberline {3.4}{\ignorespaces Exact values for the optima found by the sparse grid points and the gradient descent algorithm. The values for $ x_{min}^{grid} $ are the configurations evaluated by the sparse grid during the generation and the coordinates in column $ x_{min}^{opt} $ are found by the optimizer. In bold, the best function values of the optimum found by the sparse grid and gradient descent algorithm, respectively. \relax }}{45}{table.caption.52}%
\contentsline {table}{\numberline {3.5}{\ignorespaces Overview over the datasets used for the first comparison of the optimization algorithms. They are all available on OpenML \blx@tocontentsinit {0}\cite {OpenML2013}. \relax }}{50}{table.caption.55}%
\contentsline {table}{\numberline {3.6}{\ignorespaces Hyperparameters with their type and interval for the nine dimensional space. The values for the Int-Interval are discretized with Python's \textit {int()} function. The learning rate is sampled logarithmic and the dropout probability can take continuous values between 0 and 1. \relax }}{57}{table.caption.60}%
\contentsline {table}{\numberline {3.7}{\ignorespaces Overview over the best configurations found by the different algorithms grid search (GS), random search (RS), bayesian optimization (BO) and sparse grid optimization (SG). The configuration is given as tuple (epochs, batch size, learning rate, number conv layers, number fully connected layers, kernel size, pool size, neurons per fc layer, dropout probability). \relax }}{59}{table.caption.62}%
\contentsline {table}{\numberline {3.8}{\ignorespaces Architecture used in \blx@tocontentsinit {0}\cite {WU201926} for MNIST image classification. \relax }}{59}{table.caption.63}%
\contentsline {table}{\numberline {3.9}{\ignorespaces Best configurations and corresponding test accuracy found by the four algorithms for the MNIST dataset. \relax }}{61}{table.caption.65}%
\contentsline {table}{\numberline {3.10}{\ignorespaces Resulting optimum found by the three different refinement strategies interval based refinement strategy (1), uniform d-ball sampling strategy (2), and normal distribution sampling strategy (3). The best of each refinement algorithm is marked in bold. \relax }}{70}{table.caption.78}%
\contentsline {table}{\numberline {3.11}{\ignorespaces Best configurations found by the adaptive iterative random search with three different refinement strategies and budgets 10, 50, and 100. \relax }}{78}{table.caption.87}%
\contentsline {table}{\numberline {3.12}{\ignorespaces Hyperparameters with their type and interval for the 6-dimensional optimization problem solved with the iterative adaptive random search. \relax }}{79}{table.caption.89}%
\contentsline {table}{\numberline {3.13}{\ignorespaces Comparison of the optimization algorithms in terms of mean average percentage error for the 6-dimensional optimization problem. \relax }}{81}{table.caption.91}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
